{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordsEmbeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPHZOT2pVPmxW0FVH7hCzv+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taedriel/ZSL-v2/blob/wordEmbedding/WordsEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYxydB4dlTpi",
        "outputId": "55360744-9d24-4877-af86-8defb7323d93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Kingston/ZSL-v2\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# %cd /content/drive/MyDrive/Kingston/ZSL-v2/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers wget tensorflow_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEq14T6Nl535",
        "outputId": "e1d46e36-4bfb-4324-dcff-181b880c1f37"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.1)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (4.0.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.16.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.1.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (5.7.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.7.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.1.7)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.3.4)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (3.17.3)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (21.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.56.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level = logging.INFO)\n",
        "\n",
        "from typing import List"
      ],
      "metadata": {
        "id": "spXWSv2iW9i5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WordToVecteur:\n",
        "\n",
        "    def __init__(self, list_tags : List[str] = []):\n",
        "        self.list_tags = list_tags\n",
        "\n",
        "    def export(self, filename):\n",
        "        return NotImplementedError\n",
        "\n",
        "    def importTagList(self, filename):\n",
        "        return NotImplementedError\n",
        "\n",
        "    def convert(self):\n",
        "        return NotImplementedError\n",
        "\n",
        "\n",
        "class BERTModel(WordToVecteur):\n",
        "\n",
        "    def __init__(self, list_tag : List[str] = [], big: bool = False):\n",
        "        super(BERTModel, self).__init__(list_tag)\n",
        "        self.model_size = \"bert-large-uncased\" if big else \"bert-base-uncased\"\n",
        "        self.embeddings = []\n",
        "        self.cosine_sim_matrix = None\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.model_size, padding=True, truncation=True,)\n",
        "        self.model = BertModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "\n",
        "    def export(self, filename):\n",
        "        if len(self.embeddings) == 0:\n",
        "            raise Exception(\"Tags not converted yet !\")\n",
        "        \n",
        "        try:\n",
        "            f = open(filename, \"w\")\n",
        "        except OSError:\n",
        "            raise OSError(\"Could not open file\")\n",
        "\n",
        "        with f:\n",
        "            for embedding in self.embeddings:\n",
        "                line = \",\".join(list(map(str, map(float, embedding[1]))))\n",
        "                print(embedding[0], \",\", line, sep=\"\", file=f)\n",
        "\n",
        "    def import_tag_list(self, filename):\n",
        "        self.embeddings.clear()\n",
        "        self.cosine_sim_matrix = None\n",
        "\n",
        "        try:\n",
        "            f = open(filename, \"r\")\n",
        "        except OSError:\n",
        "            return OSError(\"Could not open file\")\n",
        "\n",
        "        with f:\n",
        "            data = f.read().split(\"\\n\")\n",
        "            for item in data:\n",
        "                if item not in self.list_tags and str.strip(item) != \"\":\n",
        "                    self.list_tags.append(item)\n",
        "            print(self.list_tags)\n",
        "            logging.info(f\"Import finished : {len(self.list_tags)} elements imported.\")\n",
        "            \n",
        "\n",
        "    def convert(self):\n",
        "        \"\"\" convert all word in their embeddings\"\"\"\n",
        "\n",
        "        logging.info(\"Starting converting tokens...\")\n",
        "        nb_token = len(self.list_tags)\n",
        "        current_percent = 0\n",
        "\n",
        "        for i, tag in enumerate(self.list_tags):\n",
        "            \n",
        "            percent_completion = round((i / nb_token) * 100, 2)\n",
        "            logging.info(f\"{percent_completion}% completed\")\n",
        "            \n",
        "            inputs = self.tokenizer(tag, return_tensors = \"pt\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            hidden_states = outputs[2]\n",
        "\n",
        "            # log.info(f\"[{i}]\",\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
        "            # log.info(f\"[{i}]\",\"Number of batches:\", len(hidden_states[0]))\n",
        "            # logging.info(f\"[{i}] Number of tokens: {len(hidden_states[0][0]) - 2}\")\n",
        "            # log.info(f\"[{i}]\",\"Number of hidden units:\", len(hidden_states[0][0][0]))\n",
        "\n",
        "            # [# layers, # batches, # tokens, # features] ==> [# tokens, # layers, # features]\n",
        "            token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "            token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "            token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "            # apply different strategy to summarize word embeddings\n",
        "            # tokenized_text = self.tokenizer.tokenize(tag)\n",
        "            # acc = []\n",
        "            # for i, token in reversed(list(enumerate(tokenized_text))):\n",
        "\n",
        "            #     embed = self.merging_strategy.merge(token_embeddings[i+1])\n",
        "            #     if i == 0:\n",
        "            #         if len(acc) != 0:\n",
        "            #             embed = torch.mean(torch.stack([x[1] for x in acc]), dim=0)\n",
        "            #             token = tag\n",
        "            #             acc = []\n",
        "\n",
        "            #         self.embeddings.append((token, embed))\n",
        "            #     else:\n",
        "            #         acc.append((token, embed))\n",
        "            \n",
        "            self.embeddings.append((tag, self.merging_strategy.merge(token_embeddings[0])))\n",
        "\n",
        "\n",
        "    def compute_sim(self):\n",
        "        \"\"\" compute cosine similarity between all vectors \"\"\"\n",
        "        if len(self.embeddings) == 0:\n",
        "            raise Exception(\"Tags not converted yet !\")\n",
        "\n",
        "        logging.info(\"Computing cosine similarity, this could take some time...\")\n",
        "\n",
        "        n_tokens = len(self.embeddings)\n",
        "        self.cosine_sim_matrix = [[1 for j in range(n_tokens)] for i in range(n_tokens)]\n",
        "\n",
        "        for j, vector in enumerate(self.embeddings):\n",
        "\n",
        "            for i, other_vector in enumerate(self.embeddings):\n",
        "\n",
        "                if i == j:\n",
        "                    continue\n",
        "\n",
        "                \n",
        "                cos = torch.nn.CosineSimilarity(dim=0)\n",
        "                similarity = cos(vector[1], other_vector[1])\n",
        "\n",
        "                self.cosine_sim_matrix[i][j] = similarity\n",
        "                self.cosine_sim_matrix[j][i] = similarity\n",
        "\n",
        "    def export_sim_matrix(self, filename):\n",
        "        if self.cosine_sim_matrix == None:\n",
        "            self.compute_sim()\n",
        "        \n",
        "        try:\n",
        "            f = open(filename, \"w\")\n",
        "        except OSError:\n",
        "            raise OSError(\"Could not open file\")\n",
        "\n",
        "        with f:\n",
        "            print(\"/\", \",\".join([tag[0] for tag in self.embeddings]), sep = \",\", file = f)\n",
        "\n",
        "            for j, tag_y in enumerate(self.embeddings):\n",
        "                print(tag_y[0], \",\".join( [str(round(float(self.cosine_sim_matrix[j][i]), 3)) for i in range(len(self.embeddings))]), sep = \",\", file = f)\n",
        "\n",
        "    def sim_between(self, token1, token2):\n",
        "        if self.cosine_sim_matrix is None:\n",
        "            self.compute_co_sim()\n",
        "\n",
        "        index1 = [i for i, v in enumerate(self.embeddings) if v[0] == token1][0]\n",
        "        index2 = [i for i, v in enumerate(self.embeddings) if v[0] == token2][0]\n",
        "\n",
        "        return self.cosine_sim_matrix[index1][index2]\n",
        "\n",
        "    def get_embedding_of(self, token):\n",
        "        res = [v for v in self.embeddings if v[0] == token]\n",
        "        if len(res) == 0:\n",
        "            raise Exception(\"no such token\")\n",
        "        \n",
        "        return res[0]\n",
        "\n",
        "    def get_nearest_embedding_of(self, embedding, nb = 10):\n",
        "\n",
        "        if nb > len(self.embeddings):\n",
        "            raise Exception(\"nb too high, not enough token\")\n",
        "\n",
        "        nearest = []\n",
        "        for e in self.embeddings:\n",
        "\n",
        "            cos = torch.nn.CosineSimilarity(dim=0)\n",
        "            similarity = cos(embedding, e[1])\n",
        "\n",
        "            nearest.append((e[0], similarity))\n",
        "        \n",
        "        nearest.sort(key = lambda tup : tup[1])\n",
        "        return nearest[-nb:]\n",
        "\n",
        "    def get_class_list(self):\n",
        "        return [x[0] for x in self.embeddings]\n",
        "\n",
        "class Sum4LastLayers:\n",
        "\n",
        "    def merge(self, vector):\n",
        "        return torch.sum(vector[-4:], dim = 0)"
      ],
      "metadata": {
        "id": "UVJUSGWQorYj"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_name = 'oxford_flowers102'\n",
        "splits = ['test', 'validation', 'train']\n",
        "ds, info = tfds.load(ds_name, split = splits, with_info=True)\n",
        "(train_examples, validation_examples, test_examples) = ds\n",
        "print(f\"Number of flower types {info.features['label'].num_classes}\")\n",
        "print(f\"Number of training examples: {tf.data.experimental.cardinality(train_examples)}\")\n",
        "print(f\"Number of validation examples: {tf.data.experimental.cardinality(validation_examples)}\")\n",
        "print(f\"Number of test examples: {tf.data.experimental.cardinality(test_examples)}\\n\")\n",
        "\n",
        "print('Flower types full list:')\n",
        "\n",
        "class_list = info.features['label'].names\n",
        "print(class_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06vV8RTuKuX4",
        "outputId": "7768a826-a36d-4309-8e55-caf435a3db09"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:Load dataset info from /root/tensorflow_datasets/oxford_flowers102/2.1.1\n",
            "INFO:absl:Reusing dataset oxford_flowers102 (/root/tensorflow_datasets/oxford_flowers102/2.1.1)\n",
            "INFO:absl:Constructing tf.data.Dataset for split ['test', 'validation', 'train'], from /root/tensorflow_datasets/oxford_flowers102/2.1.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of flower types 102\n",
            "Number of training examples: 6149\n",
            "Number of validation examples: 1020\n",
            "Number of test examples: 1020\n",
            "\n",
            "Flower types full list:\n",
            "['pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', 'english marigold', 'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', 'globe thistle', 'snapdragon', \"colt's foot\", 'king protea', 'spear thistle', 'yellow iris', 'globe-flower', 'purple coneflower', 'peruvian lily', 'balloon flower', 'giant white arum lily', 'fire lily', 'pincushion flower', 'fritillary', 'red ginger', 'grape hyacinth', 'corn poppy', 'prince of wales feathers', 'stemless gentian', 'artichoke', 'sweet william', 'carnation', 'garden phlox', 'love in the mist', 'mexican aster', 'alpine sea holly', 'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip', 'lenten rose', 'barbeton daisy', 'daffodil', 'sword lily', 'poinsettia', 'bolero deep blue', 'wallflower', 'marigold', 'buttercup', 'oxeye daisy', 'common dandelion', 'petunia', 'wild pansy', 'primula', 'sunflower', 'pelargonium', 'bishop of llandaff', 'gaura', 'geranium', 'orange dahlia', 'pink-yellow dahlia?', 'cautleya spicata', 'japanese anemone', 'black-eyed susan', 'silverbush', 'californian poppy', 'osteospermum', 'spring crocus', 'bearded iris', 'windflower', 'tree poppy', 'gazania', 'azalea', 'water lily', 'rose', 'thorn apple', 'morning glory', 'passion flower', 'lotus', 'toad lily', 'anthurium', 'frangipani', 'clematis', 'hibiscus', 'columbine', 'desert-rose', 'tree mallow', 'magnolia', 'cyclamen', 'watercress', 'canna lily', 'hippeastrum', 'bee balm', 'ball moss', 'foxglove', 'bougainvillea', 'camellia', 'mallow', 'mexican petunia', 'bromelia', 'blanket flower', 'trumpet creeper', 'blackberry lily']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [\"king\", \"queen\", \"man\", \" woman\", \"splurgle\", \"pladonf\"]\n",
        "model = BERTModel(class_list)\n",
        "\n",
        "# model.import_tag_list(\"en-basic\")\n",
        "model.convert()\n",
        "model.export(\"flower-embeddeding.csv\")\n",
        "print(model.get_class_list())\n",
        "\n",
        "# model.computeCoSim()\n",
        "# model.simBetween(\"cat\", \"dog\")\n",
        "\n",
        "# man = model.get_embedding_of(\"man\")[1]\n",
        "# woman = model.get_embedding_of(\"woman\")[1]\n",
        "\n",
        "# king = model.get_embedding_of(\"king\")[1]\n",
        "\n",
        "# totest = king.sub(man).add(woman)\n",
        "# print(model.get_nearest_embedding_of(totest, 3))\n",
        "\n",
        "# model.export_sim_matrix(\"sim_matrix.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOOlgG8koVD6",
        "outputId": "dfdcc0f6-bbbe-4a5f-e16a-4b30a44646ae"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "INFO:root:Starting converting tokens...\n",
            "INFO:root:0.0% completed\n",
            "INFO:root:0.98% completed\n",
            "INFO:root:1.96% completed\n",
            "INFO:root:2.94% completed\n",
            "INFO:root:3.92% completed\n",
            "INFO:root:4.9% completed\n",
            "INFO:root:5.88% completed\n",
            "INFO:root:6.86% completed\n",
            "INFO:root:7.84% completed\n",
            "INFO:root:8.82% completed\n",
            "INFO:root:9.8% completed\n",
            "INFO:root:10.78% completed\n",
            "INFO:root:11.76% completed\n",
            "INFO:root:12.75% completed\n",
            "INFO:root:13.73% completed\n",
            "INFO:root:14.71% completed\n",
            "INFO:root:15.69% completed\n",
            "INFO:root:16.67% completed\n",
            "INFO:root:17.65% completed\n",
            "INFO:root:18.63% completed\n",
            "INFO:root:19.61% completed\n",
            "INFO:root:20.59% completed\n",
            "INFO:root:21.57% completed\n",
            "INFO:root:22.55% completed\n",
            "INFO:root:23.53% completed\n",
            "INFO:root:24.51% completed\n",
            "INFO:root:25.49% completed\n",
            "INFO:root:26.47% completed\n",
            "INFO:root:27.45% completed\n",
            "INFO:root:28.43% completed\n",
            "INFO:root:29.41% completed\n",
            "INFO:root:30.39% completed\n",
            "INFO:root:31.37% completed\n",
            "INFO:root:32.35% completed\n",
            "INFO:root:33.33% completed\n",
            "INFO:root:34.31% completed\n",
            "INFO:root:35.29% completed\n",
            "INFO:root:36.27% completed\n",
            "INFO:root:37.25% completed\n",
            "INFO:root:38.24% completed\n",
            "INFO:root:39.22% completed\n",
            "INFO:root:40.2% completed\n",
            "INFO:root:41.18% completed\n",
            "INFO:root:42.16% completed\n",
            "INFO:root:43.14% completed\n",
            "INFO:root:44.12% completed\n",
            "INFO:root:45.1% completed\n",
            "INFO:root:46.08% completed\n",
            "INFO:root:47.06% completed\n",
            "INFO:root:48.04% completed\n",
            "INFO:root:49.02% completed\n",
            "INFO:root:50.0% completed\n",
            "INFO:root:50.98% completed\n",
            "INFO:root:51.96% completed\n",
            "INFO:root:52.94% completed\n",
            "INFO:root:53.92% completed\n",
            "INFO:root:54.9% completed\n",
            "INFO:root:55.88% completed\n",
            "INFO:root:56.86% completed\n",
            "INFO:root:57.84% completed\n",
            "INFO:root:58.82% completed\n",
            "INFO:root:59.8% completed\n",
            "INFO:root:60.78% completed\n",
            "INFO:root:61.76% completed\n",
            "INFO:root:62.75% completed\n",
            "INFO:root:63.73% completed\n",
            "INFO:root:64.71% completed\n",
            "INFO:root:65.69% completed\n",
            "INFO:root:66.67% completed\n",
            "INFO:root:67.65% completed\n",
            "INFO:root:68.63% completed\n",
            "INFO:root:69.61% completed\n",
            "INFO:root:70.59% completed\n",
            "INFO:root:71.57% completed\n",
            "INFO:root:72.55% completed\n",
            "INFO:root:73.53% completed\n",
            "INFO:root:74.51% completed\n",
            "INFO:root:75.49% completed\n",
            "INFO:root:76.47% completed\n",
            "INFO:root:77.45% completed\n",
            "INFO:root:78.43% completed\n",
            "INFO:root:79.41% completed\n",
            "INFO:root:80.39% completed\n",
            "INFO:root:81.37% completed\n",
            "INFO:root:82.35% completed\n",
            "INFO:root:83.33% completed\n",
            "INFO:root:84.31% completed\n",
            "INFO:root:85.29% completed\n",
            "INFO:root:86.27% completed\n",
            "INFO:root:87.25% completed\n",
            "INFO:root:88.24% completed\n",
            "INFO:root:89.22% completed\n",
            "INFO:root:90.2% completed\n",
            "INFO:root:91.18% completed\n",
            "INFO:root:92.16% completed\n",
            "INFO:root:93.14% completed\n",
            "INFO:root:94.12% completed\n",
            "INFO:root:95.1% completed\n",
            "INFO:root:96.08% completed\n",
            "INFO:root:97.06% completed\n",
            "INFO:root:98.04% completed\n",
            "INFO:root:99.02% completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', 'english marigold', 'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', 'globe thistle', 'snapdragon', \"colt's foot\", 'king protea', 'spear thistle', 'yellow iris', 'globe-flower', 'purple coneflower', 'peruvian lily', 'balloon flower', 'giant white arum lily', 'fire lily', 'pincushion flower', 'fritillary', 'red ginger', 'grape hyacinth', 'corn poppy', 'prince of wales feathers', 'stemless gentian', 'artichoke', 'sweet william', 'carnation', 'garden phlox', 'love in the mist', 'mexican aster', 'alpine sea holly', 'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip', 'lenten rose', 'barbeton daisy', 'daffodil', 'sword lily', 'poinsettia', 'bolero deep blue', 'wallflower', 'marigold', 'buttercup', 'oxeye daisy', 'common dandelion', 'petunia', 'wild pansy', 'primula', 'sunflower', 'pelargonium', 'bishop of llandaff', 'gaura', 'geranium', 'orange dahlia', 'pink-yellow dahlia?', 'cautleya spicata', 'japanese anemone', 'black-eyed susan', 'silverbush', 'californian poppy', 'osteospermum', 'spring crocus', 'bearded iris', 'windflower', 'tree poppy', 'gazania', 'azalea', 'water lily', 'rose', 'thorn apple', 'morning glory', 'passion flower', 'lotus', 'toad lily', 'anthurium', 'frangipani', 'clematis', 'hibiscus', 'columbine', 'desert-rose', 'tree mallow', 'magnolia', 'cyclamen', 'watercress', 'canna lily', 'hippeastrum', 'bee balm', 'ball moss', 'foxglove', 'bougainvillea', 'camellia', 'mallow', 'mexican petunia', 'bromelia', 'blanket flower', 'trumpet creeper', 'blackberry lily']\n"
          ]
        }
      ]
    }
  ]
}