{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordsEmbeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taedriel/ZSL-v2/blob/wordEmbedding/WordsEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation part"
      ],
      "metadata": {
        "id": "bYtbXyM-iE1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import stuff\n"
      ],
      "metadata": {
        "id": "7fqDjGPZE1hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yes | pip install transformers wget wikipedia unzip mxnet gluonnlp \"scipy>=1.7\" scikit-bio wikipedia2vec --quiet --upgrade\n",
        "!mkdir -p temp article"
      ],
      "metadata": {
        "id": "HEq14T6Nl535"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as  np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, RobertaModel, RobertaTokenizer\n",
        "import tensorflow as tf\n",
        "import gluonnlp as nlp\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from wikipedia2vec import Wikipedia2Vec\n",
        "import wikipedia\n",
        "wikipedia.set_rate_limiting(True)\n",
        "\n",
        "import gc\n",
        "import traceback\n",
        "import pickle\n",
        "import json\n",
        "import logging\n",
        "logging.basicConfig(level = logging.INFO, filename = \"BERT.log\" )\n",
        "# logging.basicConfig(level = logging.INFO)\n",
        "\n",
        "from tqdm import tqdm\n",
        "from typing import List, Tuple, Dict, Callable\n",
        "from os.path import exists, join, abspath\n",
        "from os import system\n",
        "from enum import Enum\n",
        "from time import perf_counter\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "from scipy.stats import SpearmanRConstantInputWarning\n",
        "from scipy.stats import spearmanr\n",
        "from skbio import DistanceMatrix\n",
        "from skbio.tree import nj"
      ],
      "metadata": {
        "id": "spXWSv2iW9i5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils classes"
      ],
      "metadata": {
        "id": "kjt7fI8Olf5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usefull functions ✅"
      ],
      "metadata": {
        "id": "W3-C-5ztMDZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dict2csv(filename : str, embeddings : Dict[str, List[float]]) -> None:\n",
        "    \"\"\" write a dict of embeddings under a .CSV file\n",
        "\n",
        "    the .CSV file is construct with a header looking like this :\n",
        "    \\tembeddings\\t | 0 | 1 | 2 | 3 | ...\n",
        "    where each line contain an embeddings for the word in the first row\n",
        "    Args:\n",
        "        filename (str) : a path to the file where the .csv is to be written\n",
        "        embeddings (Dict[str, List[float]]): a dictionnary of embeddings\n",
        "\n",
        "    \"\"\"\n",
        "    try:\n",
        "        f = open(filename, \"w\")\n",
        "    except OSError:\n",
        "        raise OSError(\"Could not open file\")\n",
        "\n",
        "    dimension_number = len(next(iter(embeddings.values())))\n",
        "    with f:\n",
        "        print(\"embeddings\", *[str(i) for i in range(dimension_number)], sep=\",\", file=f)\n",
        "        for tag, embedding in embeddings.items():\n",
        "            print(tag, *list(map(lambda x: str(float(x)), embedding)), sep=\",\", file=f)\n",
        "\n",
        "def sim2dist(mat : List[List[float]], func : Callable[[float], float] \\\n",
        "             = lambda x: 1 - x, hollow : bool = True) -> List[List[float]]:\n",
        "    \"\"\" map the function func to each elements in the matrix\n",
        "\n",
        "    apply the lambda function func to each element of the matrix. if hollow is set \n",
        "    to True, set the diagonal of the matrix to 0.\n",
        "    Args:\n",
        "        mat (List[List[float]]) : a matrix of number\n",
        "        func (Callable[[float], float]) : a simple function to apply to each elem of the matrix\n",
        "        hollow (bool) : whether to consider the diagonal of the matrix or not\n",
        "    \n",
        "    \"\"\"\n",
        "    inv_data = [[0 for i in range(len(mat[0]))] for j in range(len(mat))]\n",
        "\n",
        "    for i, elem in enumerate(mat):\n",
        "        for j, case in enumerate(elem):\n",
        "            if i == j and hollow: \n",
        "                inv_data[i][j] = 0\n",
        "            else:\n",
        "                inv_data[i][j] = func(case)\n",
        "    \n",
        "    return inv_data\n",
        "\n",
        "def print_mat(mat : List[List[float]], format_function : Callable[[float], str]=lambda x: x) -> None:\n",
        "    \"\"\" print a matrice on stdout\n",
        "\n",
        "    format each number in the matrice using the format_function\n",
        "    Args:\n",
        "        mat (List[List[float]]) : a matrix of number\n",
        "        format_function (Callable[[float], str]) : a simple format function to display numbers froms the matrix\n",
        "    \"\"\"\n",
        "    for line in mat:\n",
        "        for case in line:\n",
        "            print(f\"{format_function(case):8}\", end=\"\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "COZfnM3QNwUc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Strategy ✅\n",
        "\n"
      ],
      "metadata": {
        "id": "-Wd47Q50gPSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTMergeStrategy:\n",
        "    \"\"\" strategy to extract BERT embeddings\n",
        "    \n",
        "    different approach exist, \n",
        "    see https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/BERT/bert-feature-extraction-contextualized-embeddings.png\n",
        "    for more possible strategy\n",
        "    \"\"\"\n",
        "\n",
        "    def merge(self, vector : List[List[float]]) -> List[float]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "class Sum4LastLayers(BERTMergeStrategy):\n",
        "\n",
        "    def merge(self, vector : List[List[float]]) -> List[float]:\n",
        "        return torch.sum(vector[-4:], dim = 0)\n",
        "\n",
        "class Concat4LastLayer(BERTMergeStrategy):\n",
        "\n",
        "    def merge(self, vector : List[List[float]]) -> List[float]:\n",
        "        return torch.concat(vector[-4:], dim = 0)\n",
        "\n",
        "class Similarity:\n",
        "    \"\"\"strategy to compute similarity between embeddings. Cosine similarity should \n",
        "    be the only valid one in word embeddings, other aren't relevant\n",
        "    \"\"\"\n",
        "\n",
        "    def sim(self, embed1 : List[float], embed2 : List[float]) -> float:\n",
        "        raise NotImplementedError\n",
        "\n",
        "class CosineSim(Similarity):\n",
        "\n",
        "    def sim(self, embed1 : List[float], embed2 : List[float]) -> float:\n",
        "        cos = torch.nn.CosineSimilarity(dim=0)\n",
        "        return cos(embed1, embed2)\n",
        "\n",
        "class EuclidianDistSim(Similarity):\n",
        "\n",
        "    def sim(self, embed1 : List[float], embed2 : List[float]) -> float:\n",
        "        return np.linalg.norm(embed1-embed2)\n",
        "\n",
        "class ManhattanDistSim(Similarity):\n",
        "\n",
        "    def sim(self, embed1 : List[float], embed2 : List[float]) -> float:\n",
        "        return cityblock(embed1, embed2)"
      ],
      "metadata": {
        "id": "LC8LdC2ngRtx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Article"
      ],
      "metadata": {
        "id": "7G26rtCEfQny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class customWikiArticle:\n",
        "\n",
        "    def __init__(self, index : int, title : str, realtitle : str, summary : str, ambiguous : bool):\n",
        "        self.index = index\n",
        "        self.title = title\n",
        "        self.realtitle = realtitle\n",
        "        self.summary = summary\n",
        "        self.ambiguous = ambiguous\n",
        "\n",
        "class ArticleRetriever:\n",
        "\n",
        "    article_dir = \"./article\"\n",
        "\n",
        "    def __init__(self, name : str, list_title : List[str]):\n",
        "\n",
        "        self.name = name\n",
        "        self.list_title = list_title\n",
        "        self.modified = False\n",
        "\n",
        "        if not exists(self.get_filename()):\n",
        "            self.articles_map = {}\n",
        "        else:\n",
        "            with open(self.get_filename(), \"rb\") as mapfile:\n",
        "                self.articles_map = pickle.load(mapfile)\n",
        "\n",
        "    def get_filename(self):\n",
        "        return join(ArticleRetriever.article_dir,self.name)\n",
        "\n",
        "    def load_article(self, title, force_reload : bool = False) -> customWikiArticle:\n",
        "        if title not in self.articles_map:\n",
        "            self.modified = True\n",
        "            realtitle, summary, ambiguous = self._retrieve_article(title)\n",
        "            self.articles_map[title] = customWikiArticle(len(self.articles_map), title, realtitle, summary, ambiguous)\n",
        "\n",
        "        if title in self.articles_map and self.articles_map[title].summary == None and force_reload:\n",
        "            self.modified = True\n",
        "            realtitle, summary, ambiguous = self._retrieve_article(title)\n",
        "            self.articles_map[title].summary = summary\n",
        "\n",
        "        return self.articles_map[title]\n",
        "\n",
        "    def _retrieve_article(self, title, closed_list : List = []) -> Tuple[str, str, bool]: \n",
        "        closed_list.append(title)\n",
        "        try:\n",
        "            article = wikipedia.page(title, auto_suggest=False, redirect=True)\n",
        "            return (article.title, article.summary, False)\n",
        "\n",
        "        except wikipedia.PageError as e:\n",
        "            search_result = wikipedia.search(title, suggestion = False)\n",
        "\n",
        "            logging.warning(f\"{title} misspelled or article missing. Best find is {search_result[0]}\")\n",
        "            if search_result[0] is not None and search_result[0] not in closed_list:            \n",
        "                return self._retrieve_article(search_result[0], closed_list)  \n",
        "            else: return (title, None, False)\n",
        "\n",
        "        except wikipedia.DisambiguationError as e:\n",
        "            logging.warning(f\"{title} is ambiguous, trying first {e.options[0]}\")\n",
        "            if e.options[0] is not None and e.options[0] not in closed_list:\n",
        "                res = self._retrieve_article(e.options[0], closed_list)\n",
        "                return (res[0], res[1], True)\n",
        "        \n",
        "            return (None, None, None)\n",
        "\n",
        "\n",
        "    def load_all_articles(self, force_reload : bool = False) -> None:\n",
        "        logging.info(f\"Starting loading articles...\")\n",
        "        nb_success = 0\n",
        "\n",
        "        nb_article = len(self.list_title)\n",
        "        for i, title in tqdm(enumerate(self.list_title), total=nb_article):\n",
        "            self.load_article(title, force_reload)\n",
        "\n",
        "            if self.articles_map[title].summary is not None: \n",
        "                nb_success += 1\n",
        "        logging.info(f\"Finished loading {nb_success} article(s) !\")\n",
        "        return self.modified\n",
        "\n",
        "    def __call__(self, force_reload : bool = True) -> None:\n",
        "        return self.load_all_articles(force_reload)\n",
        "\n",
        "    def get_article(self, title):\n",
        "        if title not in self.articles_map:\n",
        "            self.load_article(title)\n",
        "\n",
        "        return self.articles_map[title]\n",
        "        \n",
        "    def save(self):\n",
        "        with open(self.get_filename(), \"wb\") as mapfile:\n",
        "            pickle.dump(self.articles_map, mapfile)\n",
        "\n",
        "class ArticleViewer():\n",
        "\n",
        "    def __init__(self, filename):\n",
        "        self.name = filename\n",
        "\n",
        "        if not exists(self.name):\n",
        "            raise FileNotFoundError()\n",
        "        else:\n",
        "            with open(self.name, \"rb\") as mapfile:\n",
        "                self.articles_map = pickle.load(mapfile)\n",
        "\n",
        "    def get(self, title):\n",
        "        return self.articles_map[title]\n",
        "\n",
        "    def get_all_articles(self):\n",
        "        return self.articles_map.keys()\n"
      ],
      "metadata": {
        "id": "QZiZ2Pr-lUiS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings operations "
      ],
      "metadata": {
        "id": "bw3z2xRJfeYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WordToVector:\n",
        "\n",
        "    def __init__(self, list_tags : List[str] = []):\n",
        "        self.list_tags = list_tags\n",
        "        self.embeddings = {}\n",
        "\n",
        "    def set_list_class(self, list_class : List[str]):\n",
        "        self.list_tags = list_class\n",
        "        self.reset_embeddings()\n",
        "\n",
        "    def convert(self):\n",
        "        return NotImplementedError\n",
        "\n",
        "    def reset_embeddings(self):\n",
        "        self.embeddings.clear()\n",
        "\n",
        "    def get_embedding_of(self, token):\n",
        "        if token not in self.embeddings:\n",
        "            raise Exception(f\"no such token {token}\")\n",
        "        \n",
        "        return self.embeddings[token]\n",
        "\n",
        "    def get_class_list(self):\n",
        "        return self.embeddings.keys()\n",
        "\n",
        "    def ask_alt(self, word):\n",
        "        print(f\"Unable to find {word} in the dict, choose a replacement : \")\n",
        "        alt = str(input())\n",
        "        return alt\n",
        "\n",
        "    def export(self, filename):\n",
        "        \"\"\"export all the embeddings in filename under a .csv format.\n",
        "           Raise exception if embeddings hasn't been calculed yet.\"\"\"\n",
        "\n",
        "        if len(self.embeddings) == 0:\n",
        "            raise Exception(\"Tags not converted yet !\")\n",
        "        \n",
        "        dict2csv(filename, self.embeddings)\n",
        "\n",
        "class EmbeddingsLoader:\n",
        "\n",
        "    def __init__(self, filename : str):\n",
        "\n",
        "        self.file = filename\n",
        "        self.embeddings = {}\n",
        "\n",
        "        self._load_file()\n",
        "\n",
        "    def _load_file(self):\n",
        "        try:\n",
        "            with open(self.file, \"r\") as f:\n",
        "                lines = f.readlines()\n",
        "                \n",
        "            for line in lines[1:]:\n",
        "                data = line.split(\",\")\n",
        "                self.embeddings[data[0]] = torch.FloatTensor(list(map(float, data[1:])))\n",
        "\n",
        "        except IOError as e:\n",
        "            raise IOError(f\"No file {self.file}\")\n",
        "\n",
        "class SimilarityMatrix(EmbeddingsLoader):\n",
        "\n",
        "    def __init__(self, embeddings : Dict[str, List[float]], strategy : Similarity):\n",
        "        super(SimilarityMatrix, self).__init__(embeddings)\n",
        "        self.strategy = strategy\n",
        "        self._create_matrix()\n",
        "        self.computed : bool = False\n",
        "\n",
        "    def _create_matrix(self) -> None:\n",
        "        n_tokens = len(self.embeddings)\n",
        "        self.cosine_sim_matrix : Dict[Dict[float]] = {}\n",
        "        for tag in self.embeddings.keys():\n",
        "            self.cosine_sim_matrix[tag] = {}\n",
        "\n",
        "    def compute_sim(self) -> None:\n",
        "        \"\"\" compute cosine similarity between all vectors \"\"\"\n",
        "\n",
        "        logging.info(\"Computing cosine similarity, this could take some time...\")\n",
        "        for tag, vector in tqdm(self.embeddings.items(), total = len(self.embeddings)):\n",
        "\n",
        "            for otag, other_vector in self.embeddings.items():\n",
        "\n",
        "                if otag == tag:\n",
        "                    continue\n",
        "\n",
        "                similarity = self.strategy.sim(vector, other_vector)\n",
        "\n",
        "                self.cosine_sim_matrix[otag][tag] = similarity\n",
        "                self.cosine_sim_matrix[tag][otag] = similarity\n",
        "\n",
        "        self.computed = True\n",
        "\n",
        "    def export_sim_matrix(self, filename):\n",
        "        if not self.computed:\n",
        "            self.compute_sim()\n",
        "        \n",
        "        try:\n",
        "            f = open(filename, \"w\")\n",
        "        except OSError:\n",
        "            raise OSError(\"Could not open file\")\n",
        "\n",
        "        with f:\n",
        "            print(\"/\", *[tag for tag in self.embeddings.keys()], sep = \",\", file = f)\n",
        "\n",
        "            for tag in self.embeddings.keys():\n",
        "                print(tag, *[str(round(float(self.cosine_sim_matrix[tag][otag]), 3)) for otag in self.embeddings.keys()], sep = \",\", file = f)\n",
        "\n",
        "    def get_sim_matrix(self) -> Tuple[List[str], List[List[float]]]:\n",
        "        \"\"\"return the similarity matrix of the embeddings\n",
        "        \"\"\"\n",
        "        if not self.computed:\n",
        "            self.compute_sim()\n",
        "\n",
        "        X = len(self.embeddings)\n",
        "        matrix = [[0 for j in range(X)] for i in range(X)]\n",
        "        ids = []\n",
        "        \n",
        "        for i, tag in enumerate(self.embeddings.keys()):\n",
        "            ids.append(tag)\n",
        "            for j, otag in enumerate(self.embeddings.keys()):\n",
        "                if i == j:\n",
        "                    continue\n",
        "\n",
        "                matrix[i][j] = self.cosine_sim_matrix[tag][otag]\n",
        "                matrix[j][i] = self.cosine_sim_matrix[tag][otag]\n",
        "\n",
        "        return ids, matrix\n",
        "\n",
        "    def sim_between(self, token1 : str, token2 : str) -> float:\n",
        "        v1 = self.embeddings[token1]\n",
        "        v2 = self.embeddings[token2]\n",
        "\n",
        "        if token2 not in self.cosine_sim_matrix[token1] or token1 not in self.cosine_sim_matrix[token2]:\n",
        "            similarity = self.strategy.sim(v1, v2)\n",
        "            self.computed = True\n",
        "\n",
        "            self.cosine_sim_matrix[token1][token2] = similarity\n",
        "            self.cosine_sim_matrix[token1][token2] = similarity\n",
        "\n",
        "        return self.cosine_sim_matrix[token1][token2]\n",
        "\n",
        "class SimilarityMatrixFromDict(SimilarityMatrix):\n",
        "\n",
        "    def __init__(self, embeddings : Dict[str, List[float]], strategy : Similarity):\n",
        "        self.embeddings = embeddings\n",
        "        self._create_matrix()\n",
        "        self.strategy = strategy\n",
        "        self.computed : bool = False"
      ],
      "metadata": {
        "id": "5AxDIhr1rkca"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solver"
      ],
      "metadata": {
        "id": "0bRoDbKafoVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Solver(EmbeddingsLoader):\n",
        "\n",
        "    DEFAULT_MIN_LIST_RESULT = 10\n",
        "\n",
        "    def __init__(self, embeddings):\n",
        "        super(Solver, self).__init__(embeddings)\n",
        "\n",
        "    def get_nearest_embedding_of(self, embedding, nb = 10):\n",
        "\n",
        "        if nb > len(self.embeddings):\n",
        "            raise Exception(\"nb too high, not enough token\")\n",
        "\n",
        "        nearest = []\n",
        "        for tag, e in self.embeddings.items():\n",
        "\n",
        "            cos = torch.nn.CosineSimilarity(dim=0)\n",
        "            similarity = cos(embedding, e)\n",
        "\n",
        "            nearest.append((tag, similarity))\n",
        "        \n",
        "        nearest.sort(key = lambda tup : tup[1])\n",
        "        return nearest[-1:-nb-1:-1]\n",
        "\n",
        "    def __call__(self, embeddeding, tag=None):\n",
        "        result = self.get_nearest_embedding_of(embeddeding, min(Solver.DEFAULT_MIN_LIST_RESULT, len(self.embeddings)))\n",
        "        if tag is not None:\n",
        "            print(f\"Nearest Word for {tag}:\")\n",
        "        for i in result:\n",
        "            print(f\"\\t{i[0]:12}: {round(float(i[1]) * 100, 3)}%\")\n",
        "    \n",
        "    def score(self, embedding, target):\n",
        "        target_embeddings = self.embeddings[target]\n",
        "\n",
        "        cos = torch.nn.CosineSimilarity(dim=0)\n",
        "        return float(cos(embedding, target_embeddings))\n",
        "\n",
        "    def least_squared_score(self, embedding, target):\n",
        "        target_embeddings = self.embeddings[target]\n",
        "        return float(np.linalg.norm(target_embeddings - embedding))\n",
        "\n",
        "    def mean_squared_score(self, embedding, target):\n",
        "        target_embeddings = self.embeddings[target]\n",
        "        return float(np.square(np.subtract(embedding, target_embeddings)).mean())\n",
        "        "
      ],
      "metadata": {
        "id": "KpVPJ9IxfnC5"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "nYaqmgXe1-EX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT model"
      ],
      "metadata": {
        "id": "-0g4CJtelYfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTModel(WordToVector):\n",
        "\n",
        "    temp_dir = \"./temp\"\n",
        "\n",
        "    def __init__(self, list_tag : List[str], big: bool = False, window : int = 100):\n",
        "        super(BERTModel, self).__init__(list_tag)\n",
        "        self.window_size = window\n",
        "\n",
        "        self.model_size = \"bert-large-uncased\" if big else \"bert-base-uncased\"\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.model_size, padding=True, truncation=True)\n",
        "        self.model = BertModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "    def convert(self, article_ret : ArticleRetriever):\n",
        "        \"\"\" convert all word in their embeddings\"\"\"\n",
        "\n",
        "        if len(self.list_tags) == 0:\n",
        "            raise Exception(\"no tags yet !\")\n",
        "\n",
        "        logging.info(\"Starting converting tokens...\")\n",
        "        nb_token = len(self.list_tags)\n",
        "        for i, tag in tqdm(enumerate(self.list_tags), total = nb_token):\n",
        "            \n",
        "            if tag in self.embeddings: continue\n",
        "\n",
        "            article = article_ret.get_article(tag)\n",
        "            tag_plus_context = tag\n",
        "            if article.summary is not None:\n",
        "                max_size_article = min(len(article.summary), self.window_size)\n",
        "                tag_plus_context = tag + \". \" + article.summary[:max_size_article]\n",
        "\n",
        "            inputs = self.tokenizer(tag_plus_context, return_tensors = \"pt\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            hidden_states = outputs[2]\n",
        "\n",
        "            # [# layers, # batches, # tokens, # features] ==> [# tokens, # layers, # features]\n",
        "            token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "            token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "            token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "            self.embeddings[tag] = self.merging_strategy.merge(token_embeddings[0])"
      ],
      "metadata": {
        "id": "UVJUSGWQorYj"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RoBERTa model"
      ],
      "metadata": {
        "id": "eGVvWFvtlqLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ROBERTAModel(BERTModel):\n",
        "\n",
        "    def __init__(self, list_tag : List[str], big: bool = False, window : int = 100):\n",
        "        WordToVector.__init__(self, list_tag)\n",
        "        self.window_size = window\n",
        "\n",
        "        self.model_size = \"roberta-large\" if big else \"roberta-base\"\n",
        "\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_size, padding=True, truncation=True)\n",
        "        self.model = RobertaModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()"
      ],
      "metadata": {
        "id": "7oWZOwG-mCYI"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DocBERT model"
      ],
      "metadata": {
        "id": "GqQ1HgNyz-zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DocBERT(BERTModel):\n",
        "\n",
        "    def __init__(self, list_tag : List[str], big : bool = False):\n",
        "        WordToVector.__init__(self, list_tag)\n",
        "        self.window_size = \"document\"\n",
        "\n",
        "        self.model_size = \"bert-large-uncased\" if big else \"bert-base-uncased\"\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.model_size, padding=True, truncation=True)\n",
        "        self.model = BertModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.max_size = self.tokenizer.model_max_length\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "    def _one_pass(self, subinputs):\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids = subinputs[\"input_ids\"], attention_mask = subinputs[\"attention_mask\"])\n",
        "\n",
        "        hidden_states = outputs[2]\n",
        "\n",
        "        # [# layers, # batches, # tokens, # features] ==> [# tokens, # layers, # features]\n",
        "        token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "        token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "        return self.merging_strategy.merge(token_embeddings[0])\n",
        "\n",
        "\n",
        "    def convert(self, article_ret):\n",
        "        \"\"\" convert all word in their embeddings\"\"\"\n",
        "\n",
        "        if len(self.list_tags) == 0:\n",
        "            raise Exception(\"no tags yet !\")\n",
        "\n",
        "        logging.info(\"Starting converting tokens...\")\n",
        "        nb_token = len(self.list_tags)\n",
        "        for i, tag in tqdm(enumerate(self.list_tags), total = nb_token):\n",
        "            \n",
        "            if tag in self.embeddings: continue\n",
        "\n",
        "            article = article_ret.get_article(tag)\n",
        "            tag_plus_context = tag\n",
        "            if article.summary is not None:\n",
        "\n",
        "                torch_cls = []\n",
        "\n",
        "                ids = self.tokenizer.encode(article.summary)\n",
        "                nb_token = len(ids)\n",
        "                nb_pass = nb_token // self.max_size\n",
        "                \n",
        "                nb_pass = nb_token // self.max_size\n",
        "                logging.info(f\"{tag} is {nb_pass * 2} pass\")\n",
        "\n",
        "                for i in range(nb_pass * 2 + 1):\n",
        "                    start = i * (self.max_size // 2)\n",
        "                    stop = min(nb_token, start + self.max_size)\n",
        "                    sub_ids = ids[start:stop]\n",
        "\n",
        "                    subinputs = { \"input_ids\": torch.IntTensor(sub_ids).unsqueeze(0), \\\n",
        "                                  \"token_type_ids\": torch.IntTensor([0 for i in range(len(sub_ids))]).unsqueeze(0), \\\n",
        "                                  \"attention_mask\": torch.IntTensor([1 for i in range(len(sub_ids))]).unsqueeze(0)  }\n",
        "                    torch_cls.append(self._one_pass(subinputs))\n",
        "\n",
        "                self.embeddings[tag] = torch.mean(torch.stack(tuple(t for t in torch_cls)), axis=0)\n",
        "            else:\n",
        "                logging.warning(f\"no article for {tag}\")\n",
        "                self.embeddings[tag] = self._one_pass(self.tokenizer(tag, return_tensors = \"pt\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "nnq0gSYxiIQd"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DocBERTA model"
      ],
      "metadata": {
        "id": "mRkUxi2g0FT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DocBERTA(DocBERT):\n",
        "\n",
        "    def __init__(self, list_tag : List[str], big : bool = False):\n",
        "        WordToVector.__init__(self, list_tag)\n",
        "        self.window_size = \"document\"\n",
        "\n",
        "        self.model_size = \"roberta-large\" if big else \"roberta-base\"\n",
        "\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_size, padding=True, truncation=True)\n",
        "        self.model = RobertaModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.max_size = self.tokenizer.model_max_length\n",
        "\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()"
      ],
      "metadata": {
        "id": "FHdFrESi0Joa"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wikipedia2Vec"
      ],
      "metadata": {
        "id": "mFf6OoBG1xVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Wiki2VecModel(WordToVector):\n",
        "\n",
        "    file = []\n",
        "\n",
        "    def __init__(self, list_tag : List[str], size : int = 300):\n",
        "        WordToVector.__init__(self, list_tag)\n",
        "        self.window_size = size\n",
        "\n",
        "        self.model_size = \"wikipedia2vec\"\n",
        "        assert size in [100, 300, 500], f\"size should be one of this value (100, 300, 500)\"\n",
        "        self.zip_filename = f\"enwiki_20180420_{self.window_size}d.pkl.bz2\"\n",
        "        self.unzip_filename = self.zip_filename[:-4]\n",
        "        \n",
        "        try:\n",
        "            self._load()\n",
        "        except:\n",
        "            return\n",
        "        \n",
        "        self.model = Wikipedia2Vec.load(self.path)\n",
        "\n",
        "    def _load(self):\n",
        "\n",
        "        self.path = abspath(self.unzip_filename)\n",
        "\n",
        "        if exists(self.path):\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            system(f\"wget http://wikipedia2vec.s3.amazonaws.com/models/en/2018-04-20/{self.zip_filename}\")\n",
        "        except:\n",
        "            raise SystemError(f\"can't retrieve the file {self.zip_filename}\")\n",
        "\n",
        "        try:\n",
        "            system(f\"bunzip2 ./{self.zip_filename}\")\n",
        "        except:\n",
        "            raise SystemError(f\"can't unzip the file {self.zip_filename}\")\n",
        "\n",
        "    def _retrieve(self, word):\n",
        "        try:\n",
        "            return self.model.get_word_vector(word)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            return self.model.get_word_vector(word.capitalize())\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            return self.model.get_entity_vector(word)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            return self.model.get_entity_vector(word.capitalize())\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        return None\n",
        "    \n",
        "    def _one_turn(self, resolve_dict = {}):\n",
        "        unk = []\n",
        "\n",
        "        for word in self.list_tags:\n",
        "            w = word.replace(\"_\", \" \")\n",
        "            if w in self.embeddings: continue\n",
        "\n",
        "            if w in resolve_dict:\n",
        "                embed = self._retrieve(resolve_dict[w])\n",
        "            else:\n",
        "                embed = self._retrieve(w)\n",
        "\n",
        "            if embed is None:\n",
        "                logging.warning(f\"{w} cannot be retrieved.\")\n",
        "                unk.append(word)\n",
        "            else:\n",
        "                self.embeddings[w] = torch.from_numpy(embed)\n",
        "        \n",
        "        return unk\n",
        "\n",
        "    def convert(self, ar):\n",
        "        resolve_filename = f\"./temp/{ar.name[:-4]}_resolve.json\"\n",
        "        resolve = {}\n",
        "        while True: \n",
        "            unk_list = self._one_turn(resolve)\n",
        "        \n",
        "            if unk_list is None or len(unk_list) == 0:\n",
        "                break\n",
        "\n",
        "            print(len(unk_list), \"items haven't been found, resolve mode.\")\n",
        "\n",
        "            with open(resolve_filename, 'w') as f:\n",
        "                resolve_dict = {word: \"\" for word in unk_list}\n",
        "                json.dump(resolve_dict, f)\n",
        "\n",
        "            input(\"press enter to resume resolve\")\n",
        "\n",
        "            resolve = {}\n",
        "            with open(resolve_filename, 'r') as f:\n",
        "                resolve = json.load(f)\n",
        "                assert(type(resolve) == type(dict()))"
      ],
      "metadata": {
        "id": "w0zukX6Zk3W5"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "44KuFxQOG0WX",
        "outputId": "e70481ac-b897-4c67-b762-42ed97570b1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5305"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical part"
      ],
      "metadata": {
        "id": "ue8SGNE5iKaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings to word proba"
      ],
      "metadata": {
        "id": "JNFmhVBmUTgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "solver = Solver(\"/content/animal10-roberta-base-0.csv\")\n",
        "\n",
        "totest = solver.embeddings[\"cat\"]\n",
        "\n",
        "solver(totest, \"cat\")\n",
        "print(solver.score(totest, \"dog\"))\n",
        "print(solver.mean_squared_score(totest, \"dog\"))\n",
        "print(solver.least_squared_score(totest, \"dog\"))"
      ],
      "metadata": {
        "id": "tD8XokPDUhkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word to embeddings"
      ],
      "metadata": {
        "id": "LN273RlCUGza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
        "imagenet = list(np.array(open(labels_path).read().splitlines()))\n",
        "subimagenet = imagenet[:200]"
      ],
      "metadata": {
        "id": "NIJ8zsk-UTuh",
        "outputId": "8e4c98d9-1ad2-40a9-bec7-178402d783b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt\n",
            "16384/10484 [==============================================] - 0s 0us/step\n",
            "24576/10484 [======================================================================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "animal10 = [\"dog\", \"cat\", \"horse\", \"spider\", \"butterfly\", \"chicken\", \"sheep\", \"cow\", \"squirrel\", \"elephant\"]\n",
        "cifar10  = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "cifar100 = [\"apple\", \"aquarium_fish\", \"baby\", \"bear\", \"beaver\", \"bed\", \"bee\", \"beetle\", \"bicycle\", \"bottle\", \"bowl\", \"boy\", \"bridge\", \"bus\", \"butterfly\", \"camel\", \"can\", \\\n",
        "            \"castle\", \"caterpillar\", \"cattle\", \"chair\", \"chimpanzee\", \"clock\", \"cloud\", \"cockroach\", \"computer_keyboard\", \"couch\", \"crab\", \"crocodile\", \"cup\", \\\n",
        "            \"dinosaur\", 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'lamp', 'lawn_mower', 'leopard', 'lion', \\\n",
        "            'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', \\\n",
        "            'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark',\\\n",
        "            'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', \\\n",
        "            'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n",
        "\n",
        "king = [\"king\", \"woman\", \"man\", \"queen\", \"boy\", \"girl\", \"male\", \"female\"]\n",
        "custom = []\n",
        "\n",
        "#@title Choose a dataset\n",
        "save_name = \"cifar100\" #@param [\"animal10\", \"cifar10\", \"cifar100\", \"king\", \"imagenet\", \"subimagenet\", \"custom\"]\n",
        "mapping_save_list = {\n",
        "    \"animal10\": animal10,\n",
        "    \"cifar10\" : cifar10,\n",
        "    \"cifar100\" : cifar100,\n",
        "    \"king\" : king,\n",
        "    \"imagenet\" : imagenet,\n",
        "    \"subimagenet\": subimagenet,\n",
        "    \"custom\" : custom\n",
        "}\n",
        "\n",
        "vocab = mapping_save_list[save_name]"
      ],
      "metadata": {
        "id": "EYnbJc84xuh5",
        "cellView": "form"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose a model and params\n",
        "window_size  = 300 #@param [\"0\", \"100\", \"200\", \"300\", \"400\", \"500\"] {type:\"raw\"}\n",
        "is_big       = True #@param {type:\"boolean\"}\n",
        "model_choice = \"BERT\" #@param [\"ROBERTA\", \"BERT\", \"Wikipedia2Vec\", \"DocBERT\", \"DocBERTA\"]\n",
        "\n",
        "if model_choice == \"ROBERT\":\n",
        "    model = ROBERTAModel(vocab, big = is_big, window = window_size)\n",
        "elif model_choice == \"BERT\":\n",
        "    model = BERTModel(vocab, big = is_big, window = window_size)\n",
        "elif model_choice == \"Wikipedia2Vec\":\n",
        "    model = Wiki2VecModel(vocab, size = window_size)\n",
        "elif model_choice == \"DocBERT\":\n",
        "    model = DocBERT(vocab, big = is_big)\n",
        "elif model_choice == \"DocBERTA\":\n",
        "    model = DocBERTA(vocab, big = is_big)\n",
        "    \n",
        "articlesRetriever = ArticleRetriever(save_name + \".art\", vocab)"
      ],
      "metadata": {
        "id": "NvIn_JeFlYfK",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92d1b74b-caa8-47fd-bc74-cbae582bf339"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if articlesRetriever():\n",
        "    articlesRetriever.save()\n",
        "\n",
        "model.reset_embeddings()\n",
        "\n",
        "model.convert(articlesRetriever)\n",
        "\n",
        "csv_file = f\"{save_name}-{model.model_size}-{model.window_size}.csv\"\n",
        "\n",
        "model.export(csv_file)\n",
        "\n",
        "print(\"\\n\", len(model.get_class_list()))"
      ],
      "metadata": {
        "id": "WOOlgG8koVD6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e69fe33-b34a-4d85-f29e-d7c68aa5515b"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 16/100 [00:02<00:14,  5.88it/s]/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n",
            "100%|██████████| 100/100 [00:22<00:00,  4.36it/s]\n",
            "100%|██████████| 100/100 [02:07<00:00,  1.28s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wiki2vec_dict = model.model.dictionary\n",
        "\n",
        "# with open(\"./temp/wiki_article.txt\", \"w\") as f:\n",
        "#     for word in wiki2vec_dict.words():\n",
        "#         print(word.text, file=f)\n",
        "\n",
        "#     for ent in wiki2vec_dict.entities():\n",
        "#         print(ent.title, file=f)"
      ],
      "metadata": {
        "id": "z0qnv2UbfqZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neighboor joining Tree"
      ],
      "metadata": {
        "id": "0WZFTDaGbr1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neighboor_sim = SimilarityMatrix(csv_file, CosineSim())\n",
        "\n",
        "ids, data = neighboor_sim.get_sim_matrix()\n",
        "\n",
        "ids = [tids.replace(\" \", \"_\") for tids in ids]\n",
        "inv_data  = sim2dist(data, lambda x: 1 - x) \n",
        "\n",
        "# print()\n",
        "# print_mat(data_inv, format_function = lambda x: round(float(x), 2))"
      ],
      "metadata": {
        "id": "crcMR8dDuH7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f180c39-e898-462a-c85b-03f2b98547bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 122.41it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dm = DistanceMatrix(inv_data, ids)\n",
        "tree = nj(dm)\n",
        "\n",
        "with open(f\"{save_name}-{model_choice}-.tree\", \"w\") as f:\n",
        "    print(tree, file = f)\n",
        "\n",
        "print(\"\\n\\n\", tree.ascii_art())"
      ],
      "metadata": {
        "id": "_J431TiosB8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wikipedia debug"
      ],
      "metadata": {
        "id": "7jubA6JXUMk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Article to search for { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "totest = \"buck\" #@param {type:\"string\"}\n",
        "\n",
        "result = wikipedia.search(totest, suggestion = False)\n",
        "print(result)\n",
        "print(f\"first result is: {result[0]}\")\n",
        "\n",
        "try:\n",
        "    print(wikipedia.page(totest, auto_suggest=False, redirect=True))\n",
        "    print(wikipedia.page(result[0], auto_suggest=False, redirect=True))\n",
        "except Exception as e:\n",
        "    print(f\"best option envisaged: {e.options[0]}\")\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "PeM8XAu6h5gD",
        "outputId": "147aec66-0c3d-4062-e7f3-c46a0e508c0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Buck', 'Buck buck', 'Joe Buck', 'Young Buck', 'Buck Owens', 'Buck Rogers', 'Buck converter', 'Buck Showalter', 'Uncle Buck', 'Bucking']\n",
            "first result is: Buck\n",
            "best option envisaged: dollar\n",
            "\"buck\" may refer to: \n",
            "dollar\n",
            "List of animal names\n",
            "Derby shoes\n",
            "Buck (nickname)\n",
            "Buck Pierce\n",
            "Buck (surname)\n",
            "Buck 65\n",
            "Buck Angel\n",
            "Buck Dharma\n",
            "Buck Ellison\n",
            "Buck Henry\n",
            "Buck Jones\n",
            "Buck Owens\n",
            "Young Buck\n",
            "David Paul Grove\n",
            "Bunkhouse Buck\n",
            "Buck Quartermain\n",
            "Buck Zumhofe\n",
            "Buck Creek (disambiguation)\n",
            "Buck, Pennsylvania\n",
            "Buck Township, Hardin County, Ohio\n",
            "Buck Township, Luzerne County, Pennsylvania\n",
            "The High Chaparral\n",
            "Chicken Little\n",
            "Buck Frobisher\n",
            "Adventures of Huckleberry Finn\n",
            "Beverly Hills Teens\n",
            "Best in Show\n",
            "Buck Mulligan\n",
            "Chuck & Buck\n",
            "Buck Rogers\n",
            "Uncle Buck\n",
            "Time Squad\n",
            "Dr. Strangelove\n",
            "Cameron \"Buck\" Williams\n",
            "The Call of the Wild\n",
            "Married... with Children\n",
            "Home on the Range\n",
            "187 Ride or Die\n",
            "Wonder Pets\n",
            "Ice Age 3: Dawn of the Dinosaurs\n",
            "The Good Dinosaur\n",
            "Halo 3: ODST\n",
            "Midnight Cowboy\n",
            "USS Buck\n",
            "Buck: A Memoir\n",
            "Buck (cocktail)\n",
            "Buck (crater)\n",
            "Buck (design company)\n",
            "Buck (film)\n",
            "Buck (human resources consulting company)\n",
            "Buck Knives\n",
            "Buck (magazine)\n",
            "Buck (software)\n",
            "Buck (video game)\n",
            "brony\n",
            "krump\n",
            "Buck converter\n",
            "Bucking\n",
            "Bucks (disambiguation)\n",
            "Bucky (disambiguation)\n",
            "Log bucking\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articleviewer = ArticleViewer(\"/content/article/cifar100.art\")\n",
        "print(articleviewer.get_all_articles())\n",
        "articleviewer.get(\"king\").summary"
      ],
      "metadata": {
        "id": "O5sAnE0nuhD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test part"
      ],
      "metadata": {
        "id": "ax8Aw_qVRxer"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## similarity test"
      ],
      "metadata": {
        "id": "Eu9U-_PfeXSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_list(listed):\n",
        "    print(f\"{'word':20}{'target':20}{'similarity':10}\")\n",
        "    print(\"=\"*50)\n",
        "    for ida, idb, sim in listed:\n",
        "        print(f\"{ida:20}{idb:20}{round(float(sim), 6):6}\")\n",
        "\n",
        "def similarity_above(filename, thresold, strat = CosineSim()):\n",
        "\n",
        "    sim_matrix = SimilarityMatrix(filename, strat)\n",
        "    ids, cosine_mat = sim_matrix.get_sim_matrix()\n",
        "\n",
        "    sim_list = []\n",
        "    closed_list = []\n",
        "\n",
        "    for i, idsa in tqdm(enumerate(ids), total = len(ids)):\n",
        "        for j, idsb in enumerate(ids):\n",
        "\n",
        "            if i == j: continue \n",
        "            if (i, j) in closed_list or (j, i) in closed_list: continue\n",
        "\n",
        "            cos_val = cosine_mat[i][j]\n",
        "\n",
        "            # if cos_val >= thresold:\n",
        "            sim_list.append((idsa, idsb, cos_val))\n",
        "            \n",
        "            closed_list.append((i, j))\n",
        "            closed_list.append((j, i))\n",
        "\n",
        "    sim_list.sort(key=lambda x: x[2], reverse = True)\n",
        "    return len(list(filter(lambda x: x[2] > thresold, sim_list))), sim_list\n",
        "\n"
      ],
      "metadata": {
        "id": "Kiq-UPEApL0U"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"/content/cifar100-bert-large-uncased-300.csv\"\n",
        "\n",
        "result = similarity_above(filename, 0.90)\n",
        "\n",
        "print(result[0])\n",
        "\n",
        "# print_list(result[1][:result[0]+1])\n"
      ],
      "metadata": {
        "id": "4JUE701weWzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e683ca32-cc19-48da-f5f4-93b7fb0659f4"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 158.73it/s]\n",
            "100%|██████████| 100/100 [00:02<00:00, 42.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Different result for King - men + women equation with different context window size"
      ],
      "metadata": {
        "id": "uUU3kl9sbmg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "\n",
        "save_name = \"king-test\"\n",
        "vocab = [\"king\", \"queen\", \"men\", \"woman\"]\n",
        "\n",
        "# king_test_model = ROBERTAModel(vocab, big = False, window = 0)\n",
        "try:\n",
        "    king_test_model = Wiki2VecModel(vocab)\n",
        "except:\n",
        "    print(\"haha\")\n",
        "\n",
        "king_test_articlesRetriever = ArticleRetriever(save_name + \".art\", vocab)\n",
        "if king_test_articlesRetriever():\n",
        "    king_test_articlesRetriever.save()\n",
        "\n",
        "result = king_test_model.convert(king_test_articlesRetriever)\n",
        "\n",
        "if result is not None and len(result) > 0:\n",
        "    print(result)\n",
        "king_test_model.export(save_name + \".csv\")\n",
        "king_test_solver = Solver(save_name + \".csv\")\n",
        "\n",
        "men = king_test_model.get_embedding_of(\"men\")\n",
        "woman = king_test_model.get_embedding_of(\"woman\")\n",
        "king = king_test_model.get_embedding_of(\"king\")\n",
        "\n",
        "print(type(men), type(woman), type(king))\n",
        "#totest = king - men + woman\n",
        "input(\"debug\")\n",
        "print()\n",
        "king_test_solver(totest, \"King - man + woman\")"
      ],
      "metadata": {
        "id": "8uNXNj_wCWhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| model | window size | rank of Queen | distance with first |\n",
        "|-------|-------------|---------------|-----------|\n",
        "| bert-large | 0  | 2 |  .0693 |\n",
        "| bert-large | 10 | 3 |  .1191 | \n",
        "| bert-large | 50 | 2 |  .1672 |\n",
        "| bert-large | 100 | 2 | .1275 |\n",
        "| bert-large | 150 | 3 | .1134 |\n",
        "| bert-large | 200 | 3 | .1923 |\n",
        "| bert-large | 300 | 3 | .0939 |\n",
        "| bert-large | 400 | 3 | .1455 |\n",
        "| roberta-large | 0 | 2 | .0001 |\n",
        "| roberta-large | 10 | 3 | .0011 |\n",
        "| roberta-large | 50 | 2 | .0029 |\n",
        "| roberta-large | 100 | 4 | .0061 |\n",
        "| roberta-large | 150 | 3 | .0023 |\n",
        "| roberta-large | 200 | 4 | .0055 |\n",
        "| roberta-large | 300 | 4 | .0127 |\n",
        "| roberta-large | 400 | 4 | .0045 |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kY8yWSmJw6Ss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 0.1656 (1.8208E-03)\n",
        "* 0.3361 (9.6914E-11) DocBERT big\n",
        "* 0.1349 (1.1278E-02) DocBERTa small"
      ],
      "metadata": {
        "id": "1HdvjW_cwqg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test superclass"
      ],
      "metadata": {
        "id": "ols0X2IDmqJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Test:\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "    \n",
        "    def _start(self, model):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _end(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __call__(self, model):\n",
        "\n",
        "        logging.info(f\"Start test {self.name}\")\n",
        "\n",
        "        tic = perf_counter()\n",
        "        self._start(model)\n",
        "        result = self._end(model)\n",
        "        toc = perf_counter()\n",
        "\n",
        "        logging.info(f\"End test {self.name}\")\n",
        "\n",
        "        return result, toc - tic"
      ],
      "metadata": {
        "id": "OQcNCqBSmo9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tests "
      ],
      "metadata": {
        "id": "AW1u-xshmuZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wordsim353"
      ],
      "metadata": {
        "id": "QDP8gaRAm1s5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimilarityTest(Test):\n",
        "\n",
        "    def __init__(self, pair_set : Tuple[str, str, float], pair_name):\n",
        "        Test.__init__(self, pair_name)\n",
        "\n",
        "        self.vocab = []\n",
        "        self.pair : Tuple[str, str, float] = pair_set\n",
        "\n",
        "        for w1, w2, i in self.pair:\n",
        "            if w1 not in self.vocab:\n",
        "                self.vocab.append(w1)\n",
        "            if w2 not in self.vocab:\n",
        "                self.vocab.append(w2)\n",
        "    \n",
        "\n",
        "    def _start(self, model : WordToVector):\n",
        "        articlesRetriever = ArticleRetriever(f\"{self.name}.art\", self.vocab)\n",
        "\n",
        "        if articlesRetriever(force_reload = True):\n",
        "            articlesRetriever.save()\n",
        "\n",
        "        self.save_file = f\"test-{self.name}-{model.model_size}.csv\"\n",
        "        model.set_list_class(self.vocab)\n",
        "        model.convert(articlesRetriever)\n",
        "        model.export(self.save_file)\n",
        "\n",
        "    def _end(self, model):\n",
        "        total_comparison = 0\n",
        "        sim_list = []\n",
        "        i_list = []\n",
        "\n",
        "        sim_computer = SimilarityMatrix(self.save_file, CosineSim())\n",
        "\n",
        "        for w1, w2, i in self.pair:\n",
        "            sim = sim_computer.sim_between(w1, w2)\n",
        "\n",
        "            sim_list.append(sim)\n",
        "            i_list.append(i)\n",
        "\n",
        "            total_comparison += 1\n",
        "\n",
        "        result = spearmanr(sim_list, i_list)\n",
        "        return (result.correlation, result.pvalue)"
      ],
      "metadata": {
        "id": "zMBHN_B5bQym"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.data."
      ],
      "metadata": {
        "id": "IpTdHoIKuAEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline"
      ],
      "metadata": {
        "id": "JzRqbWiHm4mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestPipeline():\n",
        "\n",
        "    list_test = [ SimilarityTest(nlp.data.WordSim353('all'), \"Wordsim353\"), \n",
        "                  SimilarityTest(nlp.data.SimLex999('all') , \"SimLex999\")  ]\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def execute(self):\n",
        "\n",
        "        for i, test in enumerate(TestPipeline.list_test):\n",
        "\n",
        "            print(f\"Test {i} : {test.name}\".center(40, \"=\"))\n",
        "            res, time_elapsed = test(self.model)\n",
        "            print(f\"{res}\")\n",
        "            print(f\"{round(time_elapsed, 2)} sec.\".center(40, \"=\"))"
      ],
      "metadata": {
        "id": "SlsqEiRqm8dx"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERTTest = TestPipeline(BERTModel([], big = False, window = 0))\n",
        "WikiTest = TestPipeline(Wiki2VecModel([]))\n",
        "\n",
        "# BERTTest.execute()\n",
        "WikiTest.execute()"
      ],
      "metadata": {
        "id": "DhjlqisSgL7x",
        "outputId": "31cda331-faeb-4909-94fc-3da48a8a0cb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========Test 0 : Wordsim353===========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/437 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n",
            "100%|██████████| 437/437 [00:21<00:00, 20.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 items haven't been found, resolve mode.\n",
            "press enter to resume resolve\n",
            "(0.6913260432400101, 2.4595594898063737e-51)\n",
            "===============114.8 sec.===============\n",
            "===========Test 1 : SimLex999===========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1028/1028 [00:20<00:00, 50.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.38146730019142955, 5.923684029073775e-36)\n",
            "===============23.91 sec.===============\n"
          ]
        }
      ]
    }
  ]
}