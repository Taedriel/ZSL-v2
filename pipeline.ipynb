{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1OR6E3sgk-F1D3bS7Da1dyPBZYeidS0Kr",
      "authorship_tag": "ABX9TyMq3EdIe+Ib95BcYYKkDEYp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taedriel/ZSL-v2/blob/googleImageFSL/pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxCGioBVtE6f"
      },
      "outputs": [],
      "source": [
        "!pip install selenium\n",
        "!pip install scikit-learn-intelex\n",
        "!pip install easyfsl\n",
        "!pip install Pillow\n",
        "\n",
        "!cd \"./drive/MyDrive/Colab Notebooks/pipeline\" && pip install -r requirements.txt\n",
        "\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, numpy as np\n",
        "import os, itertools, shutil, requests, webbrowser, urllib.request, time\n",
        "\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import Omniglot\n",
        "from torchvision.models import resnet18\n",
        "from tqdm import tqdm\n",
        "\n",
        "import PIL.Image\n",
        "from IPython.core.display import Image\n",
        "import random\n",
        "\n",
        "from easyfsl.samplers import TaskSampler\n",
        "from easyfsl.utils import plot_images, sliding_average"
      ],
      "metadata": {
        "id": "xMwX0RACteAV"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"/content/drive/MyDrive/Colab Notebooks/pipeline/\"\n",
        "PATH_IMAGES = \"/content/drive/MyDrive/Colab Notebooks/pipeline/images/\"\n",
        "PATH_MODEL = \"/content/drive/MyDrive/Colab Notebooks/pipeline/model/\"\n",
        "\n",
        "globalSize = 20\n",
        "LEN_FOR_ONE_SCROLL = 20\n",
        "retreivalNotDone = False\n",
        "image_size = 100\n",
        "trainingNeeded = False"
      ],
      "metadata": {
        "id": "lfOI7oJptom3"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classesFile = open(PATH_MODEL+\"animals.txt\")\n",
        "index=0\n",
        "\n",
        "classes = \"\"\n",
        "classesTr = []\n",
        "classesTe = []\n",
        "classeForSimilarity = []\n",
        "\n",
        "for animal in classesFile.readlines():\n",
        "\n",
        "  animal = animal.strip(\"\\n\")\n",
        "\n",
        "  if animal!=\"list\":\n",
        "    classes+=animal+\", \"\n",
        "\n",
        "    if index%2:\n",
        "      classesTe.append(animal)\n",
        "    else:\n",
        "      classesTr.append(animal)\n",
        "\n",
        "    index+=1\n",
        "\n",
        "ridx = random.randint(0, len(classesTe))\n",
        "classeForSimilarity.append(classesTe[ridx])\n",
        "classesTe.remove(classeForSimilarity[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfqxDwYmjW-S",
        "outputId": "d12795b7-746d-4f20-a431-64b0f36cb689"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "448\n",
            "449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- IMAGE RETRIEVAL ---"
      ],
      "metadata": {
        "id": "jSc4ewvWt-rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "thumbnail / jpg (compression) may be an issue\n",
        "\"\"\"\n",
        "\n",
        "def getParser(classeName):\n",
        "\n",
        "  site = 'https://www.google.com/search?tbm=isch&q='+classeName\n",
        "\n",
        "  chrome_options = webdriver.ChromeOptions()\n",
        "  chrome_options.add_argument('--headless')\n",
        "  chrome_options.add_argument('--no-sandbox')\n",
        "  chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "  driver = webdriver.Chrome('chromedriver', options=chrome_options)\n",
        "  driver.get(site)\n",
        "  driver.execute_script(\"window.scrollBy(0, document.body.scrollHeight)\")\n",
        "  soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "  driver.close()\n",
        "\n",
        "  return soup\n",
        "\n",
        "\n",
        "def getClassesImagesURLLIB():\n",
        "\n",
        "  imagesNumber = 0\n",
        "\n",
        "  shutil.rmtree(PATH_IMAGES, ignore_errors=False)\n",
        "  os.makedirs(PATH_IMAGES)\n",
        "\n",
        "  for classe in classes.split(\",\"):\n",
        "\n",
        "    try:\n",
        "      classeName = classe.replace(\" \", \"\")\n",
        "      os.makedirs(PATH_IMAGES+classeName)\n",
        "\n",
        "      soup = getParser(classeName)\n",
        "      img_tags = soup.find_all(\"img\", class_=\"rg_i\")\n",
        "      for index in range(0, len(img_tags)):\n",
        "        try:\n",
        "            urllib.request.urlretrieve(img_tags[index]['src'], PATH_IMAGES+classeName+\"/\"+str(classeName+str(index))+\".jpg\")\n",
        "            imagesNumber+=1\n",
        "        except Exception as e:\n",
        "            pass\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  print(str(imagesNumber) + \" images were downloaded. \" + str(imagesNumber/len(classes.split(\",\"))) + \" per classes\")"
      ],
      "metadata": {
        "id": "62epbrFRtxhG"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if retreivalNotDone:\n",
        "  getClassesImagesURLLIB()\n",
        "  retreivalNotDone = False"
      ],
      "metadata": {
        "id": "I33Xnwvzt6HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "--- PROTOTYPICAL NETWORK ---"
      ],
      "metadata": {
        "id": "1yxyHTPMuGR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PrototypicalNetworks(nn.Module):\n",
        "    def __init__(self, backbone: nn.Module):\n",
        "        super(PrototypicalNetworks, self).__init__()\n",
        "        self.backbone = backbone\n",
        "\n",
        "    def forward(self, support_images: torch.Tensor, support_labels: torch.Tensor, query_images: torch.Tensor,) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Predict query labels using labeled support images.\n",
        "        \"\"\"\n",
        "        z_support = self.backbone.forward(support_images)\n",
        "        z_query = self.backbone.forward(query_images)\n",
        "\n",
        "        n_way = len(torch.unique(support_labels))\n",
        "        z_proto = torch.cat(\n",
        "            [\n",
        "                z_support[torch.nonzero(support_labels == label)].mean(0)\n",
        "                for label in range(n_way)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        dists = torch.cdist(z_query, z_proto)\n",
        "        scores = -dists\n",
        "        #print(\"d(query, prototype) = \", dists)\n",
        "        return scores\n",
        "\n",
        "\n",
        "convolutional_network = resnet18(pretrained=False)\n",
        "convolutional_network.fc = nn.Flatten()\n",
        "model = PrototypicalNetworks(convolutional_network).cuda()\n",
        "\n",
        "try:\n",
        "  model.load_state_dict(torch.load(PATH_MODEL+\"proto_model.pt\"))\n",
        "  print(\"model loaded from disk\")\n",
        "except:\n",
        "  print(\"couldn't find a model in the specified folder. Do you have the right path ? [The model is in training configuration]\")\n",
        "  trainingNeeded = True"
      ],
      "metadata": {
        "id": "r3ejVC8tu5Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ZSLDataset(Dataset):\n",
        "  \n",
        "    def __init__(self, path, classesList, sizePerClass, transform=None):\n",
        "      self.path = path\n",
        "      self.classes = classesList\n",
        "      self.innerSize = sizePerClass\n",
        "      self.transform = transform\n",
        "\n",
        "      self.samples = []\n",
        "      self.get_labels = lambda : []\n",
        "      for className in self.classes:\n",
        "        images = os.listdir(self.path+className)\n",
        "        for filename in images:\n",
        "          self.samples.append((filename, self.classes.index(className)))\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "      info = self.samples[idx]\n",
        "      filename, classIndex = info\n",
        "      image = PIL.Image.open(self.path+self.classes[classIndex]+\"/\"+filename, mode=\"r\")\n",
        "      if self.transform:\n",
        "        image = self.transform(image)\n",
        "      return image, classIndex"
      ],
      "metadata": {
        "id": "SAITXIwtt7Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataManager:\n",
        "\n",
        "  def __init__(self, data, N_WAY, N_SHOT, N_QUERY, N_EPISODES):\n",
        "    self.data = data\n",
        "    self.data.get_labels = lambda: [instance[1] for instance in data.samples]\n",
        "\n",
        "    self.sampler = TaskSampler(self.data, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_EPISODES)\n",
        "    self.loader = DataLoader(self.data, batch_sampler=self.sampler, num_workers=12, pin_memory=True, collate_fn=self.sampler.episodic_collate_fn,)\n",
        "\n",
        "    (\n",
        "    self.example_support_images,\n",
        "    self.example_support_labels,\n",
        "    self.example_query_images,\n",
        "    self.example_query_labels,\n",
        "    self.example_class_ids,\n",
        "    ) = next(iter(self.loader))\n",
        "\n",
        "  def loadNextIter(self):\n",
        "    (\n",
        "    self.example_support_images,\n",
        "    self.example_support_labels,\n",
        "    self.example_query_images,\n",
        "    self.example_query_labels,\n",
        "    self.example_class_ids,\n",
        "    ) = next(iter(self.loader))"
      ],
      "metadata": {
        "id": "f-ovtLiUxvF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformFunctionTrain = transform=transforms.Compose(\n",
        "        [\n",
        "            transforms.Grayscale(num_output_channels=3),\n",
        "            transforms.RandomResizedCrop(image_size),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "transformFunctionTest = transforms.Compose(\n",
        "        [\n",
        "            # images have 1 channel, but our model will expect 3-channel images\n",
        "            transforms.Grayscale(num_output_channels=3),\n",
        "            transforms.Resize([int(image_size * 1.15), int(image_size * 1.15)]),\n",
        "            transforms.CenterCrop(image_size),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "dataTr = ZSLDataset(PATH_IMAGES, classesTr, 20, transformFunctionTrain)\n",
        "dataTe = ZSLDataset(PATH_IMAGES, classesTe, 20, transformFunctionTest)\n",
        "dataTeval = ZSLDataset(PATH_IMAGES, classeForSimilarity, 20, transformFunctionTest)"
      ],
      "metadata": {
        "id": "Lot6HHhAuW8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_WAY_tr = 5  \n",
        "N_SHOT_tr = 5\n",
        "N_QUERY_tr = 10 \n",
        "\n",
        "N_WAY_te = 1  \n",
        "N_SHOT_te = 19 \n",
        "N_QUERY_te = 1 \n",
        "\n",
        "N_EVALUATION_TASKS = 1000\n",
        "N_TRAINING_EPISODES = 10000\n",
        "N_VALIDATION_TASKS = 100\n",
        "\n",
        "dataTrManager = DataManager(dataTr, N_WAY_tr, N_SHOT_tr, N_QUERY_tr, N_TRAINING_EPISODES)\n",
        "dataTeManager = DataManager(dataTe, N_WAY_tr, N_SHOT_tr, N_QUERY_tr, N_EVALUATION_TASKS)\n",
        "dataTevalManager = DataManager(dataTeval, N_WAY_te, N_SHOT_te, N_QUERY_te, 20)"
      ],
      "metadata": {
        "id": "tYDvY8iAvAD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_images(dataTrManager.example_support_images, \"support images training\", images_per_row=N_SHOT_tr)\n",
        "plot_images(dataTrManager.example_query_images, \"query images traning\", images_per_row=N_QUERY_tr)\n",
        "\n",
        "plot_images(dataTeManager.example_support_images, \"support images test\", images_per_row=N_SHOT_tr)\n",
        "plot_images(dataTeManager.example_query_images, \"query images test\", images_per_row=N_QUERY_tr)"
      ],
      "metadata": {
        "id": "wQU0XwYGycSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# those two followings are the meta training\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "def fit(\n",
        "    support_images: torch.Tensor,\n",
        "    support_labels: torch.Tensor,\n",
        "    query_images: torch.Tensor,\n",
        "    query_labels: torch.Tensor,\n",
        ") -> float:\n",
        "    optimizer.zero_grad()\n",
        "    classification_scores = model(\n",
        "        support_images.cuda(), support_labels.cuda(), query_images.cuda()\n",
        "    )\n",
        "\n",
        "    loss = criterion(classification_scores, query_labels.cuda())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "def trainOnSet(dataset):\n",
        "  log_update_frequency = 10\n",
        "  all_loss = []\n",
        "  model.train()\n",
        "  with tqdm(enumerate(dataset.loader), total=len(dataset.loader)) as tqdm_train:\n",
        "      for episode_index, (\n",
        "          support_images,\n",
        "          support_labels,\n",
        "          query_images,\n",
        "          query_labels,\n",
        "          _,\n",
        "      ) in tqdm_train:\n",
        "          loss_value = fit(support_images, support_labels, query_images, query_labels)\n",
        "          all_loss.append(loss_value)\n",
        "\n",
        "          if episode_index % log_update_frequency == 0:\n",
        "              tqdm_train.set_postfix(loss=sliding_average(all_loss, log_update_frequency))\n",
        "\n",
        "  torch.save(model.state_dict(), PATH_MODEL+\"proto_model.pt\")"
      ],
      "metadata": {
        "id": "6W1LCGL6yyEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if trainingNeeded:\n",
        "  trainOnSet(dataTrManager)\n",
        "else:\n",
        "  model.eval()\n",
        "\n",
        "  example_scores = model(\n",
        "    dataTeManager.example_support_images.cuda(),\n",
        "    dataTeManager.example_support_labels.cuda(),\n",
        "    dataTeManager.example_query_images.cuda(),\n",
        "  ).detach()\n",
        "\n",
        "  _, example_predicted_labels = torch.max(example_scores.data, 1)\n",
        "  data = dataTeManager.data\n",
        "\n",
        "  print(\"Ground Truth / Predicted\")\n",
        "  for i in range(len(dataTeManager.example_query_labels)):\n",
        "    print(\n",
        "        f\"{data.classes[dataTeManager.example_class_ids[dataTeManager.example_query_labels[i]]]} / {data.classes[dataTeManager.example_class_ids[example_predicted_labels[i]]]}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "0cPmMp1-y09b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_on_one_task(support_images: torch.Tensor, support_labels: torch.Tensor, query_images: torch.Tensor, query_labels: torch.Tensor,) -> [int, int]:\n",
        "  \n",
        "    \"\"\"\n",
        "    Returns the number of correct predictions of query labels, and the total number of predictions.\n",
        "    \"\"\"\n",
        "    similarity = model(support_images.cuda(), support_labels.cuda(), query_images.cuda()).detach().data\n",
        "    return (torch.max(similarity,1,)[1] == query_labels.cuda()).sum().item(), len(query_labels), similarity\n",
        "\n",
        "\n",
        "def evaluate(data_loader: DataLoader):\n",
        "    # We'll count everything and compute the ratio at the end\n",
        "    total_predictions = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    # eval mode affects the behaviour of some layers (such as batch normalization or dropout)\n",
        "    # no_grad() tells torch not to keep in memory the whole computational graph (it's more lightweight this way)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for episode_index, (\n",
        "            support_images,\n",
        "            support_labels,\n",
        "            query_images,\n",
        "            query_labels,\n",
        "            class_ids,\n",
        "        ) in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "\n",
        "            correct, total, _ = evaluate_on_one_task(support_images, support_labels, query_images, query_labels)\n",
        "\n",
        "            total_predictions += total\n",
        "            correct_predictions += correct\n",
        "\n",
        "    print(\n",
        "        f\"Model tested on {len(data_loader)} tasks. Accuracy: {(100 * correct_predictions/total_predictions):.2f}%\"\n",
        "    )"
      ],
      "metadata": {
        "id": "s47Wfsxvzepw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(dataTeManager.loader)"
      ],
      "metadata": {
        "id": "N1JJkQw6zmdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_images(dataTevalManager.example_support_images, \"support images test\", images_per_row=N_SHOT_te)\n",
        "\n",
        "for i in range(0, LEN_FOR_ONE_SCROLL):\n",
        "\n",
        "  _, _, similarity = evaluate_on_one_task(dataTevalManager.example_support_images, dataTevalManager.example_support_labels, dataTevalManager.example_query_images, dataTevalManager.example_query_labels)\n",
        "  plot_images(dataTevalManager.example_query_images, \"query images test has similarity of \"+str(2*torch.sigmoid(-similarity)-1), images_per_row=N_QUERY_te)\n",
        "  dataTevalManager.loadNextIter()"
      ],
      "metadata": {
        "id": "tZUF3mMvzkUy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}