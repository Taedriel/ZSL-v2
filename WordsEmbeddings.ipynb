{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordsEmbeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taedriel/ZSL-v2/blob/wordEmbedding/WordsEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yes | pip install transformers wget tensorflow_datasets wikipedia unzip mxnet gluonnlp \"scipy>=1.7\" scikit-bio wikipedia2vec --quiet --upgrade\n",
        "!mkdir -p temp article"
      ],
      "metadata": {
        "id": "HEq14T6Nl535"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "import wikipedia\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "\n",
        "import gluonnlp as nlp\n",
        "import traceback\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, RobertaModel, RobertaTokenizer\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level = logging.INFO, filename = \"BERT.log\" )\n",
        "# logging.basicConfig(level = logging.INFO)\n",
        "\n",
        "from tqdm import tqdm\n",
        "from typing import List, Tuple, Dict\n",
        "from os.path import exists, join\n",
        "from enum import Enum\n",
        "\n",
        "wikipedia.set_rate_limiting(True)"
      ],
      "metadata": {
        "id": "spXWSv2iW9i5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils classes"
      ],
      "metadata": {
        "id": "kjt7fI8Olf5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class customWikiArticle:\n",
        "\n",
        "    def __init__(self, index : int, title : str, realtitle : str, summary : str, ambiguous : bool):\n",
        "        self.index = index\n",
        "        self.title = title\n",
        "        self.realtitle = realtitle\n",
        "        self.summary = summary\n",
        "        self.ambiguous = ambiguous\n",
        "\n",
        "class ArticleRetriever:\n",
        "\n",
        "    article_dir = \"./article\"\n",
        "\n",
        "    def __init__(self, name : str, list_title : List[str]):\n",
        "\n",
        "        self.name = name\n",
        "        self.list_title = list_title\n",
        "        self.modified = False\n",
        "\n",
        "        if not exists(self.get_filename()):\n",
        "            self.articles_map = {}\n",
        "        else:\n",
        "            with open(self.get_filename(), \"rb\") as mapfile:\n",
        "                self.articles_map = pickle.load(mapfile)\n",
        "\n",
        "    def get_filename(self):\n",
        "        return join(ArticleRetriever.article_dir,self.name)\n",
        "\n",
        "    def load_article(self, title, force_reload : bool = False) -> customWikiArticle:\n",
        "        if title not in self.articles_map:\n",
        "            self.modified = True\n",
        "            realtitle, summary, ambiguous = self._retrieve_article(title)\n",
        "            self.articles_map[title] = customWikiArticle(len(self.articles_map), title, realtitle, summary, ambiguous)\n",
        "\n",
        "        if title in self.articles_map and self.articles_map[title].summary == None and force_reload:\n",
        "            self.modified = True\n",
        "            realtitle, summary, ambiguous = self._retrieve_article(title)\n",
        "            self.articles_map[title].summary = summary\n",
        "\n",
        "        return self.articles_map[title]\n",
        "\n",
        "    def _retrieve_article(self, title, closed_list : List = []) -> Tuple[str, str, bool]: \n",
        "        closed_list.append(title)\n",
        "        try:\n",
        "            article = wikipedia.page(title, auto_suggest=False, redirect=True)\n",
        "            return (article.title, article.summary, False)\n",
        "\n",
        "        except wikipedia.PageError as e:\n",
        "            search_result = wikipedia.search(title, suggestion = False)\n",
        "\n",
        "            logging.warning(f\"{title} misspelled or article missing. Best find is {search_result[0]}\")\n",
        "            if search_result[0] is not None and search_result[0] not in closed_list:            \n",
        "                return self._retrieve_article(search_result[0], closed_list)  \n",
        "            else: return (title, None, False)\n",
        "\n",
        "        except wikipedia.DisambiguationError as e:\n",
        "            logging.warning(f\"{title} is ambiguous, trying first {e.options[0]}\")\n",
        "            if e.options[0] is not None and e.options[0] not in closed_list:\n",
        "                res = self._retrieve_article(e.options[0], closed_list)\n",
        "                return (res[0], res[1], True)\n",
        "        \n",
        "            return (None, None, None)\n",
        "\n",
        "\n",
        "    def load_all_articles(self, force_reload : bool = False) -> None:\n",
        "        logging.info(f\"Starting loading articles...\")\n",
        "        nb_success = 0\n",
        "\n",
        "        nb_article = len(self.list_title)\n",
        "        for i, title in tqdm(enumerate(self.list_title), total=nb_article):\n",
        "            self.load_article(title, force_reload)\n",
        "\n",
        "            if self.articles_map[title].summary is not None: \n",
        "                nb_success += 1\n",
        "        logging.info(f\"Finished loading {nb_success} article(s) !\")\n",
        "        return self.modified\n",
        "\n",
        "    def __call__(self, force_reload : bool = True) -> None:\n",
        "        return self.load_all_articles(force_reload)\n",
        "\n",
        "    def get_article(self, title):\n",
        "        if title not in self.articles_map:\n",
        "            self.load_article(title)\n",
        "\n",
        "        return self.articles_map[title]\n",
        "        \n",
        "    def save(self):\n",
        "        with open(self.get_filename(), \"wb\") as mapfile:\n",
        "            pickle.dump(self.articles_map, mapfile)\n",
        "\n",
        "class ArticleViewer():\n",
        "\n",
        "    def __init__(self, filename):\n",
        "        self.name = filename\n",
        "\n",
        "        if not exists(self.name):\n",
        "            raise FileNotFoundError()\n",
        "        else:\n",
        "            with open(self.name, \"rb\") as mapfile:\n",
        "                self.articles_map = pickle.load(mapfile)\n",
        "\n",
        "    def get(self, title):\n",
        "        return self.articles_map[title]\n",
        "\n",
        "    def get_all_articles(self):\n",
        "        return self.articles_map.keys()\n",
        "\n",
        "class WordToVector:\n",
        "\n",
        "    def __init__(self, list_tags : List[str] = []):\n",
        "        self.list_tags = list_tags\n",
        "        self.embeddings = {}\n",
        "\n",
        "    def export(self, filename):\n",
        "        return NotImplementedError\n",
        "\n",
        "    def convert(self):\n",
        "        return NotImplementedError\n",
        "\n",
        "class Sum4LastLayers:\n",
        "\n",
        "    def merge(self, vector):\n",
        "        return torch.sum(vector[-4:], dim = 0)\n"
      ],
      "metadata": {
        "id": "QZiZ2Pr-lUiS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingsLoader:\n",
        "\n",
        "    def __init__(self, filename : str):\n",
        "\n",
        "        self.file = filename\n",
        "        self.embeddings = {}\n",
        "\n",
        "        self._load_file()\n",
        "\n",
        "    def _load_file(self):\n",
        "        try:\n",
        "            with open(self.file, \"r\") as f:\n",
        "                lines = f.readlines()\n",
        "                \n",
        "            for line in lines[1:]:\n",
        "                data = line.split(\",\")\n",
        "                self.embeddings[data[0]] = torch.FloatTensor(list(map(float, data[1:])))\n",
        "\n",
        "        except IOError as e:\n",
        "            raise IOError(f\"No file {self.file}\")\n",
        "\n",
        "class SimilarityCompute(EmbeddingsLoader):\n",
        "\n",
        "    def __init__(self, embeddings):\n",
        "        super(SimilarityCompute, self).__init__(embeddings)\n",
        "        self._create_matrix()\n",
        "        self.computed : bool = False\n",
        "\n",
        "    def _create_matrix(self) -> None:\n",
        "        n_tokens = len(self.embeddings)\n",
        "        self.cosine_sim_matrix : Dict[Dict[float]] = {}\n",
        "        for tag in self.embeddings.keys():\n",
        "            self.cosine_sim_matrix[tag] = {}\n",
        "\n",
        "    def compute_sim(self) -> None:\n",
        "        \"\"\" compute cosine similarity between all vectors \"\"\"\n",
        "\n",
        "        logging.info(\"Computing cosine similarity, this could take some time...\")\n",
        "        for tag, vector in tqdm(self.embeddings.items(), total = len(self.embeddings)):\n",
        "\n",
        "            for otag, other_vector in self.embeddings.items():\n",
        "\n",
        "                if otag == tag:\n",
        "                    continue\n",
        "\n",
        "                cos = torch.nn.CosineSimilarity(dim=0)\n",
        "                similarity = cos(vector, other_vector)\n",
        "\n",
        "                self.cosine_sim_matrix[otag][tag] = similarity\n",
        "                self.cosine_sim_matrix[tag][otag] = similarity\n",
        "\n",
        "        self.computed = True\n",
        "\n",
        "    def export_sim_matrix(self, filename):\n",
        "        if not self.computed:\n",
        "            self.compute_sim()\n",
        "        \n",
        "        try:\n",
        "            f = open(filename, \"w\")\n",
        "        except OSError:\n",
        "            raise OSError(\"Could not open file\")\n",
        "\n",
        "        with f:\n",
        "            print(\"/\", *[tag for tag in self.embeddings.keys()], sep = \",\", file = f)\n",
        "\n",
        "            for tag in self.embeddings.keys():\n",
        "                print(tag, *[str(round(float(self.cosine_sim_matrix[tag][otag]), 3)) for otag in self.embeddings.keys()], sep = \",\", file = f)\n",
        "\n",
        "    def get_sim_matrix(self) -> Tuple[List[str], List[List[float]]]:\n",
        "        \"\"\"return the similarity matrix of the embeddings\n",
        "        \"\"\"\n",
        "        if not self.computed:\n",
        "            self.compute_sim()\n",
        "\n",
        "        X = len(self.embeddings)\n",
        "        matrix = [[0 for j in range(X)] for i in range(X)]\n",
        "        ids = []\n",
        "        \n",
        "        for i, tag in enumerate(self.embeddings.keys()):\n",
        "            ids.append(tag)\n",
        "            for j, otag in enumerate(self.embeddings.keys()):\n",
        "                if i == j:\n",
        "                    continue\n",
        "\n",
        "                matrix[i][j] = self.cosine_sim_matrix[tag][otag]\n",
        "                matrix[j][i] = self.cosine_sim_matrix[tag][otag]\n",
        "\n",
        "        return ids, matrix\n",
        "\n",
        "    def sim_between(self, token1 : str, token2 : str) -> float:\n",
        "        v1 = self.embeddings[token1]\n",
        "        v2 = self.embeddings[token2]\n",
        "\n",
        "        if token2 not in self.cosine_sim_matrix[token1] or token1 not in self.cosine_sim_matrix[token2]:\n",
        "            cos = torch.nn.CosineSimilarity(dim=0)\n",
        "            similarity = cos(v1, v2)\n",
        "\n",
        "            self.cosine_sim_matrix[token1][token2] = similarity\n",
        "            self.cosine_sim_matrix[token1][token2] = similarity\n",
        "\n",
        "        return self.cosine_sim_matrix[token1][token2]\n",
        "\n",
        "class SimilaritcyComputeFromDict(SimilarityCompute):\n",
        "\n",
        "    def __init__(self, embeddings : Dict[str, List[float]]):\n",
        "        self.embeddings = embeddings\n",
        "        self._create_matrix()\n",
        "        self.computed : bool = False\n",
        "\n",
        "\n",
        "class Solver(EmbeddingsLoader):\n",
        "\n",
        "    DEFAULT_MIN_LIST_RESULT = 10\n",
        "\n",
        "    def __init__(self, embeddings):\n",
        "        super(Solver, self).__init__(embeddings)\n",
        "\n",
        "    def get_nearest_embedding_of(self, embedding, nb = 10):\n",
        "\n",
        "        if nb > len(self.embeddings):\n",
        "            raise Exception(\"nb too high, not enough token\")\n",
        "\n",
        "        nearest = []\n",
        "        for tag, e in self.embeddings.items():\n",
        "\n",
        "            cos = torch.nn.CosineSimilarity(dim=0)\n",
        "            similarity = cos(embedding, e)\n",
        "\n",
        "            nearest.append((tag, similarity))\n",
        "        \n",
        "        nearest.sort(key = lambda tup : tup[1])\n",
        "        return nearest[-1:-nb-1:-1]\n",
        "\n",
        "    def __call__(self, embeddeding, tag=None):\n",
        "        result = self.get_nearest_embedding_of(embeddeding, min(Solver.DEFAULT_MIN_LIST_RESULT, len(self.embeddings)))\n",
        "        if tag is not None:\n",
        "            print(f\"Nearest Word for {tag}:\")\n",
        "        for i in result:\n",
        "            print(f\"\\t{i[0]:12}: {round(float(i[1]) * 100, 3)}%\")\n",
        "    \n",
        "    def score(self, embedding, target):\n",
        "        target_embeddings = self.embeddings[target]\n",
        "\n",
        "        cos = torch.nn.CosineSimilarity(dim=0)\n",
        "        return float(cos(embedding, target_embeddings))\n",
        "\n",
        "    def least_squared_score(self, embedding, target):\n",
        "        target_embeddings = self.embeddings[target]\n",
        "        return float(np.linalg.norm(target_embeddings - embedding))\n",
        "\n",
        "    def mean_squared_score(self, embedding, target):\n",
        "        target_embeddings = self.embeddings[target]\n",
        "        return float(np.square(np.subtract(embedding, target_embeddings)).mean())\n",
        "        "
      ],
      "metadata": {
        "id": "5AxDIhr1rkca"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "nYaqmgXe1-EX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT model"
      ],
      "metadata": {
        "id": "-0g4CJtelYfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTModel(WordToVector):\n",
        "\n",
        "    temp_dir = \"./temp\"\n",
        "\n",
        "    def __init__(self, list_tag : List[str], big: bool = False, window : int = 100):\n",
        "        super(BERTModel, self).__init__(list_tag)\n",
        "        self.list_articles = {}\n",
        "        self.window_size = window\n",
        "\n",
        "        self.model_size = \"bert-large-uncased\" if big else \"bert-base-uncased\"\n",
        "        self.cosine_sim_matrix = None\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.model_size, padding=True, truncation=True,)\n",
        "        self.model = BertModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "    def export(self, filename):\n",
        "        \"\"\"export all the embeddings in filename under a .csv format.\n",
        "           Raise exception if embeddings hasn't been calculed yet.\"\"\"\n",
        "\n",
        "        if len(self.embeddings) == 0:\n",
        "            raise Exception(\"Tags not converted yet !\")\n",
        "        \n",
        "        try:\n",
        "            f = open(filename, \"w\")\n",
        "        except OSError:\n",
        "            raise OSError(\"Could not open file\")\n",
        "\n",
        "        dimension_number = len(next(iter(self.embeddings.values())))\n",
        "        with f:\n",
        "            print(\"embeddings\", *[str(i) for i in range(dimension_number)], sep=\",\", file=f)\n",
        "            for tag, embedding in self.embeddings.items():\n",
        "                print(tag, *list(map(lambda x: str(float(x)), embedding)), sep=\",\", file=f)\n",
        "\n",
        "    def reset_embeddings(self):\n",
        "        self.embeddings.clear()\n",
        "\n",
        "    def convert(self, article_ret : ArticleRetriever):\n",
        "        \"\"\" convert all word in their embeddings\"\"\"\n",
        "\n",
        "        if len(self.list_tags) == 0:\n",
        "            raise Exception(\"not tags yet !\")\n",
        "\n",
        "        logging.info(\"Starting converting tokens...\")\n",
        "        nb_token = len(self.list_tags)\n",
        "        for i, tag in tqdm(enumerate(self.list_tags), total = nb_token):\n",
        "            \n",
        "            article = article_ret.get_article(tag)\n",
        "            tag_plus_context = tag\n",
        "            if article.summary is not None:\n",
        "                max_size_article = min(len(article.summary), self.window_size)\n",
        "                tag_plus_context = tag + \". \" + article.summary[:max_size_article]\n",
        "\n",
        "            inputs = self.tokenizer(tag_plus_context, return_tensors = \"pt\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            hidden_states = outputs[2]\n",
        "\n",
        "            # [# layers, # batches, # tokens, # features] ==> [# tokens, # layers, # features]\n",
        "            token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "            token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "            token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "            # apply different strategy to summarize word embeddings\n",
        "            # tokenized_text = self.tokenizer.tokenize(tag)\n",
        "            # acc = []\n",
        "            # for i, token in reversed(list(enumerate(tokenized_text))):\n",
        "\n",
        "            #     embed = self.merging_strategy.merge(token_embeddings[i+1])\n",
        "            #     if i == 0:\n",
        "            #         if len(acc) != 0:\n",
        "            #             embed = torch.mean(torch.stack([x[1] for x in acc]), dim=0)\n",
        "            #             token = tag\n",
        "            #             acc = []\n",
        "\n",
        "            #         self.embeddings.append((token, embed))\n",
        "            #     else:\n",
        "            #         acc.append((token, embed))\n",
        "            \n",
        "            # here we are just taking the [CLS] (for classification) as an embedding for the tag\n",
        "            self.embeddings[tag] = self.merging_strategy.merge(token_embeddings[0])\n",
        "\n",
        "    def get_embedding_of(self, token):\n",
        "        if token not in self.embeddings:\n",
        "            raise Exception(f\"no such token {token}\")\n",
        "        \n",
        "        return self.embeddings[token]\n",
        "\n",
        "    def get_class_list(self):\n",
        "        return self.embeddings.keys()"
      ],
      "metadata": {
        "id": "UVJUSGWQorYj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RoBERTa model"
      ],
      "metadata": {
        "id": "eGVvWFvtlqLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ROBERTAModel(BERTModel):\n",
        "\n",
        "    def __init__(self, list_tag : List[str], big: bool = False, window : int = 100):\n",
        "        WordToVector.__init__(self, list_tag)\n",
        "        self.window_size = window\n",
        "\n",
        "        self.model_size = \"roberta-large\" if big else \"roberta-base\"\n",
        "        self.cosine_sim_matrix = None\n",
        "\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_size, padding=True, truncation=True,)\n",
        "        self.model = RobertaModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()"
      ],
      "metadata": {
        "id": "7oWZOwG-mCYI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wikipedia2Vec"
      ],
      "metadata": {
        "id": "mFf6OoBG1xVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://wikipedia2vec.s3.amazonaws.com/models/en/2018-04-20/enwiki_20180420_300d.pkl.bz2\n",
        "!bunzip2 ./enwiki_20180420_300d.pkl.bz2"
      ],
      "metadata": {
        "id": "tB7Odt1813if",
        "outputId": "7f46b58c-2932-421f-a34d-9f9d37026126",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-26 10:29:35--  http://wikipedia2vec.s3.amazonaws.com/models/en/2018-04-20/enwiki_20180420_500d.pkl.bz2\n",
            "Resolving wikipedia2vec.s3.amazonaws.com (wikipedia2vec.s3.amazonaws.com)... 52.219.17.17\n",
            "Connecting to wikipedia2vec.s3.amazonaws.com (wikipedia2vec.s3.amazonaws.com)|52.219.17.17|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17294111805 (16G) [application/x-bzip2]\n",
            "Saving to: ‘enwiki_20180420_500d.pkl.bz2’\n",
            "\n",
            "enwiki_20180420_500 100%[===================>]  16.11G  18.8MB/s    in 15m 3s  \n",
            "\n",
            "2022-05-26 10:44:38 (18.3 MB/s) - ‘enwiki_20180420_500d.pkl.bz2’ saved [17294111805/17294111805]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from wikipedia2vec import Wikipedia2Vec\n",
        "wiki2vec = Wikipedia2Vec.load(\"./enwiki_20180420_300d.pkl.bz2\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ipqWYMmFmtjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "wiki2vec_embed = {}\n",
        "for word in vocab:\n",
        "\n",
        "    embed = wiki2vec.get_word_vector(word)\n",
        "    wiki2vec_embed[word] = torch.from_numpy(embed)"
      ],
      "metadata": {
        "id": "ngUD39cpxGCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neighboor_sim = SimilaritcyComputeFromDict(wiki2vec_embed)\n",
        "\n",
        "ids, data = neighboor_sim.get_sim_matrix()\n",
        "\n",
        "dm = DistanceMatrix(data, ids)\n",
        "tree = nj(dm)\n",
        "print(\"\\n\\n\", tree.ascii_art())"
      ],
      "metadata": {
        "id": "HHaMhKyEx_te",
        "outputId": "65c6cc75-dd0a-4dbe-88a1-d806e2c2897f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 187.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "                     /-bird\n",
            "          /--------|\n",
            "         |         |          /-dog\n",
            "         |          \\--------|\n",
            "         |                    \\-automobile\n",
            "         |\n",
            "         |                    /-truck\n",
            "         |          /--------|\n",
            "         |         |          \\-frog\n",
            "---------|---------|\n",
            "         |         |          /-horse\n",
            "         |          \\--------|\n",
            "         |                   |          /-cat\n",
            "         |                    \\--------|\n",
            "         |                              \\-airplane\n",
            "         |\n",
            "         |          /-ship\n",
            "          \\--------|\n",
            "                    \\-deer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings to word proba"
      ],
      "metadata": {
        "id": "JNFmhVBmUTgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "solver = Solver(\"/content/animal10-roberta-base-0.csv\")\n",
        "\n",
        "totest = solver.embeddings[\"cat\"]\n",
        "\n",
        "solver(totest, \"cat\")\n",
        "print(solver.score(totest, \"dog\"))\n",
        "print(solver.mean_squared_score(totest, \"dog\"))\n",
        "print(solver.least_squared_score(totest, \"dog\"))"
      ],
      "metadata": {
        "id": "tD8XokPDUhkp",
        "outputId": "d0b06fcc-0172-4f66-eba0-b9928eb0db51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest Word for cat:\n",
            "\tcat         : 100.0%\n",
            "\tdog         : 99.998%\n",
            "\thorse       : 99.998%\n",
            "\tcow         : 99.998%\n",
            "\tsquirrel    : 99.996%\n",
            "\tspider      : 99.996%\n",
            "\tsheep       : 99.996%\n",
            "\tchicken     : 99.996%\n",
            "\tcomputer    : 99.995%\n",
            "\tbutterfly   : 99.994%\n",
            "0.9999799728393555\n",
            "0.0004379133752081543\n",
            "0.5799288749694824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word to embeddings"
      ],
      "metadata": {
        "id": "LN273RlCUGza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
        "imagenet = list(np.array(open(labels_path).read().splitlines()))\n",
        "subimagenet = imagenet[:200]"
      ],
      "metadata": {
        "id": "NIJ8zsk-UTuh"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "animal10 = [\"dog\", \"cat\", \"horse\", \"spider\", \"butterfly\", \"chicken\", \"sheep\", \"cow\", \"squirrel\", \"elephant\"]\n",
        "cifar10  = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "cifar100 = [\"apple\", \"aquarium_fish\", \"baby\", \"bear\", \"beaver\", \"bed\", \"bee\", \"beetle\", \"bicycle\", \"bottle\", \"bowl\", \"boy\", \"bridge\", \"bus\", \"butterfly\", \"camel\", \"can\", \\\n",
        "            \"castle\", \"caterpillar\", \"cattle\", \"chair\", \"chimpanzee\", \"clock\", \"cloud\", \"cockroach\", \"computer_keyboard\", \"couch\", \"crab\", \"crocodile\", \"cup\", \\\n",
        "            \"dinosaur\", 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'lamp', 'lawn_mower', 'leopard', 'lion', \\\n",
        "            'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', \\\n",
        "            'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark',\\\n",
        "            'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', \\\n",
        "            'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n",
        "\n",
        "king = [\"king\", \"woman\", \"man\", \"queen\", \"boy\", \"girl\", \"male\", \"female\"]"
      ],
      "metadata": {
        "id": "EYnbJc84xuh5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose a dataset\n",
        "save_name = \"imagenet\" #@param [\"animal10\", \"cifar10\", \"cifar100\", \"king\", \"imagenet\", \"subimagenet\"]\n",
        "mapping_save_list = {\n",
        "    \"animal10\": animal10,\n",
        "    \"cifar10\" : cifar10,\n",
        "    \"cifar100\" : cifar100,\n",
        "    \"king\" : king,\n",
        "    \"imagenet\" : imagenet,\n",
        "    \"subimagenet\": subimagenet\n",
        "}\n",
        "\n",
        "vocab = mapping_save_list[save_name]\n",
        "\n",
        "# model = BERTModel(model, big = True, window = 100)\n",
        "model = ROBERTAModel(vocab, big = False, window = 0)\n",
        "\n",
        "articlesRetriever = ArticleRetriever(save_name + \".art\", vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvIn_JeFlYfK",
        "outputId": "06830407-8d90-466a-fe1b-fffd49fed15a"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if articlesRetriever():\n",
        "    articlesRetriever.save()\n",
        "\n",
        "model.reset_embeddings()\n",
        "model.convert(articlesRetriever)\n",
        "csv_file = f\"{save_name}-{model.model_size}-{model.window_size}.csv\"\n",
        "model.export(csv_file)\n",
        "print(\"\\n\", len(model.get_class_list()))"
      ],
      "metadata": {
        "id": "WOOlgG8koVD6",
        "outputId": "d51f1a2d-dde2-4ede-e204-a859853c0b78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1001 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n",
            "100%|██████████| 1001/1001 [00:04<00:00, 210.44it/s]\n",
            "100%|██████████| 1001/1001 [01:27<00:00, 11.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neighboor joining Tree"
      ],
      "metadata": {
        "id": "0WZFTDaGbr1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import SpearmanRConstantInputWarning\n",
        "from skbio import DistanceMatrix\n",
        "from skbio.tree import nj"
      ],
      "metadata": {
        "id": "SPQXoPQEbrWS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neighboor_sim = SimilarityCompute(csv_file)\n",
        "\n",
        "ids, data = neighboor_sim.get_sim_matrix()\n",
        "\n",
        "ids = [tids.replace(\" \", \"_\") for tids in ids]\n",
        "\n",
        "dm = DistanceMatrix(data, ids)\n",
        "tree = nj(dm)\n",
        "print(\"\\n\\n\", tree.ascii_art())"
      ],
      "metadata": {
        "id": "crcMR8dDuH7s",
        "outputId": "29b976c7-c6f2-431b-81dd-b92655c1a44c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 158.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "                                         /-man\n",
            "                              /--------|\n",
            "                             |          \\-flatfish\n",
            "                    /--------|\n",
            "                   |         |          /-crab\n",
            "                   |          \\--------|\n",
            "                   |                   |          /-willow tree\n",
            "                   |                    \\--------|\n",
            "                   |                              \\-castle\n",
            "                   |\n",
            "                   |                              /-whale\n",
            "          /--------|                    /--------|\n",
            "         |         |                   |          \\-cloud\n",
            "         |         |          /--------|\n",
            "         |         |         |         |          /-tractor\n",
            "         |         |         |          \\--------|\n",
            "         |         |         |                   |          /-sweet pepper\n",
            "         |         |         |                    \\--------|\n",
            "         |         |         |                              \\-chair\n",
            "         |          \\--------|\n",
            "         |                   |                    /-orchid\n",
            "         |                   |          /--------|\n",
            "         |                   |         |         |          /-shrew\n",
            "         |                   |         |          \\--------|\n",
            "         |                   |         |                    \\-rocket\n",
            "         |                    \\--------|\n",
            "         |                             |                    /-kangaroo\n",
            "         |                             |          /--------|\n",
            "         |                             |         |          \\-bed\n",
            "         |                              \\--------|\n",
            "         |                                       |          /-telephone\n",
            "         |                                        \\--------|\n",
            "         |                                                 |          /-rose\n",
            "         |                                                  \\--------|\n",
            "         |                                                            \\-palm tree\n",
            "         |\n",
            "         |                                        /-trout\n",
            "         |                              /--------|\n",
            "         |                             |         |          /-skyscraper\n",
            "         |                             |          \\--------|\n",
            "         |                             |                    \\-orange\n",
            "         |                    /--------|\n",
            "         |                   |         |                    /-worm\n",
            "         |                   |         |          /--------|\n",
            "         |                   |         |         |          \\-couch\n",
            "         |                   |          \\--------|\n",
            "         |                   |                   |          /-dinosaur\n",
            "         |                   |                    \\--------|\n",
            "         |                   |                              \\-beetle\n",
            "         |                   |\n",
            "         |                   |                              /-tiger\n",
            "         |          /--------|                    /--------|\n",
            "         |         |         |                   |         |          /-sunflower\n",
            "         |         |         |                   |          \\--------|\n",
            "         |         |         |          /--------|                    \\-clock\n",
            "         |         |         |         |         |\n",
            "         |         |         |         |         |          /-bottle\n",
            "         |         |         |         |          \\--------|\n",
            "         |         |         |         |                   |          /-computer keyboard\n",
            "         |         |         |         |                    \\--------|\n",
            "         |         |         |         |                              \\-can\n",
            "         |         |          \\--------|\n",
            "         |         |                   |                    /-pear\n",
            "         |         |                   |          /--------|\n",
            "         |         |                   |         |         |          /-lamp\n",
            "         |         |                   |         |          \\--------|\n",
            "         |         |                   |         |                   |          /-turtle\n",
            "         |         |                   |         |                    \\--------|\n",
            "         |         |                    \\--------|                              \\-bus\n",
            "         |         |                             |\n",
            "         |         |                             |                    /-cattle\n",
            "         |         |                             |          /--------|\n",
            "         |         |                             |         |         |          /-oak tree\n",
            "         |         |                             |         |          \\--------|\n",
            "         |         |                              \\--------|                    \\-mouse\n",
            "         |         |                                       |\n",
            "         |         |                                       |          /-spider\n",
            "         |         |                                        \\--------|\n",
            "         |         |                                                 |          /-butterfly\n",
            "         |         |                                                  \\--------|\n",
            "         |---------|                                                            \\-bowl\n",
            "         |         |\n",
            "         |         |                                        /-bridge\n",
            "         |         |                              /--------|\n",
            "         |         |                             |          \\-beaver\n",
            "         |         |                    /--------|\n",
            "         |         |                   |         |          /-fox\n",
            "         |         |                   |          \\--------|\n",
            "         |         |                   |                    \\-crocodile\n",
            "         |         |                   |\n",
            "         |         |          /--------|                    /-seal\n",
            "         |         |         |         |          /--------|\n",
            "         |         |         |         |         |         |          /-raccoon\n",
            "         |         |         |         |         |          \\--------|\n",
            "         |         |         |         |         |                    \\-cup\n",
            "         |         |         |          \\--------|\n",
            "         |         |         |                   |                    /-ray\n",
            "---------|         |         |                   |          /--------|\n",
            "         |         |         |                   |         |          \\-aquarium fish\n",
            "         |         |         |                    \\--------|\n",
            "         |         |         |                             |          /-tank\n",
            "         |         |         |                              \\--------|\n",
            "         |         |         |                                        \\-porcupine\n",
            "         |         |         |\n",
            "         |         |         |                                        /-table\n",
            "         |         |         |                              /--------|\n",
            "         |         |         |                             |          \\-poppy\n",
            "         |         |         |                    /--------|\n",
            "         |         |         |                   |         |          /-snail\n",
            "         |          \\--------|                   |          \\--------|\n",
            "         |                   |                   |                   |          /-elephant\n",
            "         |                   |                   |                    \\--------|\n",
            "         |                   |          /--------|                              \\-boy\n",
            "         |                   |         |         |\n",
            "         |                   |         |         |                    /-wolf\n",
            "         |                   |         |         |          /--------|\n",
            "         |                   |         |         |         |          \\-mountain\n",
            "         |                   |         |          \\--------|\n",
            "         |                   |         |                   |          /-leopard\n",
            "         |                   |         |                    \\--------|\n",
            "         |                   |         |                             |          /-pickup truck\n",
            "         |                   |         |                              \\--------|\n",
            "         |                   |         |                                        \\-bear\n",
            "         |                   |         |\n",
            "         |                   |         |                                        /-shark\n",
            "         |                   |         |                              /--------|\n",
            "         |                   |         |                             |          \\-possum\n",
            "         |                   |         |                    /--------|\n",
            "         |                    \\--------|                   |         |          /-rabbit\n",
            "         |                             |                   |          \\--------|\n",
            "         |                             |                   |                    \\-forest\n",
            "         |                             |          /--------|\n",
            "         |                             |         |         |                    /-lion\n",
            "         |                             |         |         |          /--------|\n",
            "         |                             |         |         |         |          \\-camel\n",
            "         |                             |         |         |         |\n",
            "         |                             |         |          \\--------|                    /-train\n",
            "         |                             |         |                   |          /--------|\n",
            "         |                             |         |                   |         |          \\-maple tree\n",
            "         |                             |         |                    \\--------|\n",
            "         |                             |         |                             |          /-television\n",
            "         |                             |         |                              \\--------|\n",
            "         |                             |         |                                       |          /-hamster\n",
            "         |                             |         |                                        \\--------|\n",
            "         |                              \\--------|                                                  \\-girl\n",
            "         |                                       |\n",
            "         |                                       |                              /-squirrel\n",
            "         |                                       |                    /--------|\n",
            "         |                                       |                   |          \\-road\n",
            "         |                                       |          /--------|\n",
            "         |                                       |         |         |          /-wardrobe\n",
            "         |                                       |         |          \\--------|\n",
            "         |                                       |         |                   |          /-woman\n",
            "         |                                       |         |                    \\--------|\n",
            "         |                                       |         |                              \\-lobster\n",
            "         |                                       |         |\n",
            "         |                                       |         |                              /-cockroach\n",
            "         |                                        \\--------|                    /--------|\n",
            "         |                                                 |                   |         |          /-tulip\n",
            "         |                                                 |                   |          \\--------|\n",
            "         |                                                 |                   |                   |          /-chimpanzee\n",
            "         |                                                 |          /--------|                    \\--------|\n",
            "         |                                                 |         |         |                              \\-caterpillar\n",
            "         |                                                 |         |         |\n",
            "         |                                                 |         |         |          /-motorcycle\n",
            "         |                                                 |         |          \\--------|\n",
            "         |                                                  \\--------|                    \\-bee\n",
            "         |                                                           |\n",
            "         |                                                           |                    /-plate\n",
            "         |                                                           |          /--------|\n",
            "         |                                                           |         |          \\-lawn mower\n",
            "         |                                                            \\--------|\n",
            "         |                                                                     |          /-skunk\n",
            "         |                                                                      \\--------|\n",
            "         |                                                                                \\-baby\n",
            "         |\n",
            "         |                              /-streetcar\n",
            "         |                    /--------|\n",
            "         |                   |          \\-plain\n",
            "         |          /--------|\n",
            "         |         |         |          /-snake\n",
            "         |         |          \\--------|\n",
            "         |         |                   |          /-lizard\n",
            "         |         |                    \\--------|\n",
            "          \\--------|                              \\-house\n",
            "                   |\n",
            "                   |                    /-otter\n",
            "                   |          /--------|\n",
            "                   |         |          \\-dolphin\n",
            "                   |         |\n",
            "                    \\--------|                    /-sea\n",
            "                             |          /--------|\n",
            "                             |         |          \\-mushroom\n",
            "                              \\--------|\n",
            "                                       |          /-bicycle\n",
            "                                        \\--------|\n",
            "                                                 |          /-pine tree\n",
            "                                                  \\--------|\n",
            "                                                            \\-apple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wikipedia debug"
      ],
      "metadata": {
        "id": "7jubA6JXUMk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Article to search for { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "totest = \"king\" #@param {type:\"string\"}\n",
        "\n",
        "result = wikipedia.search(totest, suggestion = False)\n",
        "print(result)\n",
        "print(f\"first result is: {result[0]}\")\n",
        "\n",
        "try:\n",
        "    print(wikipedia.page(totest, auto_suggest=False, redirect=True))\n",
        "    print(wikipedia.page(result[0], auto_suggest=False, redirect=True))\n",
        "except Exception as e:\n",
        "    print(f\"best option envisaged: {e.options[0]}\")\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "PeM8XAu6h5gD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3d0a8cf-88bf-4da4-a111-1bc05d416cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['King', 'King (disambiguation)', 'Martin Luther King Jr.', 'Stephen King', 'King & King', 'George VI', 'The Lion King', 'Burger King', 'King King', 'List of King of the Hill episodes']\n",
            "first result is: King\n",
            "<WikipediaPage 'King'>\n",
            "<WikipediaPage 'King'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articleviewer = ArticleViewer(\"/content/article/cifar100.art\")\n",
        "print(articleviewer.get_all_articles())\n",
        "articleviewer.get(\"king\").summary"
      ],
      "metadata": {
        "id": "O5sAnE0nuhD_",
        "outputId": "3c5f78a6-a6b8-4753-aec0-dcc36af8648b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'computer_keyboard', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-18eff26b951e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0marticleviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArticleViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/article/cifar100.art\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticleviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0marticleviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"king\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-42125d763f18>\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, title)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marticles_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_all_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'king'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test part"
      ],
      "metadata": {
        "id": "ax8Aw_qVRxer"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Different result for King - men + women equation with different context window size"
      ],
      "metadata": {
        "id": "uUU3kl9sbmg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "\n",
        "save_name = \"king-test\"\n",
        "vocab = [\"King\", \"Queen\", \"men\", \"woman\"]\n",
        "\n",
        "king_test_model = ROBERTAModel(vocab, big = False, window = 0)\n",
        "\n",
        "king_test_articlesRetriever = ArticleRetriever(save_name + \".art\", vocab)\n",
        "if king_test_articlesRetriever():\n",
        "    king_test_articlesRetriever.save()\n",
        "\n",
        "\n",
        "king_test_model.convert(articlesRetriever)\n",
        "king_test_model.export(save_name + \".csv\")\n",
        "king_test_solver = Solver(save_name + \".csv\")\n",
        "\n",
        "\n",
        "men = king_test_model.get_embedding_of(\"men\")\n",
        "woman = king_test_model.get_embedding_of(\"woman\")\n",
        "king = king_test_model.get_embedding_of(\"King\")\n",
        "\n",
        "totest = king.sub(men).add(woman)\n",
        "print()\n",
        "king_test_solver(totest, \"King - man + woman\")"
      ],
      "metadata": {
        "id": "8uNXNj_wCWhf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fd06ee0-e343-4768-d5ca-6f393923a658"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 4/4 [00:00<00:00, 28197.00it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00,  9.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Nearest Word for King - man + woman:\n",
            "\tKing        : 99.998%\n",
            "\twoman       : 99.997%\n",
            "\tmen         : 99.993%\n",
            "\tQueen       : 99.992%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordsim353 = nlp.data.WordSim353('all')"
      ],
      "metadata": {
        "id": "dnkI3j0V-CkE",
        "outputId": "fa5fcc29-215a-4190-f9e7-2a7ac01297c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading /root/.mxnet/datasets/wordsim353/ws353simrel.tar.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/ws353/ws353simrel.tar.gz...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| model | window size | rank of Queen | distance with first |\n",
        "|-------|-------------|---------------|-----------|\n",
        "| bert-large | 0  | 2 |  .0693 |\n",
        "| bert-large | 10 | 3 |  .1191 | \n",
        "| bert-large | 50 | 2 |  .1672 |\n",
        "| bert-large | 100 | 2 | .1275 |\n",
        "| bert-large | 150 | 3 | .1134 |\n",
        "| bert-large | 200 | 3 | .1923 |\n",
        "| bert-large | 300 | 3 | .0939 |\n",
        "| bert-large | 400 | 3 | .1455 |\n",
        "| roberta-large | 0 | 2 | .0001 |\n",
        "| roberta-large | 10 | 3 | .0011 |\n",
        "| roberta-large | 50 | 2 | .0029 |\n",
        "| roberta-large | 100 | 4 | .0061 |\n",
        "| roberta-large | 150 | 3 | .0023 |\n",
        "| roberta-large | 200 | 4 | .0055 |\n",
        "| roberta-large | 300 | 4 | .0127 |\n",
        "| roberta-large | 400 | 4 | .0045 |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kY8yWSmJw6Ss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pearson correlation rank with different context window size"
      ],
      "metadata": {
        "id": "0hjQnEodbcAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "\n",
        "save_name = \"wordsim353\"\n",
        "vocab = []\n",
        "\n",
        "for w1, w2, i in wordsim353:\n",
        "    if w1 not in vocab:\n",
        "        vocab.append(w1)\n",
        "    if w2 not in vocab:\n",
        "        vocab.append(w2)\n",
        "\n",
        "print(len(vocab))\n",
        "wordsim353_model = ROBERTAModel(vocab, big = False, window = 0)\n",
        "articlesRetriever = ArticleRetriever(save_name + \".art\", vocab)\n",
        "\n",
        "if articlesRetriever(force_reload = True):\n",
        "    articlesRetriever.save()\n",
        "\n",
        "wordsim353_model.convert(articlesRetriever)\n",
        "wordsim353_model.export(save_name + \".csv\")"
      ],
      "metadata": {
        "id": "YmFXzvYYBVzf",
        "outputId": "c57990fa-255b-48d5-e6c2-9cc87998d94c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "  0%|          | 0/437 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n",
            "100%|██████████| 437/437 [01:17<00:00,  5.63it/s]\n",
            "100%|██████████| 437/437 [00:31<00:00, 13.83it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "total_comparison = 0\n",
        "sim_list = []\n",
        "i_list = []\n",
        "\n",
        "sim_computer = SimilarityCompute(save_name + \".csv\")\n",
        "\n",
        "for w1, w2, i in wordsim353:\n",
        "    # print(f\"how much {w1} is similar to {w2}:\")\n",
        "    sim = sim_computer.sim_between(w1, w2)\n",
        "\n",
        "    sim_list.append(sim)\n",
        "    i_list.append(i)\n",
        "\n",
        "    # print(f\"\\t{sim} & {i}\")\n",
        "    total_comparison += 1\n",
        "\n",
        "\n",
        "print(spearmanr(sim_list, i_list))"
      ],
      "metadata": {
        "id": "Vwy1yo7iIgnd",
        "outputId": "02c96d7a-1a43-44b7-d1da-676b541e8cee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpearmanrResult(correlation=0.20122918054751185, pvalue=0.00014416169395114014)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "| model | corpus | test set |window size | pearson rank correlation |\n",
        "|-------|--------|----------|------------|--------------------------|\n",
        "| bert large | article from wordsim vocab | wordsim353 | 100 | 0.2158 (5.8353e-05)|\n",
        "| bert base  | article from wordsim vocab | wordsim353 | 100 | 0.2284 (6.8215e-05)|\n",
        "| bert-base  | article from wordsim vocav | wordsim353 | 300 | 0.1326 (1.27e-02) |\n",
        "| bert-large  | article from wordsim vocav | wordsim353 | 300 | 0.2117 (6.2153e-05) |\n",
        "| bert-large  | article from wordsim vocav | wordsim353 | 0 | 0.2638 (5.1232-07) |\n",
        "| bert-base  | article from wordsim vocav | wordsim353 | 0 | 0.3721 (5.2306-13) |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-VuUxJc6zumT"
      }
    }
  ]
}