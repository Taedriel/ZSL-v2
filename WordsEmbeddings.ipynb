{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordsEmbeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taedriel/ZSL-v2/blob/wordEmbedding/WordsEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation part"
      ],
      "metadata": {
        "id": "bYtbXyM-iE1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import stuff\n"
      ],
      "metadata": {
        "id": "7fqDjGPZE1hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yes | pip install transformers wget \"wikipedia>=1.4.0\" unzip mxnet gluonnlp \"scipy>=1.7\" scikit-bio wikipedia2vec orange3 python-louvain networkx --quiet --upgrade\n",
        "!mkdir -p temp article"
      ],
      "metadata": {
        "id": "HEq14T6Nl535"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!yes | pip uninstall community"
      ],
      "metadata": {
        "id": "ZPNwzLOMnWW9",
        "outputId": "b82be1d6-6bc7-4f8c-f20a-28b51b988356",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping community as it is not installed.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as  np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, RobertaModel, RobertaTokenizer\n",
        "import tensorflow as tf\n",
        "import gluonnlp as nlp \n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from wikipedia2vec import Wikipedia2Vec\n",
        "import wikipedia\n",
        "wikipedia.set_rate_limiting(True)\n",
        "\n",
        "import gc\n",
        "import traceback\n",
        "import pickle\n",
        "import json\n",
        "import math\n",
        "import Orange\n",
        "\n",
        "from tqdm import tqdm\n",
        "from typing import List, Tuple, Dict, Callable\n",
        "from os.path import exists, join, abspath\n",
        "from os import system\n",
        "from enum import Enum\n",
        "from time import perf_counter\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "from scipy.stats import SpearmanRConstantInputWarning\n",
        "from scipy.stats import spearmanr\n",
        "from scipy.cluster.hierarchy import linkage\n",
        "from skbio import DistanceMatrix\n",
        "from skbio.tree import nj"
      ],
      "metadata": {
        "id": "spXWSv2iW9i5",
        "outputId": "8523f26b-4400-4e19-881c-eab86f6b611a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import warnings\n",
        "from transformers import logging as transformer_logging\n",
        "\n",
        "FORMAT = '%(levelname)-10s %(message)s'\n",
        "logging.basicConfig(format=FORMAT, level = logging.INFO, filename = \"WordsEmbeddings.log\" )\n",
        "\n",
        "transformer_logging.set_verbosity_error()\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='wikipedia')"
      ],
      "metadata": {
        "id": "MPgmkR7MdZne"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils classes"
      ],
      "metadata": {
        "id": "kjt7fI8Olf5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usefull functions ✅"
      ],
      "metadata": {
        "id": "W3-C-5ztMDZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dict2csv(filename : str, embeddings : Dict[str, List[float]]) -> None:\n",
        "    \"\"\" write a dict of embeddings under a .CSV file\n",
        "\n",
        "    the .CSV file is construct with a header looking like this :\n",
        "    \\tembeddings\\t | 0 | 1 | 2 | 3 | ...\n",
        "    where each line contain an embeddings for the word in the first row\n",
        "    Args:\n",
        "        filename (str) : a path to the file where the .csv is to be written\n",
        "        embeddings (Dict[str, List[float]]): a dictionnary of embeddings\n",
        "\n",
        "    \"\"\"\n",
        "    logging.info(f\"writing dict to {filename} file\")\n",
        "    try:\n",
        "        f = open(filename, \"w\")\n",
        "    except OSError:\n",
        "        raise OSError(\"Could not open file\")\n",
        "\n",
        "    dimension_number = len(next(iter(embeddings.values())))\n",
        "    with f:\n",
        "        print(\"embeddings\", *[str(i) for i in range(dimension_number)], sep=\",\", file=f)\n",
        "        for tag, embedding in embeddings.items():\n",
        "            print(tag, *list(map(lambda x: str(float(x)), embedding)), sep=\",\", file=f)\n",
        "    logging.info(\"done\")\n",
        "\n",
        "def sim2dist(mat : List[List[float]], func : Callable[[float], float] \\\n",
        "             = lambda x: 1 - x, hollow : bool = True) -> List[List[float]]:\n",
        "    \"\"\" map the function func to each elements in the matrix\n",
        "\n",
        "    apply the lambda function func to each element of the matrix. if hollow is set \n",
        "    to True, set the diagonal of the matrix to 0.\n",
        "    Args:\n",
        "        mat (List[List[float]]) : a matrix of number\n",
        "        func (Callable[[float], float]) : a simple function to apply to each elem of the matrix\n",
        "        hollow (bool) : whether to consider the diagonal of the matrix or not\n",
        "    \n",
        "    \"\"\"\n",
        "    logging.info(f\"converting similarity matrix to distance matrix\")\n",
        "    inv_data = [[0 for i in range(len(mat[0]))] for j in range(len(mat))]\n",
        "\n",
        "    for i, elem in enumerate(mat):\n",
        "        for j, case in enumerate(elem):\n",
        "            if i == j and hollow: \n",
        "                inv_data[i][j] = 0\n",
        "            else:\n",
        "                inv_data[i][j] = func(case)\n",
        "                \n",
        "    logging.info(\"done\")\n",
        "    return inv_data\n",
        "\n",
        "def print_mat(mat : List[List[float]], format_function : Callable[[float], str]=lambda x: x) -> None:\n",
        "    \"\"\" print a matrice on stdout\n",
        "\n",
        "    format each number in the matrice using the format_function\n",
        "    Args:\n",
        "        mat (List[List[float]]) : a matrix of number\n",
        "        format_function (Callable[[float], str]) : a simple format function to display numbers froms the matrix\n",
        "    \"\"\"\n",
        "    for line in mat:\n",
        "        for case in line:\n",
        "            print(f\"{format_function(case):8}\", end=\"\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "COZfnM3QNwUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Strategy ✅\n",
        "\n"
      ],
      "metadata": {
        "id": "-Wd47Q50gPSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTMergeStrategy:\n",
        "    \"\"\" strategy to extract BERT embeddings\n",
        "    \n",
        "    different approach exist, \n",
        "    see https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/BERT/bert-feature-extraction-contextualized-embeddings.png\n",
        "    for more possible strategy\n",
        "    \"\"\"\n",
        "\n",
        "    def merge(self, vector : List[List[float]]) -> torch.Tensor:\n",
        "        raise NotImplementedError\n",
        "\n",
        "class Sum4LastLayers(BERTMergeStrategy):\n",
        "\n",
        "    def merge(self, vector : List[List[float]]) -> torch.Tensor:\n",
        "        return torch.sum(vector[-4:], dim = 0)\n",
        "\n",
        "class Concat4LastLayer(BERTMergeStrategy):\n",
        "\n",
        "    def merge(self, vector : List[List[float]]) -> torch.Tensor:\n",
        "        return torch.concat(vector[-4:], dim = 0)\n",
        "\n",
        "class SimilarityStrategy:\n",
        "    \"\"\"strategy to compute similarity between embeddings. Cosine similarity should \n",
        "    be the only valid one in word embeddings, other aren't relevant\n",
        "    \"\"\"\n",
        "\n",
        "    def sim(self, embed1 : List[float], embed2 : List[float]) -> float:\n",
        "        raise NotImplementedError\n",
        "\n",
        "class CosineSim(SimilarityStrategy):\n",
        "\n",
        "    def sim(self, embed1 : List[float], embed2 : List[float]) -> float:\n",
        "        cos = torch.nn.CosineSimilarity(dim=0)\n",
        "        return cos(embed1, embed2)\n",
        "\n",
        "class EuclidianDistSim(SimilarityStrategy):\n",
        "\n",
        "    def sim(self, embed1 : List[float], embed2 : List[float]) -> float:\n",
        "        return np.linalg.norm(embed1-embed2)\n",
        "\n",
        "class ManhattanDistSim(SimilarityStrategy):\n",
        "\n",
        "    def sim(self, embed1 : List[float], embed2 : List[float]) -> float:\n",
        "        return cityblock(embed1, embed2)"
      ],
      "metadata": {
        "id": "LC8LdC2ngRtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Article"
      ],
      "metadata": {
        "id": "7G26rtCEfQny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class customArticle:\n",
        "    \"\"\" store a wikipedia article for further processing by models\"\"\"\n",
        "\n",
        "    def __init__(self, index : int, title : str, realtitle : str, summary : str, ambiguous : bool):\n",
        "        self.index : int = index\n",
        "        self.title : str = title\n",
        "        self.realtitle : str = realtitle\n",
        "        self.summary :str = summary\n",
        "        self.ambiguous :bool = ambiguous\n",
        "\n",
        "class ArticleRetriever:\n",
        "\n",
        "    \"\"\" Class in charge of retrieveing article from different sources and store them in orer\n",
        "    to not re retrieve them. \n",
        "    \n",
        "    Act as a proxy between wikipedia and the model. This class save all the article \n",
        "    retrieve in a dict using the name given. Further call to this retriever will \n",
        "    then load the previously saved file if it hasn't been deleted.\n",
        "    \"\"\"\n",
        "\n",
        "    article_dir = \"./article\"\n",
        "\n",
        "    def __init__(self, name : str = None, list_title : List[str] = []):\n",
        "\n",
        "        self.name : str = name\n",
        "        if self.name is None:\n",
        "            self.name = \"temp\"\n",
        "\n",
        "        self.list_title : List[str] = list_title\n",
        "        self.modified : bool = False\n",
        "        self._load()\n",
        "\n",
        "    def _load(self):\n",
        "        if not exists(self.get_filename()):\n",
        "            self.articles_map = {}\n",
        "            logging.info(f\"creating file {self.get_filename()}\")\n",
        "        else:\n",
        "            with open(self.get_filename(), \"rb\") as mapfile:\n",
        "                self.articles_map = pickle.load(mapfile)\n",
        "                assert(type(self.articles_map) == type(dict()))\n",
        "            logging.info(f\"loading file {self.get_filename()} with {len(self.articles_map)} articles\")\n",
        "    \n",
        "    def set_list_vocab(self, new_name : str, list_title : List[str]):\n",
        "        logging.info(\"changing vocab, reloading file...\")\n",
        "        self.list_title : List[str] = list_title\n",
        "        self.name = new_name\n",
        "        self._load()\n",
        "\n",
        "    def get_filename(self) -> str:\n",
        "        \"\"\" return the filename of the file where article are saved\"\"\"\n",
        "        return join(WikipediaArticleRetriever.article_dir, self.name)\n",
        "\n",
        "    def load_article(self, title : str, force_reload : bool = False) -> customArticle:\n",
        "        \"\"\" retrieve an article from wikipedia. If forcce reload is specified, re check the article \n",
        "        is summary isn't present or if article not alread in the dict\"\"\" \n",
        "\n",
        "        if title not in self.articles_map:\n",
        "            self.modified = True\n",
        "            realtitle, summary, ambiguous = self._retrieve_article(title)\n",
        "            self.articles_map[title] = customArticle(len(self.articles_map), title, realtitle, summary, ambiguous)\n",
        "\n",
        "        if title in self.articles_map and self.articles_map[title].summary == None and force_reload:\n",
        "            self.modified = True\n",
        "            realtitle, summary, ambiguous = self._retrieve_article(title)\n",
        "            self.articles_map[title].summary = summary\n",
        "\n",
        "        return self.articles_map[title]\n",
        "\n",
        "    def load_all_articles(self, force_reload : bool = False) -> None:\n",
        "        \"\"\"retrieve all article from the vocab from sources\"\"\"\n",
        "        \n",
        "        logging.info(f\"Starting loading articles... [Force reload : {force_reload}]\")\n",
        "        nb_success = 0\n",
        "\n",
        "        nb_article = len(self.list_title)\n",
        "        for i, title in tqdm(enumerate(self.list_title), total=nb_article, desc=f\"{'loading articles':30}\"):\n",
        "            self.load_article(title, force_reload)\n",
        "\n",
        "            if self.articles_map[title].summary is not None: \n",
        "                nb_success += 1\n",
        "\n",
        "        logging.info(f\"Finished loading {nb_success} article(s) / {nb_article} ({round(nb_success / nb_article * 100, 1)}%)!\")\n",
        "        return self.modified\n",
        "\n",
        "    def __call__(self, force_reload : bool = False) -> None:\n",
        "        return self.load_all_articles(force_reload)\n",
        "\n",
        "    def _retrieve_article(self, title : str, closed_list : List[str]) -> Tuple[str, str, bool]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_article(self, title) -> customArticle:\n",
        "        \"\"\"return the article if it's present, else, try to retrieve it\"\"\"\n",
        "\n",
        "        if title not in self.articles_map:\n",
        "            self.load_article(title)\n",
        "\n",
        "        return self.articles_map[title]\n",
        "        \n",
        "    def save(self):\n",
        "        \"\"\"save the articles in a binary format using pickle\"\"\"\n",
        "        logging.info(f\"saving the file {self.get_filename()}\")\n",
        "        with open(self.get_filename(), \"wb\") as mapfile:\n",
        "            pickle.dump(self.articles_map, mapfile)\n",
        "\n",
        "class WikipediaArticleRetriever(ArticleRetriever):\n",
        "\n",
        "    def __init__(self, name: str = None, list_title: List[str] = []):\n",
        "        ArticleRetriever.__init__(self, name, list_title)\n",
        "\n",
        "    def get_filename(self) -> str:\n",
        "        \"\"\" return the filename of the file where article are saved\"\"\"\n",
        "        return join(WikipediaArticleRetriever.article_dir, \"Wiki-\" + self.name)\n",
        "\n",
        "    def _retrieve_article(self, title : str, closed_list : List = []) -> Tuple[str, str, bool]: \n",
        "        closed_list.append(title)\n",
        "        try:\n",
        "            article = wikipedia.page(title, auto_suggest=False, redirect=True)\n",
        "            return (article.title, article.summary, False)\n",
        "\n",
        "        except wikipedia.PageError as e:\n",
        "            search_result = wikipedia.search(title, suggestion = False)\n",
        "\n",
        "            logging.warning(f\"{title} misspelled or article missing. Best find is {search_result[0]}\")\n",
        "            if search_result[0] is not None and search_result[0] not in closed_list:            \n",
        "                return self._retrieve_article(search_result[0], closed_list)  \n",
        "            else: return (None, None, None)\n",
        "\n",
        "        except wikipedia.DisambiguationError as e:\n",
        "            logging.warning(f\"{title} is ambiguous, fallback on {e.options[0]}\")\n",
        "            return (None, None, None)\n",
        "            # if e.options[0] is not None and e.options[0] not in closed_list:\n",
        "            #     res = self._retrieve_article(e.options[0], closed_list)\n",
        "            #     return (res[0], res[1], True)\n",
        "        return (None, None, None)\n",
        "\n",
        "class WordNetArticleRetriever(ArticleRetriever):\n",
        "\n",
        "    def __init__(self, name: str = None, list_title: List[str] = []):\n",
        "        super().__init__(name, list_title)\n",
        "\n",
        "    def get_filename(self) -> str:\n",
        "        \"\"\" return the filename of the file where article are saved\"\"\"\n",
        "        return join(WikipediaArticleRetriever.article_dir, \"Word-\" + self.name)\n",
        "\n",
        "    def _retrieve_article(self, title: str, closed_list : List = []) -> Tuple[str, str, bool]:\n",
        "        result = wordnet.synsets(title)\n",
        "        if len(result) > 0:\n",
        "            return (title, result[0].definition(), True)\n",
        "\n",
        "        return (None, None, None)\n",
        "\n",
        "class ArticleViewer():\n",
        "\n",
        "    def __init__(self, filename):\n",
        "        self.name = filename\n",
        "\n",
        "        if not exists(self.name):\n",
        "            raise FileNotFoundError()\n",
        "        else:\n",
        "            with open(self.name, \"rb\") as mapfile:\n",
        "                self.articles_map = pickle.load(mapfile)\n",
        "\n",
        "    def get(self, title):\n",
        "        return self.articles_map[title]\n",
        "\n",
        "    def get_all_articles(self):\n",
        "        return self.articles_map.keys()\n"
      ],
      "metadata": {
        "id": "QZiZ2Pr-lUiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings operations "
      ],
      "metadata": {
        "id": "bw3z2xRJfeYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingsLoader:\n",
        "\n",
        "    \"\"\"class that load an embeddings file to perform operation on it. Base class\n",
        "     for multiple operations such as matrix similarity operations.\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, filename : str):\n",
        "\n",
        "        self.file = filename\n",
        "        self.embeddings = {}\n",
        "\n",
        "        self._load_file()\n",
        "\n",
        "    def _load_file(self):\n",
        "        try:\n",
        "            with open(self.file, \"r\") as f:\n",
        "                lines = f.readlines()\n",
        "                \n",
        "            for line in lines[1:]:\n",
        "                data = line.split(\",\")\n",
        "                self.embeddings[data[0]] = torch.FloatTensor(list(map(float, data[1:])))\n",
        "\n",
        "        except IOError as e:\n",
        "            raise IOError(f\"No file {self.file}\")\n",
        "\n",
        "class SimilarityMatrix(EmbeddingsLoader):\n",
        "\n",
        "    def __init__(self, embeddings : Dict[str, List[float]], strategy : SimilarityStrategy):\n",
        "        EmbeddingsLoader.__init__(self, embeddings)\n",
        "        self.strategy = strategy\n",
        "        self._create_matrix()\n",
        "        self.computed : bool = False\n",
        "\n",
        "    def _create_matrix(self) -> None:\n",
        "        n_tokens = len(self.embeddings)\n",
        "        self.cosine_sim_matrix : Dict[Dict[float]] = {}\n",
        "        for tag in self.embeddings.keys():\n",
        "            self.cosine_sim_matrix[tag] = {}\n",
        "\n",
        "    def compute_sim(self) -> None:\n",
        "        \"\"\" compute cosine similarity between all vectors \"\"\"\n",
        "\n",
        "        closed_list = []\n",
        "\n",
        "        logging.info(\"Computing cosine similarity, this could take some time...\")\n",
        "        for tag, vector in tqdm(self.embeddings.items(), total = len(self.embeddings), desc=f\"{'computing sim matrix':30}\"):\n",
        "\n",
        "            for otag, other_vector in self.embeddings.items():\n",
        "\n",
        "                if otag == tag: continue\n",
        "                # if (tag, otag) in closed_list or (otag, tag) in closed_list: continue\n",
        "\n",
        "                similarity = self.strategy.sim(vector, other_vector)\n",
        "\n",
        "                self.cosine_sim_matrix[otag][tag] = similarity\n",
        "                self.cosine_sim_matrix[tag][otag] = similarity\n",
        "\n",
        "                # closed_list.append((tag, otag))\n",
        "                # closed_list.append((otag, tag))\n",
        "\n",
        "        self.computed = True\n",
        "\n",
        "    def export_sim_matrix(self, filename):\n",
        "        if not self.computed:\n",
        "            self.compute_sim()\n",
        "        \n",
        "        try:\n",
        "            f = open(filename, \"w\")\n",
        "        except OSError:\n",
        "            raise OSError(\"Could not open file\")\n",
        "\n",
        "        with f:\n",
        "            print(\"/\", *[tag for tag in self.embeddings.keys()], sep = \",\", file = f)\n",
        "\n",
        "            for tag in self.embeddings.keys():\n",
        "                print(tag, *[str(round(float(self.cosine_sim_matrix[tag][otag]), 3)) for otag in self.embeddings.keys()], sep = \",\", file = f)\n",
        "\n",
        "    def get_sim_matrix(self) -> Tuple[List[str], List[List[float]]]:\n",
        "        \"\"\"return the similarity matrix of the embeddings\n",
        "        \"\"\"\n",
        "        if not self.computed:\n",
        "            self.compute_sim()\n",
        "\n",
        "        X = len(self.embeddings)\n",
        "        matrix = [[0 for j in range(X)] for i in range(X)]\n",
        "        ids = []\n",
        "        \n",
        "        for i, tag in enumerate(self.embeddings.keys()):\n",
        "            ids.append(tag)\n",
        "            for j, otag in enumerate(self.embeddings.keys()):\n",
        "                if i == j:\n",
        "                    continue\n",
        "\n",
        "                matrix[i][j] = self.cosine_sim_matrix[tag][otag]\n",
        "                matrix[j][i] = self.cosine_sim_matrix[tag][otag]\n",
        "\n",
        "        return ids, matrix\n",
        "\n",
        "    def sim_between(self, token1 : str, token2 : str) -> float:\n",
        "        v1 = self.embeddings[token1]\n",
        "        v2 = self.embeddings[token2]\n",
        "\n",
        "        if token2 not in self.cosine_sim_matrix[token1] or token1 not in self.cosine_sim_matrix[token2]:\n",
        "            similarity = self.strategy.sim(v1, v2)\n",
        "            self.computed = True\n",
        "\n",
        "            self.cosine_sim_matrix[token1][token2] = similarity\n",
        "            self.cosine_sim_matrix[token1][token2] = similarity\n",
        "\n",
        "        return self.cosine_sim_matrix[token1][token2]"
      ],
      "metadata": {
        "id": "5AxDIhr1rkca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloader"
      ],
      "metadata": {
        "id": "4ptlmjOSifg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Downloader:\n",
        "\n",
        "    def __init__(self, base_addr : str, file_zipname : str):\n",
        "        self.__address = base_addr\n",
        "        self.__zip_filename = file_zipname\n",
        "        self.__unzip_filename = \"\".join(file_zipname.split(\".\")[:-1])\n",
        "\n",
        "    def download(self) -> str:\n",
        "        \"\"\" download the embedding file and return a path to the file.\n",
        "         If the file is already downloaded, only return the path\n",
        "         \"\"\"\n",
        "        self.path = abspath(self.__unzip_filename)\n",
        "        logging.info(f\"Checking the presence of {self.path}...\")\n",
        "        if exists(self.path):\n",
        "            logging.info(\"File found ! No download needed\")\n",
        "            return self.path\n",
        "\n",
        "        logging.info(\"File Not present, need to download\")\n",
        "        logging.info(f\"Starting to download {self.__address}{self.__unzip_filename}\")\n",
        "        try:\n",
        "            ret = system(f\"wget {self.__address}{self.__zip_filename}\")\n",
        "            if ret != 0:\n",
        "                raise SystemError\n",
        "        except:\n",
        "            logging.error(f\"can't retrieve the file {self.__zip_filename}\")\n",
        "            raise SystemError(f\"can't retrieve the file {self.__zip_filename}\")\n",
        "\n",
        "        logging.info(f\"Download finished\")\n",
        "        logging.info(f\"Starting unziping the file\")\n",
        "\n",
        "        ext = self.__zip_filename[-4:]\n",
        "        if ext == \".zip\":\n",
        "            unzip_command = \"unzip -p\"\n",
        "        elif ext == \".bz2\":\n",
        "            unzip_command = \"bunzip2 -c\"\n",
        "\n",
        "        try:\n",
        "            ret = system(f\"{unzip_command} ./{self.__zip_filename} > {self.__unzip_filename}\")\n",
        "            if ret != 0:\n",
        "                raise SystemError\n",
        "        except:\n",
        "            logging.error(f\"can't unzip the file {self.__zip_filename}\")\n",
        "            raise SystemError(f\"can't unzip the file {self.__zip_filename}\")\n",
        "\n",
        "        logging.info(f\"Unzipping finished ! ({self.path})\")\n",
        "        return self.path\n"
      ],
      "metadata": {
        "id": "oPSvrgj9if8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solver"
      ],
      "metadata": {
        "id": "0bRoDbKafoVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Solver(EmbeddingsLoader):\n",
        "\n",
        "    DEFAULT_MIN_LIST_RESULT = 10\n",
        "\n",
        "    def __init__(self, embeddings):\n",
        "        super(Solver, self).__init__(embeddings)\n",
        "\n",
        "    def get_nearest_embedding_of(self, embedding, nb = 10):\n",
        "\n",
        "        if nb > len(self.embeddings):\n",
        "            raise Exception(\"nb too high, not enough token\")\n",
        "\n",
        "        nearest = []\n",
        "        for tag, e in self.embeddings.items():\n",
        "\n",
        "            cos = torch.nn.CosineSimilarity(dim=0)\n",
        "            similarity = cos(embedding, e)\n",
        "\n",
        "            nearest.append((tag, similarity))\n",
        "        \n",
        "        nearest.sort(key = lambda tup : tup[1])\n",
        "        return nearest[-1:-nb-1:-1]\n",
        "\n",
        "    def __call__(self, embeddeding, tag=None):\n",
        "        result = self.get_nearest_embedding_of(embeddeding, min(Solver.DEFAULT_MIN_LIST_RESULT, len(self.embeddings)))\n",
        "        if tag is not None:\n",
        "            print(f\"Nearest Word for {tag}:\")\n",
        "        for i in result:\n",
        "            print(f\"\\t{i[0]:12}: {round(float(i[1]) * 100, 3)}%\")\n",
        "    \n",
        "    def score(self, embedding, target):\n",
        "        target_embeddings = self.embeddings[target]\n",
        "\n",
        "        cos = torch.nn.CosineSimilarity(dim=0)\n",
        "        return float(cos(embedding, target_embeddings))\n",
        "\n",
        "    def least_squared_score(self, embedding, target):\n",
        "        target_embeddings = self.embeddings[target]\n",
        "        return float(np.linalg.norm(target_embeddings - embedding))\n",
        "\n",
        "    def mean_squared_score(self, embedding, target):\n",
        "        target_embeddings = self.embeddings[target]\n",
        "        return float(np.square(np.subtract(embedding, target_embeddings)).mean())\n",
        "\n",
        "class OutOfVocabSolver(Downloader):\n",
        "\n",
        "    DEFAULT_MIN_LIST_RESULT = 10\n",
        "\n",
        "    def __init__(self):\n",
        "        address = \"http://wikipedia2vec.s3.amazonaws.com/models/en/2018-04-20/\"\n",
        "        filename = \"enwiki_20180420_300d.pkl.bz2\"\n",
        "\n",
        "        super(OutOfVocabSolver, self).__init__(address, filename)\n",
        "\n",
        "        self.download()\n",
        "        self.model = Wikipedia2Vec.load(self.path)\n",
        "\n",
        "    def get_nearest_embedding_of(self, embedding, nb):\n",
        "        embedding = np.array(embedding)\n",
        "        return self.model.most_similar_by_vector(embedding, count=nb, min_count=nb)\n",
        "\n",
        "    def __call__(self, embedding, tag = None):\n",
        "        result = self.get_nearest_embedding_of(embedding,OutOfVocabSolver.DEFAULT_MIN_LIST_RESULT)\n",
        "        if tag is not None:\n",
        "            print(f\"Nearest Word for {tag}:\")\n",
        "        for i in result:\n",
        "            print(f\"\\t{repr(i[0]):12}: {round(float(i[1]) * 100, 3)}%\")\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "KpVPJ9IxfnC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "nYaqmgXe1-EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WordToVector:\n",
        "\n",
        "    def __init__(self, list_tags : List[str] = []):\n",
        "        self.list_tags = list_tags\n",
        "        self.embeddings = {}\n",
        "\n",
        "    def set_list_class(self, list_class : List[str]):\n",
        "        self.list_tags = list_class\n",
        "        self.reset_embeddings()\n",
        "\n",
        "    def check_embeddings_exist(self, filename : str, article_ret : WikipediaArticleRetriever):\n",
        "        temp_tags_list = self.list_tags\n",
        "        first_tag = self.list_tags[0]\n",
        "        self.list_tags = [first_tag]\n",
        "\n",
        "        self.convert(article_ret)\n",
        "        try:\n",
        "            loader = EmbeddingsLoader(filename)\n",
        "        except OSError:\n",
        "            self.list_tags = temp_tags_list\n",
        "            return False\n",
        "\n",
        "        first_embedding = self.embeddings[first_tag]\n",
        "        to_compare = loader.embeddings[first_tag]\n",
        "\n",
        "        intersect = set(temp_tags_list) & set(loader.embeddings.keys())\n",
        "\n",
        "        self.list_tags = temp_tags_list\n",
        "        if torch.equal(first_embedding, to_compare) and len(intersect) == len(temp_tags_list):\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def convert(self, article_ret : WikipediaArticleRetriever):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def reset_embeddings(self):\n",
        "        self.embeddings.clear()\n",
        "\n",
        "    def get_embedding_of(self, token):\n",
        "        if token not in self.embeddings:\n",
        "            raise Exception(f\"no such token {token}\")\n",
        "        \n",
        "        return self.embeddings[token]\n",
        "\n",
        "    def get_class_list(self):\n",
        "        return self.embeddings.keys()\n",
        "\n",
        "    def export(self, filename):\n",
        "        \"\"\"export all the embeddings in filename under a .csv format.\n",
        "           Raise exception if embeddings hasn't been calculed yet.\"\"\"\n",
        "\n",
        "        if len(self.embeddings) == 0:\n",
        "            raise Exception(\"Tags not converted yet !\")\n",
        "        \n",
        "        dict2csv(filename, self.embeddings)\n",
        "\n",
        "class FixedEmbedding(WordToVector):\n",
        "\n",
        "    def __init__(self, base_addr : str, file_zipname : str):\n",
        "        self.downloader = Downloader(base_addr, file_zipname)\n",
        "        self.downloader.download()\n",
        "\n",
        "    def check_embeddings_exist(self, filename : str, article_ret : WikipediaArticleRetriever):\n",
        "        return False\n",
        "\n",
        "    def _one_turn(self, resolve_dict = {}):\n",
        "        print(\"here\")\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def convert(self, ar):\n",
        "        resolve_filename = f\"./temp/{ar.name[:-4]}_resolve.json\"\n",
        "        resolve = {}\n",
        "\n",
        "        while True: \n",
        "            unk_list = self._one_turn(resolve)\n",
        "        \n",
        "            if unk_list is None or len(unk_list) == 0:\n",
        "                break\n",
        "\n",
        "            print(len(unk_list), \"items haven't been found, resolve mode.\")\n",
        "\n",
        "            with open(resolve_filename, 'w') as f:\n",
        "                resolve_dict = {word: \"\" for word in unk_list}\n",
        "                json.dump(resolve_dict, f, indent = 4)\n",
        "\n",
        "            input(\"press enter to resume resolve\")\n",
        "\n",
        "            resolve = {}\n",
        "            with open(resolve_filename, 'r') as f:\n",
        "                resolve = json.load(f)\n",
        "                assert(type(resolve) == type(dict()))\n"
      ],
      "metadata": {
        "id": "Q448MeSB7ay8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT model"
      ],
      "metadata": {
        "id": "-0g4CJtelYfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTModel(WordToVector):\n",
        "\n",
        "    temp_dir = \"./temp\"\n",
        "\n",
        "    def __init__(self, list_tag : List[str], big: bool = False, window : int = 100):\n",
        "        super(BERTModel, self).__init__(list_tag)\n",
        "        self.window_size = window\n",
        "\n",
        "        self.model_size = \"bert-large-uncased\" if big else \"bert-base-uncased\"\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.model_size, padding=True, truncation=True)\n",
        "        self.model = BertModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "    def _one_pass(self, inputs):\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids = inputs[\"input_ids\"], attention_mask = inputs[\"attention_mask\"])\n",
        "\n",
        "        hidden_states = outputs[2]\n",
        "\n",
        "        # [# layers, # batches, # tokens, # features] ==> [# tokens, # layers, # features]\n",
        "        token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "        token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "        return self.merging_strategy.merge(token_embeddings[0])\n",
        "\n",
        "    def convert(self, article_ret : WikipediaArticleRetriever):\n",
        "        \"\"\" convert all word in their embeddings\"\"\"\n",
        "\n",
        "        if len(self.list_tags) == 0:\n",
        "            raise Exception(\"no tags yet !\")\n",
        "\n",
        "        logging.info(\"Starting converting tokens...\")\n",
        "        nb_token = len(self.list_tags)\n",
        "        for i, tag in tqdm(enumerate(self.list_tags), total = nb_token, desc=f\"{'converting to embedding':30}\"):\n",
        "            \n",
        "            if tag in self.embeddings: continue\n",
        "\n",
        "            article = article_ret.get_article(tag)\n",
        "            if article.summary is None or self.window_size == 0:\n",
        "                self.embeddings[tag] = self._one_pass(self.tokenizer(tag, return_tensors = \"pt\"))\n",
        "                continue\n",
        "\n",
        "            sub_ids = self.tokenizer.encode(tag + \". \" + article.summary)[0:self.window_size]\n",
        "            subinputs = {   \"input_ids\": torch.IntTensor(sub_ids).unsqueeze(0), \\\n",
        "                            \"token_type_ids\": torch.IntTensor([0 for k in range(len(sub_ids))]).unsqueeze(0), \\\n",
        "                            \"attention_mask\": torch.IntTensor([1 for k in range(len(sub_ids))]).unsqueeze(0)  }\n",
        "\n",
        "            self.embeddings[tag] = self._one_pass(subinputs)\n",
        "\n"
      ],
      "metadata": {
        "id": "UVJUSGWQorYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RoBERTa model"
      ],
      "metadata": {
        "id": "eGVvWFvtlqLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ROBERTAModel(BERTModel):\n",
        "\n",
        "    def __init__(self, list_tag : List[str], big: bool = False, window : int = 100):\n",
        "        WordToVector.__init__(self, list_tag)\n",
        "        self.window_size = window\n",
        "\n",
        "        self.model_size = \"roberta-large\" if big else \"roberta-base\"\n",
        "\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_size, padding=True, truncation=True)\n",
        "        self.model = RobertaModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()"
      ],
      "metadata": {
        "id": "7oWZOwG-mCYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DocBERTModel model"
      ],
      "metadata": {
        "id": "GqQ1HgNyz-zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DocBERTModel(BERTModel):\n",
        "\n",
        "    def __init__(self, list_tag : List[str], big : bool = False):\n",
        "        WordToVector.__init__(self, list_tag)\n",
        "        self.window_size = \"document\"\n",
        "\n",
        "        self.model_size = \"bert-large-uncased\" if big else \"bert-base-uncased\"\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.model_size, padding=True, truncation=True)\n",
        "        self.model = BertModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.max_size = self.tokenizer.model_max_length\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "    def _one_pass(self, subinputs):\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids = subinputs[\"input_ids\"], attention_mask = subinputs[\"attention_mask\"])\n",
        "\n",
        "        hidden_states = outputs[2]\n",
        "\n",
        "        # [# layers, # batches, # tokens, # features] ==> [# tokens, # layers, # features]\n",
        "        token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "        token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "        return self.merging_strategy.merge(token_embeddings[0])\n",
        "\n",
        "\n",
        "    def convert(self, article_ret):\n",
        "        \"\"\" convert all word in their embeddings\"\"\"\n",
        "\n",
        "        if len(self.list_tags) == 0:\n",
        "            raise Exception(\"no tags yet !\")\n",
        "\n",
        "        logging.info(\"Starting converting tokens...\")\n",
        "        nb_token = len(self.list_tags)\n",
        "        for i, tag in tqdm(enumerate(self.list_tags), total = nb_token, desc=f\"{'converting to embedding':30}\"):\n",
        "            \n",
        "            if tag in self.embeddings: continue\n",
        "\n",
        "            article = article_ret.get_article(tag)\n",
        "\n",
        "            if article.summary is None:\n",
        "                logging.warning(f\"no article for {tag}\")\n",
        "                self.embeddings[tag] = self._one_pass(self.tokenizer(tag, return_tensors = \"pt\"))\n",
        "                continue\n",
        "\n",
        "            torch_cls = []\n",
        "\n",
        "            ids = self.tokenizer.encode(article.summary)\n",
        "            nb_token = len(ids)\n",
        "\n",
        "            if nb_token < self.max_size:\n",
        "                self.embeddings[tag] = self._one_pass(self.tokenizer(tag, return_tensors = \"pt\"))\n",
        "                continue\n",
        "\n",
        "            nb_pass = math.ceil(nb_token / self.max_size)\n",
        "            logging.info(f\"{tag} is {nb_pass} pass\")\n",
        "\n",
        "            stop = 50\n",
        "            for j in range(nb_pass):\n",
        "                start = stop - 50\n",
        "                stop = min(nb_token, start + self.max_size)\n",
        "                \n",
        "                sub_ids = ids[start:stop]\n",
        "\n",
        "                subinputs = { \"input_ids\": torch.IntTensor(sub_ids).unsqueeze(0), \\\n",
        "                            \"token_type_ids\": torch.IntTensor([0 for k in range(len(sub_ids))]).unsqueeze(0), \\\n",
        "                            \"attention_mask\": torch.IntTensor([1 for k in range(len(sub_ids))]).unsqueeze(0)  }\n",
        "                torch_cls.append(self._one_pass(subinputs))\n",
        "                if stop == nb_token: break\n",
        "\n",
        "            self.embeddings[tag] = torch.mean(torch.stack(tuple(t for t in torch_cls)), axis=0)"
      ],
      "metadata": {
        "id": "nnq0gSYxiIQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DocBERTAModel model"
      ],
      "metadata": {
        "id": "mRkUxi2g0FT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DocBERTAModel(DocBERTModel):\n",
        "\n",
        "    def __init__(self, list_tag : List[str], big : bool = False):\n",
        "        WordToVector.__init__(self, list_tag)\n",
        "        self.window_size = \"document\"\n",
        "\n",
        "        self.model_size = \"roberta-large\" if big else \"roberta-base\"\n",
        "\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_size, padding=True, truncation=True)\n",
        "        self.model = RobertaModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.max_size = self.tokenizer.model_max_length\n",
        "\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()"
      ],
      "metadata": {
        "id": "FHdFrESi0Joa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wikipedia2Vec"
      ],
      "metadata": {
        "id": "mFf6OoBG1xVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Wiki2VecModel(FixedEmbedding):\n",
        "\n",
        "    address = \"http://wikipedia2vec.s3.amazonaws.com/models/en/2018-04-20/\"\n",
        "    \n",
        "    def __init__(self, list_tag : List[str], size : int = 300):\n",
        "        WordToVector.__init__(self, list_tag)\n",
        "        self.window_size = size\n",
        "        self.model_size = \"wikipedia2vec\"\n",
        "\n",
        "        assert size in [100, 300, 500], f\"size should be one of this value (100, 300, 500)\"\n",
        "        filename = f\"enwiki_20180420_{self.window_size}d.pkl.bz2\"\n",
        "        FixedEmbedding.__init__(self, Wiki2VecModel.address, filename)\n",
        "        \n",
        "        self.model = Wikipedia2Vec.load(self.downloader.path)\n",
        "\n",
        "    def _retrieve(self, word):\n",
        "        try: return self.model.get_word_vector(word)\n",
        "        except: pass\n",
        "\n",
        "        try: return self.model.get_word_vector(word.capitalize())\n",
        "        except: pass\n",
        "\n",
        "        try: return self.model.get_word_vector(word.lower())\n",
        "        except: pass\n",
        "\n",
        "        try: return self.model.get_entity_vector(word)\n",
        "        except: pass\n",
        "\n",
        "        try: return self.model.get_entity_vector(word.capitalize())\n",
        "        except: pass\n",
        "        \n",
        "        try: return self.model.get_entity_vector(word.lower())\n",
        "        except: pass\n",
        "\n",
        "        return None\n",
        "    \n",
        "    def _one_turn(self, resolve_dict = {}):\n",
        "        unk = []\n",
        "\n",
        "        for word in self.list_tags:\n",
        "            w = word.replace(\"_\", \" \")\n",
        "            if w in self.embeddings: continue\n",
        "\n",
        "            if w in resolve_dict:\n",
        "                embed = self._retrieve(resolve_dict[w])\n",
        "            else:\n",
        "                embed = self._retrieve(w)\n",
        "\n",
        "            if embed is None:\n",
        "                logging.warning(f\"{w} cannot be retrieved.\")\n",
        "                unk.append(word)\n",
        "            else:\n",
        "                self.embeddings[w] = torch.from_numpy(embed)\n",
        "        \n",
        "        return unk"
      ],
      "metadata": {
        "id": "w0zukX6Zk3W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "44KuFxQOG0WX",
        "outputId": "8b334c5e-bb9f-4f4b-b8af-e76c510431c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GloVE"
      ],
      "metadata": {
        "id": "wrbpA901PYwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GloVEModel(FixedEmbedding):\n",
        "\n",
        "    address = \"https://nlp.stanford.edu/data/\"\n",
        "    all_dict = \"glove_all\"\n",
        "    \n",
        "    def __init__(self, list_tag : List[str]):\n",
        "        WordToVector.__init__(self, list_tag)\n",
        "\n",
        "        self.window_size = 300\n",
        "        self.model_size = \"GloVe\"\n",
        "\n",
        "        filename = \"glove.840B.300d.zip\"\n",
        "        FixedEmbedding.__init__(self, GloVEModel.address, filename)\n",
        "\n",
        "        self.download()\n",
        "\n",
        "    def _in_list(self, word, list_tag):\n",
        "        if word in list_tag:\n",
        "            return word\n",
        "        \n",
        "        if word.capitalize() in list_tag:\n",
        "            return word.capitalize()\n",
        "\n",
        "        if word.lower() in list_tag:\n",
        "            return word.lower()\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _one_turn(self, resolve_dict = {}):\n",
        "        uncaped_list_tag = [x.replace(\"_\", \" \") for x in self.list_tags]\n",
        "        unk = [item if item not in resolve_dict.keys() else resolve_dict[item] for item in uncaped_list_tag]\n",
        "\n",
        "        with open(self.path, \"r\") as file:\n",
        "            for line in file:\n",
        "                tag, coefs = line.split(maxsplit=1)\n",
        "                coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "\n",
        "                r_tag = self._in_list(tag, unk)\n",
        "                if r_tag is not False:\n",
        "                    self.embeddings[r_tag] = torch.from_numpy(coefs)\n",
        "                    unk.remove(r_tag)\n",
        "\n",
        "        return unk"
      ],
      "metadata": {
        "id": "ZBf3vNJlPfiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical part"
      ],
      "metadata": {
        "id": "ue8SGNE5iKaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings to word proba"
      ],
      "metadata": {
        "id": "JNFmhVBmUTgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# solver = Solver(\"/content/animal10-roberta-base-0.csv\")\n",
        "\n",
        "# totest = solver.embeddings[\"cat\"]\n",
        "\n",
        "# solver(totest, \"cat\")\n",
        "# print(solver.score(totest, \"dog\"))\n",
        "# print(solver.mean_squared_score(totest, \"dog\"))\n",
        "# print(solver.least_squared_score(totest, \"dog\"))\n",
        "\n",
        "solver_advanced = OutOfVocabSolver()\n",
        "\n",
        "totest = [ 0.7862, -1.3992, -4.2574, -2.4260, -1.8522, -4.9722,  1.5762,  4.2181, 2.7479,  0.4882, -3.8462,  0.3663,  1.6023,  1.0386, -2.1188, -3.6747, -3.3854, -1.9940, -0.9328,  0.2879, -2.9643,  4.0447,  2.3717, -1.2791, 2.0610,  3.1366, -3.9570, -1.6283,  0.8152, -2.0822,  1.6625, -0.3196,\n",
        "        -2.6504, -4.9068,  2.3788,  0.9988,  4.3755,  0.7723, -0.3931, -1.9057, -5.7305,  0.1909, -2.5958, -2.7752, -2.6477,  2.0469, -0.4043,  1.5702, -1.8779,  1.7021,  0.8994,  0.5958, -7.5035,  1.0676,  2.3697, -1.0680, -3.2171,  1.1602, -2.5989, -1.4238,  0.7296,  2.6037, -2.8898,  2.3770,\n",
        "         2.5836,  2.9701, -3.8032,  0.5565,  9.3480,  0.8646, -0.9671,  0.5712, -2.0034, -2.5540,  1.9856, -1.7896, -0.7399, -4.1039,  1.8379,  0.3972, -3.5441, -3.4940,  1.7804, -1.5334,  2.1030,  3.3775,  3.8066, -2.0847, -0.7945, -0.5985,  2.5008, -1.0824,  0.3379, -1.9175, -2.3626,  2.9385,\n",
        "         2.5060,  0.5920, -2.1384,  1.0008, -1.5074, -1.7252,  1.9592, -1.6477, -2.2149,  0.5712, -5.1326, -3.3159, -0.0771,  5.4355, -0.7996,  2.2194, 2.8152,  3.1241,  0.2127, -0.4711, -1.0102,  2.6980,  3.4281,  3.9176, 3.4158,  4.7137,  0.0745, -1.0222, -0.5676, -3.1130, -1.5231, -2.9252,\n",
        "        -0.8840,  1.0718,  5.9833, -0.0269, -1.9574, -0.8195,  1.0675,  2.6848, 1.4984,  1.1614,  1.4133, -3.3854, -3.5907,  0.9117,  3.2912,  2.6879,  0.7066,  0.4763,  2.1558, -2.6881,  1.3685, -2.8319, -1.3616,  2.9175, 2.3130,  3.4277,  0.4531, -1.3417,  0.6093, -0.9372, -0.7949, -4.0459,\n",
        "        -0.3607,  2.7486,  3.3538,  3.0184,  1.3375, -2.2510,  2.5175, -2.4213, 2.3837, -0.0391,  0.2196,  4.3168,  2.0774, -1.7432,  2.5663,  3.0488, 4.4666,  0.4470, -0.7640, -3.6402,  3.4627, -0.0654, -2.9533, -3.1311, 5.8393,  4.9384, -3.4591,  0.9942,  4.2474, -3.5547,  3.4043,  2.3078,\n",
        "        -0.2588,  0.9352,  2.2546, -1.2557,  0.5715,  1.5651, -0.8553,  4.0117, -1.8648,  4.4550, -1.7419,  2.7950, -0.4952,  1.2926, -0.4914, -1.1312, -0.9716, -0.3429,  1.6704,  2.2321,  3.8714,  3.3676,  1.7274, -1.3730, 3.8956, -0.5492,  1.2591, -2.2064,  4.4911, -4.2279, -0.5980,  1.6788,\n",
        "        -0.6136, -2.7168,  3.7387,  3.8866,  3.1262,  2.0629, -2.6571, -1.4073, 3.9042,  0.0150,  1.5226, -0.8405,  0.8074,  1.5322, -1.4147, -2.0555, -0.4014, -0.8865,  1.5190, -0.6994, -3.5390, -2.2263,  0.8470,  3.6711, 2.4342, -0.9778, -1.2095,  2.3892,  2.9762,  1.0258, -1.6725,  1.0209,\n",
        "         0.4480,  1.1777, -2.1264, -1.7206, -3.9378, -2.3451,  2.7490,  0.8951, 0.7263,  2.8524,  1.7494,  0.9067,  0.1311,  0.4994,  0.3548, -2.7868, -0.1992,  5.1012,  0.3275,  1.9652,  1.1535, -3.4700,  0.4942, -0.6859, 0.6918, -2.8642, -2.0151,  1.3709,  1.3846,  0.1103,  0.7877, -0.6042,\n",
        "        -1.1805,  0.2325,  2.1466,  2.7526,  0.7050,  0.8945,  4.1594,  0.3798, -1.0938,  0.7618, -1.3813, -1.3719]\n",
        "\n",
        "solver_advanced(totest, \"chimpanzee\")"
      ],
      "metadata": {
        "id": "tD8XokPDUhkp",
        "outputId": "f73dce61-c050-468a-b485-d11e4a3cff7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-700ad12929fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         -1.1805,  0.2325,  2.1466,  2.7526,  0.7050,  0.8945,  4.1594,  0.3798, -1.0938,  0.7618, -1.3813, -1.3719]\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0msolver_advanced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"chimpanzee\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'solver_advanced' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word to embeddings"
      ],
      "metadata": {
        "id": "LN273RlCUGza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title List of vocab :\n",
        "#@markdown * animal10\n",
        "#@markdown * cifar 10 / 100 \n",
        "#@markdown * imagenet / subimagenet (200 first)\n",
        "#@markdown * king / queen \n",
        "\n",
        "labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
        "imagenet = list(np.array(open(labels_path).read().splitlines()))\n",
        "subimagenet = imagenet[:200]\n",
        "\n",
        "animal10 = [\"dog\", \"cat\", \"horse\", \"spider\", \"butterfly\", \"chicken\", \"sheep\", \"cow\", \"squirrel\", \"elephant\"]\n",
        "\n",
        "cifar10  = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "\n",
        "cifar100 = [\"apple\", \"aquarium_fish\", \"baby\", \"bear\", \"beaver\", \"bed\", \"bee\", \"beetle\", \"bicycle\", \"bottle\", \"bowl\", \"boy\", \"bridge\", \"bus\", \"butterfly\", \"camel\", \"can\", \\\n",
        "            \"castle\", \"caterpillar\", \"cattle\", \"chair\", \"chimpanzee\", \"clock\", \"cloud\", \"cockroach\", \"computer_keyboard\", \"couch\", \"crab\", \"crocodile\", \"cup\", \\\n",
        "            \"dinosaur\", 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'lamp', 'lawn_mower', 'leopard', 'lion', \\\n",
        "            'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', \\\n",
        "            'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark',\\\n",
        "            'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', \\\n",
        "            'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n",
        "\n",
        "king = [\"king\", \"woman\", \"man\", \"queen\", \"boy\", \"girl\", \"male\", \"female\"]\n",
        "\n",
        "custom = [\"outdoor\", \"food\", \"clothes\", \"objects\", \"small animal\", \"aquatic animal\", \"animal\", \"plant\", \"insect\", \"transport\"]"
      ],
      "metadata": {
        "id": "NIJ8zsk-UTuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose a dataset\n",
        "save_name = \"custom\" #@param [\"animal10\", \"cifar10\", \"cifar100\", \"king\", \"imagenet\", \"subimagenet\", \"custom\"]\n",
        "mapping_save_list = {\n",
        "    \"animal10\": animal10,\n",
        "    \"cifar10\" : cifar10,\n",
        "    \"cifar100\" : cifar100,\n",
        "    \"king\" : king,\n",
        "    \"imagenet\" : imagenet,\n",
        "    \"subimagenet\": subimagenet,\n",
        "    \"custom\" : custom\n",
        "}\n",
        "\n",
        "vocab = mapping_save_list[save_name]"
      ],
      "metadata": {
        "id": "EYnbJc84xuh5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose a model and params\n",
        "window_size  = 300 #@param [\"0\", \"100\", \"200\", \"300\", \"400\", \"512\"] {type:\"raw\"}\n",
        "is_big       = False #@param {type:\"boolean\"}\n",
        "model_choice = \"Wikipedia2Vec\" #@param [\"ROBERTA\", \"BERT\", \"Wikipedia2Vec\", \"DocBERT\", \"DocBERTA\", \"GloVe\"]\n",
        "\n",
        "if model_choice == \"ROBERTA\":\n",
        "    model = ROBERTAModel(vocab, big = is_big, window = window_size)\n",
        "elif model_choice == \"BERT\":\n",
        "    model = BERTModel(vocab, big = is_big, window = window_size)\n",
        "elif model_choice == \"Wikipedia2Vec\":\n",
        "    model = Wiki2VecModel(vocab, size = window_size)\n",
        "elif model_choice == \"DocBERT\":\n",
        "    model = DocBERTModel(vocab, big = is_big)\n",
        "elif model_choice == \"DocBERTA\":\n",
        "    model = DocBERTAModel(vocab, big = is_big)\n",
        "elif model_choice == \"GloVe\":\n",
        "    model = GloVEModel(vocab)\n",
        "    \n",
        "articlesRetriever = WikipediaArticleRetriever(save_name + \".art\", vocab)"
      ],
      "metadata": {
        "id": "NvIn_JeFlYfK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file = f\"{save_name}-{model.model_size}-{model.window_size}.csv\"\n",
        "\n",
        "if articlesRetriever():\n",
        "    articlesRetriever.save()\n",
        "\n",
        "model.reset_embeddings()\n",
        "for i, super in enumerate(model.__class__.__bases__):\n",
        "    print(i, super.__name__)\n",
        "\n",
        "if not model.check_embeddings_exist(csv_file, articlesRetriever):\n",
        "    model.convert(articlesRetriever)\n",
        "    model.export(csv_file)\n",
        "\n",
        "print(\"\\n\", len(model.get_class_list()))"
      ],
      "metadata": {
        "id": "WOOlgG8koVD6",
        "outputId": "93e522f2-e91a-4071-ecfa-ac46fa571170",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading articles              : 100%|██████████| 10/10 [00:12<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 FixedEmbedding\n",
            "['outdoor', 'food', 'clothes', 'objects', 'small animal', 'aquatic animal', 'animal', 'plant', 'insect', 'transport']\n",
            "1 items haven't been found, resolve mode.\n",
            "press enter to resume resolve\n",
            "['outdoor', 'food', 'clothes', 'objects', 'small animal', 'aquatic animal', 'animal', 'plant', 'insect', 'transport']\n",
            "\n",
            " 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wiki2vec_dict = model.model.dictionary\n",
        "\n",
        "# with open(\"./temp/wiki_article.txt\", \"w\") as f:\n",
        "#     for word in wiki2vec_dict.words():\n",
        "#         print(word.text, file=f)\n",
        "\n",
        "#     for ent in wiki2vec_dict.entities():\n",
        "#         print(ent.title, file=f)\n",
        "# !unzip -p /content/glove.840B.300d.zip > test\n",
        "\n",
        "with open(\"glove_keys.txt\", \"w\") as gf:\n",
        "\n",
        "    with open(\"/content/glove.840B.300d\", \"r\") as file:\n",
        "        for line in file:\n",
        "            tag, coefs = line.split(maxsplit=1)\n",
        "            print(tag, file = gf)\n",
        "\n"
      ],
      "metadata": {
        "id": "z0qnv2UbfqZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neighboor joining Tree"
      ],
      "metadata": {
        "id": "0WZFTDaGbr1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neighboor_sim = SimilarityMatrix(csv_file, CosineSim())\n",
        "\n",
        "ids, data = neighboor_sim.get_sim_matrix()\n",
        "\n",
        "ids = [tids.replace(\" \", \"_\") for tids in ids]\n",
        "inv_data  = sim2dist(data, lambda x: 1 - x) \n",
        "\n",
        "# print()\n",
        "# print_mat(data_inv, format_function = lambda x: round(float(x), 2))"
      ],
      "metadata": {
        "id": "crcMR8dDuH7s",
        "outputId": "afaa4635-459b-49e8-ea97-9944c546bbc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "computing sim matrix          : 100%|██████████| 68/68 [00:00<00:00, 282.01it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dm = DistanceMatrix(inv_data, ids)\n",
        "tree = nj(dm)\n",
        "\n",
        "with open(f\"{save_name}-{model_choice}-.tree\", \"w\") as f:\n",
        "    print(tree, file = f)\n",
        "\n",
        "print(\"\\n\\n\", tree.ascii_art())"
      ],
      "metadata": {
        "id": "_J431TiosB8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wikipedia debug"
      ],
      "metadata": {
        "id": "7jubA6JXUMk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Article to search for { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "totest = \"buck\" #@param {type:\"string\"}\n",
        "\n",
        "result = wikipedia.search(totest, suggestion = False)\n",
        "print(result)\n",
        "print(f\"first result is: {result[0]}\")\n",
        "\n",
        "try:\n",
        "    print(wikipedia.page(totest, auto_suggest=False, redirect=True))\n",
        "    print(wikipedia.page(result[0], auto_suggest=False, redirect=True))\n",
        "except Exception as e:\n",
        "    print(f\"best option envisaged: {e.options[0]}\")\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "PeM8XAu6h5gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articleviewer = ArticleViewer(\"/content/article/cifar100.art\")\n",
        "print(articleviewer.get_all_articles())\n",
        "articleviewer.get(articleviewer.get_all_articles()[0]).summary"
      ],
      "metadata": {
        "id": "O5sAnE0nuhD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Solver"
      ],
      "metadata": {
        "id": "6a4--t_QLVl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toguess 2\n",
        "totest = [-1.122,2.5493,-6.4934,5.1244,1.8322,-2.4903,1.7885,0.28059,-0.6473,2.7094,-1.7041,0.12659,-2.1201,1.7909,-3.4064,-1.5044,-2.3428,-0.21746,-0.51735,-0.87518,-1.8803,4.1981,-0.57839,-0.75715,-3.2789,0.094055,-2.5323,3.5364,-1.1887,-0.32185,0.83655,-2.5367,-1.5664,-1.6144,0.96499,1.101,5.7738,-1.4313,1.6498,-1.4691,-1.2193,4.6139,1.1892,0.95094,0.28716,5.3983,-2.1434,1.9556,-2.588,3.8573,1.706,-1.4814,-3.8649,3.3019,1.1959,2.1542,-0.88526,-2.5411,-0.2668,-2.5881,1.697,5.5959,-4.8047,-1.3409,0.88701,2.4658,-2.609,1.9621,3.1595,2.8576,-0.74223,-0.89093,-4.2198,3.4845,-1.988,1.8015,-1.7093,-3.737,-2.9994,0.74645,-5.3243,1.5403,-0.60917,-4.2911,-2.9025,-1.7992,-0.8596,0.21249,-3.7441,0.28825,-0.88344,-2.254,-1.3114,-2.6239,0.14179,-1.3166,2.6839,-1.4652,-0.83398,2.9815,-4.0996,-2.8034,3.9212,-2.2096,-0.87033,1.7632,-2.8293,-2.144,2.2181,2.7939,-3.6677,5.0149,-0.79786,2.0267,-0.1555,0.12539,-2.2854,1.7482,0.88181,4.9972,3.4045,6.2988,-2.4562,-0.41591,-1.8707,-2.8566,-3.9587,0.08361,-2.1874,-2.5128,3.8934,-0.45989,-0.082235,0.67877,-0.16172,1.5006,3.0004,2.9907,3.9425,3.0808,-2.2809,-1.0965,-0.55863,1.0862,-2.4945,-0.0015425,5.6565,-2.087,0.11066,-1.2479,4.0543,-0.60213,-0.53165,3.5236,-1.9206,1.6709,1.2394,0.50941,3.7742,-2.5508,-0.50954,-2.0542,1.1629,1.821,2.4256,-3.0941,2.8343,-0.2542,0.6906,0.59896,1.784,0.9476,2.8348,-0.27728,0.64667,4.4519,4.7255,-0.99693,-5.1451,-2.0515,2.8238,2.2864,-0.86283,0.97159,-0.4185,8.5407,-1.5291,1.0881,2.1247,-4.0368,3.1599,1.4262,0.41397,-1.8739,2.0356,2.9822,1.5995,-0.69884,1.3198,0.92781,4.842,2.0718,-5.3633,-0.47037,-2.0025,0.71227,1.6302,-1.3304,-1.6301,0.92339,1.2812,-2.1767,3.3847,2.6614,-0.89659,-0.3342,1.2194,-2.9878,3.2775,-1.5613,0.30171,-3.9471,1.941,0.83132,0.21692,0.93135,7.3327,-1.6191,5.4914,1.9914,-3.0784,1.3881,-1.5109,2.6814,0.041593,-3.7304,2.3987,-0.13929,-4.45,-1.5211,-5.2136,0.40236,-0.50133,-0.096171,-3.9962,0.26384,0.29978,3.2868,2.6498,0.94902,-2.5184,1.526,2.8469,-0.35854,-3.2803,2.3363,-0.56372,-1.8674,1.3576,-1.5291,-1.4955,-4.8288,1.019,-1.4444,3.4142,2.5302,1.64,-1.2125,0.034562,-0.96021,-5.5353,-3.1208,0.59937,3.9008,1.6573,-0.10828,-1.2077,0.13008,-1.8282,-2.2147,-0.50857,-1.5127,-0.23297,0.44002,1.3732,1.5556,0.93295,-1.888,0.46196,-1.9693,2.7558,1.6251,4.6096,-1.1334,-0.95527,0.79511,2.7323,-0.16591,-1.4546,2.6422]"
      ],
      "metadata": {
        "id": "nnW8xIgJThWR"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "replace __community__ by __community.community_louvain__ in module file"
      ],
      "metadata": {
        "id": "VUHV24NKp3FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Orange.clustering.hierarchical import dist_matrix_linkage, tree_from_linkage, data_clustering, leaves, WEIGHTED\n",
        "from Orange.data import Table, Domain\n",
        "from Orange.distance.distance import Cosine\n",
        "from Orange.widgets.unsupervised.owhierarchicalclustering import clusters_at_height\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from itertools import chain\n",
        "from Orange.data.variable import StringVariable"
      ],
      "metadata": {
        "id": "HSkbQhDIG0Et"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generic_table = Table(\"/content/imagenet-wikipedia2vec-300.csv\")\n",
        "supp_info_table = Table(\"/content/class_map_imagenet.csv\")\n",
        "\n",
        "superclass_embeddings = Table(\"/content/custom-wikipedia2vec-300_superclass.csv\")\n",
        "\n",
        "CLUSTER_THRESOLD = 0.75\n",
        "GROUP_BY = \"first superclass\"\n",
        "SIM_THRESOLD = 0.3\n",
        "\n",
        "\n",
        "def left_join(generic_table, supp_info_table, key: str = \"embeddings\") -> Table:\n",
        "    \"\"\"add all <b> metas </b> column from supp_info_table to generic_table using key as joint\n",
        "    \"\"\"\n",
        "    assert key in list(map(lambda x : x.name, supp_info_table.domain.metas)), \"embeddings name not present in additional data\"\n",
        "    assert len(generic_table) == len(supp_info_table), \"table don't contain the same number of line\"\n",
        "\n",
        "    name_supp_data = [i.name for i in chain(supp_info_table.domain.metas, \n",
        "                                            supp_info_table.domain.variables, \n",
        "                                            supp_info_table.attributes) if i.name != key]\n",
        "                                            \n",
        "    supp_list_list = [[] for i in range(len(name_supp_data))]\n",
        "\n",
        "    for s in generic_table:\n",
        "        for d in supp_info_table:\n",
        "            if s[key] == d[key]:\n",
        "                for i, name in enumerate(name_supp_data):\n",
        "                    supp_list_list[i].append(d[name])\n",
        "                break\n",
        "\n",
        "    for i, name in enumerate(name_supp_data):\n",
        "        # print(f\"adding {name}\")\n",
        "        generic_table = generic_table.add_column(StringVariable(name), supp_list_list[i])\n",
        "\n",
        "    return generic_table\n",
        "\n",
        "def add_to_list(cluster, list_to_add_to):\n",
        "    \"\"\" decompose a cluster tree by adding the index of all children in the list\n",
        "    \"\"\"\n",
        "    if cluster.is_leaf:\n",
        "        list_to_add_to.append(cluster.value.index)\n",
        "\n",
        "    for branch in cluster.branches:\n",
        "        add_to_list(branch, list_to_add_to)\n",
        "\n",
        "def clusterize(table : Table, thresold) -> Table:\n",
        "    \"\"\"clusterize a Oranga Table based on the height of THRESOLD\n",
        "    \"\"\"\n",
        "    cluster_tree = clusters_at_height(data_clustering(table, distance=Cosine, linkage=WEIGHTED), thresold)\n",
        "\n",
        "    list_cluster = {}\n",
        "    for i, cluster in enumerate(cluster_tree):\n",
        "        cluster_name     = 'C' + str(i) \n",
        "\n",
        "        current = []\n",
        "        add_to_list(cluster, current)\n",
        "\n",
        "        for item_index in current:\n",
        "            list_cluster[item_index] = cluster_name\n",
        "        # print(cluster_name, list(map(lambda x: table[x][\"embeddings\"].value, current)))\n",
        "\n",
        "    table = table.add_column(StringVariable(\"Cluster\"), [list_cluster[i] for i in range(len(table))])\n",
        "    return table\n",
        "\n",
        "def compute(lst):\n",
        "    return max(lst,key=lst.count)\n",
        "\n",
        "def one_pass(table, keep_cluster_line : bool = False, cluster_thresold : float = CLUSTER_THRESOLD, sim_thresold : float = SIM_THRESOLD):\n",
        "    assert GROUP_BY in list(map(lambda x: x.name, chain(generic_table.domain.metas, generic_table.domain.variables, generic_table.domain.attributes))), \"Group by not in the Table !\"\n",
        "\n",
        "    table = clusterize(table, cluster_thresold)\n",
        "\n",
        "    #===========================================================================\n",
        "    # Cluster split\n",
        "    toguess_cluster = [d[\"Cluster\"] for d in table if d[\"embeddings\"] == \"TOGUESS\"][0]\n",
        "\n",
        "    in_cluster_table  = Table.from_list(table.domain, [d for d in table if d[\"Cluster\"].value == toguess_cluster])\n",
        "    out_cluster_table = Table.from_list(table.domain, [d for d in table if d[\"Cluster\"].value != toguess_cluster])\n",
        "    \n",
        "    #===========================================================================\n",
        "    # Group by computation\n",
        "\n",
        "    filter_list = []\n",
        "    for row in in_cluster_table:\n",
        "        filter_list.append(row[GROUP_BY].value)\n",
        "\n",
        "    main_superclass = compute(filter_list)\n",
        "    main_superclass_table = Table.from_list(superclass_embeddings.domain, [i for i in superclass_embeddings if i[\"embeddings\"] == main_superclass])\n",
        "    main_superclass_table = Table.concatenate([in_cluster_table, Table.from_table(out_cluster_table.domain, main_superclass_table)])\n",
        "    #===========================================================================\n",
        "    # thresold computation\n",
        "\n",
        "    to_copy_row_instance = [d for d in main_superclass_table if d[\"embeddings\"] == \"TOGUESS\"][0]\n",
        "    to_copy = list(to_copy_row_instance.attributes())\n",
        "\n",
        "    to_compare_row_instance = [d for d in main_superclass_table if d[\"Cluster\"] == \"?\"][0]\n",
        "    to_compare = list(to_compare_row_instance.attributes())\n",
        "\n",
        "    dead_row = [k for k, (i, j) in enumerate(zip(to_copy, to_compare)) if abs(i - j) < sim_thresold]\n",
        "\n",
        "    #===========================================================================\n",
        "    # reconstruct the table filtering dead row and cluster. Remove used cluster row if \n",
        "    # keep_cluster_line is set to False\n",
        "    new_domain = Domain(attributes = [i for i in out_cluster_table.domain.attributes if int(i.name) not in dead_row], \n",
        "                        metas      = [i for i in out_cluster_table.domain.metas if i.name != \"Cluster\"])\n",
        "\n",
        "    # do the same on the data\n",
        "    data_attr = []\n",
        "    data_meta = []\n",
        "    whole_data = list(out_cluster_table) + list(toguess_table)\n",
        "    if keep_cluster_line: whole_data += list(in_cluster_table)\n",
        "\n",
        "    for rowinstance in whole_data:\n",
        "        data_attr.append([rowinstance[k] for k, i in enumerate(out_cluster_table.domain.attributes) if int(i.name) not in dead_row])\n",
        "        data_meta.append([rowinstance.metas[k] for k, i in enumerate(out_cluster_table.domain.metas) if i.name != \"Cluster\"])\n",
        "\n",
        "    return Table.from_numpy(new_domain, X = data_attr, metas = data_meta), \\\n",
        "        { \"cluster\" : {\n",
        "                \"name\" : toguess_cluster,\n",
        "                \"size\" : len(in_cluster_table),\n",
        "                \"thresold\": cluster_thresold\n",
        "            },\n",
        "           \"keep_cluster_line\"  : keep_cluster_line,\n",
        "           \"sim_thresold\"       : sim_thresold,\n",
        "           \"removed_col\"        : len(dead_row) \n",
        "        }"
      ],
      "metadata": {
        "id": "nPhqx45tbDSE"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generic_table = left_join(generic_table, supp_info_table)\n",
        "\n",
        "toguess_table = Table.from_numpy(generic_table.domain, [np.array(totest)], Y = None, metas = np.char.asarray([[\"TOGUESS\", \"?\", \"?\"]]))\n",
        "table = Table.concatenate([generic_table, toguess_table])"
      ],
      "metadata": {
        "id": "w220OOjZUJef"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = []\n",
        "\n",
        "for i in tqdm(range(5), total=5, desc=\"hierarchical clustering pass\"):\n",
        "    table, data = one_pass(table)\n",
        "    data_list.append(data)"
      ],
      "metadata": {
        "id": "BnjjnjQuFJxe",
        "outputId": "fcd9bfde-43ce-4f35-ded5-ad19c955cb59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "hierarchical clustering pass:  40%|████      | 2/5 [00:08<00:12,  4.28s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-128-2deb7eccc33d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hierarchical clustering pass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdata_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-126-9bdb1a139412>\u001b[0m in \u001b[0;36mone_pass\u001b[0;34m(table, keep_cluster_line, cluster_thresold, sim_thresold)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mto_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_copy_row_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mto_compare_row_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmain_superclass_table\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Cluster\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"?\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0mto_compare\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_compare_row_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\".join(data_list))"
      ],
      "metadata": {
        "id": "mfu1nJ0bMhVl",
        "outputId": "5239673c-3c9b-47b2-c739-f255df41f78f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.122, 2.5493, -6.4934, 5.1244, 1.8322, -2.4903, 1.7885, -0.6473, 2.7094, -1.7041, -2.1201, 1.7909, -3.4064, -1.5044, -2.3428, -0.51735, -0.87518, -1.8803, 4.1981, -0.57839, -0.75715, -3.2789, -2.5323, 3.5364, -1.1887, -0.32185, 0.83655, -2.5367, -1.5664, -1.6144, 0.96499, 1.101, 5.7738, -1.4313, 1.6498, -1.4691, -1.2193, 4.6139, 1.1892, 0.95094, 0.28716, 5.3983, -2.1434, 1.9556, -2.588, 3.8573, 1.706, -1.4814, -3.8649, 3.3019, 1.1959, 2.1542, -0.88526, -2.5411, -0.2668, -2.5881, 1.697, 5.5959, -4.8047, -1.3409, 0.88701, 2.4658, -2.609, 1.9621, 3.1595, 2.8576, -0.74223, -0.89093, -4.2198, 3.4845, -1.988, 1.8015, -1.7093, -3.737, -2.9994, 0.74645, -5.3243, 1.5403, -0.60917, -4.2911, -2.9025, -1.7992, -0.8596, 0.21249, -3.7441, 0.28825, -0.88344, -2.254, -1.3114, -2.6239, -1.3166, 2.6839, -1.4652, -0.83398, 2.9815, -4.0996, -2.8034, 3.9212, -2.2096, -0.87033, 1.7632, -2.8293, -2.144, 2.2181, 2.7939, -3.6677, 5.0149, -0.79786, 2.0267, -0.1555, -2.2854, 1.7482, 0.88181, 4.9972, 3.4045, 6.2988, -2.4562, -0.41591, -1.8707, -2.8566, -3.9587, -2.1874, -2.5128, 3.8934, -0.45989, 0.67877, 1.5006, 3.0004, 2.9907, 3.9425, 3.0808, -2.2809, -1.0965, -0.55863, 1.0862, -2.4945, 5.6565, -2.087, -1.2479, 4.0543, -0.60213, -0.53165, 3.5236, -1.9206, 1.6709, 1.2394, 0.50941, 3.7742, -2.5508, -0.50954, -2.0542, 1.1629, 1.821, 2.4256, -3.0941, 2.8343, 0.6906, 0.59896, 1.784, 0.9476, 2.8348, 0.64667, 4.4519, 4.7255, -0.99693, -5.1451, -2.0515, 2.8238, 2.2864, -0.86283, 0.97159, -0.4185, 8.5407, -1.5291, 1.0881, 2.1247, -4.0368, 3.1599, 1.4262, -1.8739, 2.0356, 2.9822, 1.5995, -0.69884, 1.3198, 0.92781, 4.842, 2.0718, -5.3633, -0.47037, -2.0025, 0.71227, 1.6302, -1.3304, -1.6301, 0.92339, 1.2812, -2.1767, 3.3847, 2.6614, -0.89659, -0.3342, 1.2194, -2.9878, 3.2775, -1.5613, 0.30171, -3.9471, 1.941, 0.83132, 0.21692, 0.93135, 7.3327, -1.6191, 5.4914, 1.9914, -3.0784, 1.3881, -1.5109, 2.6814, -3.7304, 2.3987, -4.45, -1.5211, -5.2136, 0.40236, -0.50133, -3.9962, 3.2868, 2.6498, 0.94902, -2.5184, 1.526, 2.8469, -0.35854, -3.2803, 2.3363, -0.56372, -1.8674, 1.3576, -1.5291, -1.4955, -4.8288, 1.019, -1.4444, 3.4142, 2.5302, 1.64, -1.2125, -0.96021, -5.5353, -3.1208, 0.59937, 3.9008, 1.6573, -1.2077, -1.8282, -2.2147, -0.50857, -1.5127, 0.44002, 1.3732, 1.5556, 0.93295, -1.888, -1.9693, 2.7558, 1.6251, 4.6096, -1.1334, -0.95527, 0.79511, 2.7323, -1.4546, 2.6422] {TOGUESS, ?, ?}\n",
            "275\n",
            "{'cluster': {'name': Value('Cluster', \"C6\"), 'size': 109, 'thresold': 0.75}, 'keep_cluster_line': False, 'sim_thresold': 0.3, 'removed_col': 25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test part"
      ],
      "metadata": {
        "id": "ax8Aw_qVRxer"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tests "
      ],
      "metadata": {
        "id": "AW1u-xshmuZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test superclass"
      ],
      "metadata": {
        "id": "ols0X2IDmqJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Test:\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.vocab = []\n",
        "    \n",
        "    def _start(self, model : WordToVector, articlesRetriever : ArticleRetriever):\n",
        "        articlesRetriever.set_list_vocab(f\"{self.name}.art\", self.vocab)\n",
        "\n",
        "        if articlesRetriever(force_reload = False):\n",
        "            articlesRetriever.save()\n",
        "\n",
        "        self.save_file = f\"test-{self.name}-{model.model_size}-{model.window_size}.csv\"\n",
        "        model.set_list_class(self.vocab)\n",
        "\n",
        "        if not model.check_embeddings_exist(self.save_file, articlesRetriever):\n",
        "            model.convert(articlesRetriever)\n",
        "            model.export(self.save_file)\n",
        "\n",
        "\n",
        "    def _end(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __call__(self, model, articlesRetriever):\n",
        "\n",
        "        logging.info(f\"Start test {self.name}\")\n",
        "\n",
        "        tic = perf_counter()\n",
        "        self._start(model, articlesRetriever)\n",
        "        result = self._end(model)\n",
        "        toc = perf_counter()\n",
        "\n",
        "        logging.info(f\"End test {self.name}\")\n",
        "\n",
        "        return result, toc - tic"
      ],
      "metadata": {
        "id": "OQcNCqBSmo9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding distance Test"
      ],
      "metadata": {
        "id": "dFbjPbSVpzno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingDistanceTest(Test):\n",
        "\n",
        "    def __init__(self, vocab, thresold, name):\n",
        "        Test.__init__(self, name)\n",
        "\n",
        "        self.thresold = thresold\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def _end(self, model):\n",
        "        \n",
        "        sim_matrix = SimilarityMatrix(self.save_file, CosineSim())\n",
        "        ids, cosine_mat = sim_matrix.get_sim_matrix()\n",
        "\n",
        "        sim_list = []\n",
        "\n",
        "        for i, idsa in tqdm(enumerate(ids), total = len(ids), desc=f\"{'listing sim matrix':30}\"):\n",
        "            for j, idsb in enumerate(ids):\n",
        "\n",
        "                if i == j: continue \n",
        "\n",
        "                cos_val = cosine_mat[i][j]\n",
        "\n",
        "                if cos_val >= self.thresold:\n",
        "                    sim_list.append((idsa, idsb, cos_val))\n",
        "\n",
        "        sim_list.sort(key=lambda x: x[2], reverse = True)\n",
        "        return len(sim_list) // 2\n"
      ],
      "metadata": {
        "id": "w9ibkPUfp5Tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Syntactic test"
      ],
      "metadata": {
        "id": "rYWi-UIguOly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SyntacticTest(Test):\n",
        "\n",
        "    def __init__(self, quadruple_set : Tuple[str, str, str, str], name : str):\n",
        "        Test.__init__(self, name)\n",
        "\n",
        "        self.relations : Tuple[str, str, str, str] = quadruple_set\n",
        "\n",
        "        for relation in self.relations:\n",
        "            for item in relation:\n",
        "                if item not in self.vocab:\n",
        "                    self.vocab.append(item)\n",
        "\n",
        "    def _end(self, model):\n",
        "\n",
        "        solver = Solver(self.save_file)\n",
        "        top1, top3, top5, top10 = 0, 0, 0, 0\n",
        "\n",
        "        for w1, w2, w3, w4 in tqdm(self.relations, total=len(self.relations), desc=f\"{'calculating relations':30}\"):\n",
        "\n",
        "            w1_emb = model.get_embedding_of(w1).numpy()\n",
        "            w2_emb = model.get_embedding_of(w2).numpy()\n",
        "            w3_emb = model.get_embedding_of(w3).numpy()\n",
        "\n",
        "            totest = w1_emb - w2_emb + w3_emb\n",
        "            result = solver.get_nearest_embedding_of(torch.from_numpy(totest), 13)\n",
        "            filtered_result = list(filter(lambda x: x != None, map(lambda x: x[0] if x[0] not in [w1, w2, w3] else None, result)))\n",
        "\n",
        "            if filtered_result[0] == w4:\n",
        "                top1 += 1\n",
        "\n",
        "            if w4 in filtered_result[:3]:\n",
        "                top3 += 1\n",
        "\n",
        "            if w4 in filtered_result[:5]:\n",
        "                top5 += 1\n",
        "\n",
        "            if w4 in filtered_result[:10]:\n",
        "                top10 += 1\n",
        "\n",
        "        \n",
        "        return list(map(lambda x : x / len(self.relations), (top1, top3, top5, top10)))"
      ],
      "metadata": {
        "id": "d-BMuVTTuQ2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similarity Test"
      ],
      "metadata": {
        "id": "QDP8gaRAm1s5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimilarityTest(Test):\n",
        "\n",
        "    def __init__(self, pair_set : Tuple[str, str, float], pair_name):\n",
        "        Test.__init__(self, pair_name)\n",
        "\n",
        "        self.pair : Tuple[str, str, float] = pair_set\n",
        "\n",
        "        for w1, w2, i in self.pair:\n",
        "            if w1 not in self.vocab:\n",
        "                self.vocab.append(w1)\n",
        "            if w2 not in self.vocab:\n",
        "                self.vocab.append(w2)\n",
        "    \n",
        "    def _end(self, model):\n",
        "        sim_list = []\n",
        "        i_list = []\n",
        "\n",
        "        sim_computer = SimilarityMatrix(self.save_file, CosineSim())\n",
        "\n",
        "        for w1, w2, i in tqdm(self.pair, total=len(self.pair), desc=f\"{'calculating pair':30}\"):\n",
        "            sim = sim_computer.sim_between(w1, w2)\n",
        "\n",
        "            sim_list.append(sim)\n",
        "            i_list.append(i)\n",
        "\n",
        "        result = spearmanr(sim_list, i_list)\n",
        "        return (result.correlation, result.pvalue)"
      ],
      "metadata": {
        "id": "zMBHN_B5bQym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_list(listed):\n",
        "    print(f\"{'word':20}{'target':20}{'similarity':10}\")\n",
        "    print(\"=\"*50)\n",
        "    for ida, idb, sim in listed:\n",
        "        print(f\"{ida:20}{idb:20}{round(float(sim), 6):6}\")\n"
      ],
      "metadata": {
        "id": "Kiq-UPEApL0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline"
      ],
      "metadata": {
        "id": "JzRqbWiHm4mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestPipeline():\n",
        "\n",
        "    list_test = [ SimilarityTest(nlp.data.WordSim353('all'), \"Wordsim353\"), \n",
        "                  SimilarityTest(nlp.data.SimLex999('all') , \"SimLex999\") ,\n",
        "                  SyntacticTest(nlp.data.GoogleAnalogyTestSet(), \"GoogleAnalogy\"),\n",
        "                  EmbeddingDistanceTest(cifar100, 0.80, \"Imagenet\")\n",
        "                ]\n",
        "\n",
        "    def __init__(self, model, articleRetriever, list_test = None):\n",
        "        self.model = model\n",
        "        self.articleRetriver = articleRetriever\n",
        "\n",
        "        if list_test == None:\n",
        "            self.list_test = TestPipeline.list_test\n",
        "        else:\n",
        "            self.list_test = list_test \n",
        "\n",
        "    def execute(self):\n",
        "\n",
        "        for i, test in enumerate(self.list_test):\n",
        "\n",
        "            print(f\"Test {i} : {test.name}\".center(80, \"=\"))\n",
        "            res, time_elapsed = test(self.model, self.articleRetriver)\n",
        "            print(f\"\\n{res}\")\n",
        "            print(f\"{round(time_elapsed, 2)} sec.\".center(80, \"=\"))"
      ],
      "metadata": {
        "id": "SlsqEiRqm8dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main"
      ],
      "metadata": {
        "id": "YCvIISI4E2VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "X8f8wEuBH2Uj",
        "outputId": "cdd46a32-dbec-463d-e511-4bd7a5f09691",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22132"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def all_models_gen():\n",
        "    # yield BERTModel([], big = False, window = 0)    \n",
        "    # yield BERTModel([], big = False, window = 100)\n",
        "    # yield BERTModel([], big = False, window = 300)\n",
        "    # yield BERTModel([], big = False, window = 512)\n",
        "    # yield BERTModel([], big = True, window = 0)\n",
        "    # yield BERTModel([], big = True, window = 100)\n",
        "    # yield BERTModel([], big = True, window = 300)\n",
        "    # yield BERTModel([], big = True, window = 512)\n",
        "    # yield ROBERTAModel([], big = False, window = 0)\n",
        "    # yield ROBERTAModel([], big = False, window = 100)\n",
        "    # yield ROBERTAModel([], big = False, window = 300)\n",
        "    # yield ROBERTAModel([], big = False, window = 512)\n",
        "    # yield ROBERTAModel([], big = True, window = 0)\n",
        "    # yield ROBERTAModel([], big = True, window = 100)\n",
        "    # yield ROBERTAModel([], big = True, window = 300)\n",
        "    # yield ROBERTAModel([], big = True, window = 512)\n",
        "    # yield DocBERTModel([], big = False)\n",
        "    # yield DocBERTModel([], big = True)\n",
        "    # yield DocBERTAModel([], big = False)\n",
        "    # yield DocBERTAModel([], big = True)\n",
        "    # yield Wiki2VecModel([])\n",
        "    yield GloVEModel([])\n",
        "\n",
        "def split_test(name):\n",
        "    print()\n",
        "    print(\"#\" * 80)\n",
        "    print(\"#\", name.center(78, \" \"), \"#\", sep=\"\")\n",
        "    print(\"#\" * 80)\n",
        "    print()\n",
        "\n",
        "def one_test_all_model(test_pipeline, articleRetriever):\n",
        "\n",
        "    for model in all_models_gen():\n",
        "        split_test(f\"{model.model_size} {model.window_size}\")\n",
        "        TestPipeline(model, articleRetriever, test_pipeline).execute()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "def all_test_all_model(articleRetriever):\n",
        "\n",
        "    for model in all_models_gen():\n",
        "        split_test(f\"{model.model_size} {model.window_size}\")\n",
        "        TestPipeline(model, articleRetriever).execute()\n",
        "        gc.collect()\n",
        "\n",
        "gc.collect()\n",
        "logging.info('Starting Main'.center(40, \"=\"))\n",
        "\n",
        "wikiRetriever = WikipediaArticleRetriever()\n",
        "wordRetriever = WordNetArticleRetriever()\n",
        "\n",
        "all_test_all_model(wordRetriever)\n"
      ],
      "metadata": {
        "id": "DhjlqisSgL7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d8654a0-2dd3-476d-d6b1-70ed4f160da3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################################################################################\n",
            "#                                  GloVe 300                                   #\n",
            "################################################################################\n",
            "\n",
            "==============================Test 0 : Wordsim353===============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading articles              : 100%|██████████| 437/437 [00:00<00:00, 538457.95it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: DeprecationWarning: string or file could not be read to its end due to unmatched data; this will raise a ValueError in the future.\n",
            "calculating pair              : 100%|██████████| 352/352 [00:00<00:00, 9956.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "(0.5755996374954984, 1.9089867180240961e-32)\n",
            "==================================110.55 sec.===================================\n",
            "===============================Test 1 : SimLex999===============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading articles              : 100%|██████████| 1028/1028 [00:00<00:00, 249305.84it/s]\n",
            "calculating pair              : 100%|██████████| 999/999 [00:00<00:00, 13526.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "(0.3764003606135919, 5.611502131461014e-35)\n",
            "==================================105.35 sec.===================================\n",
            "=============================Test 2 : GoogleAnalogy=============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading articles              : 100%|██████████| 905/905 [00:00<00:00, 350623.05it/s]\n",
            "calculating relations         : 100%|██████████| 19544/19544 [23:03<00:00, 14.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[0.08089439214081048, 0.16537044617273844, 0.21014121981170691, 0.2770671305771592]\n",
            "==================================1490.48 sec.==================================\n",
            "===============================Test 3 : Imagenet================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading articles              : 100%|██████████| 100/100 [00:00<00:00, 3997.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 items haven't been found, resolve mode.\n",
            "press enter to resume resolve\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "computing sim matrix          : 100%|██████████| 100/100 [00:00<00:00, 183.69it/s]\n",
            "listing sim matrix            : 100%|██████████| 100/100 [00:00<00:00, 589.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2\n",
            "==================================697.38 sec.===================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JxyvGzFO_dDQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}