{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordsEmbeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taedriel/ZSL-v2/blob/wordEmbedding/WordsEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYxydB4dlTpi"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# %cd /content/drive/MyDrive/Kingston/ZSL-v2/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers wget tensorflow_datasets wikipedia"
      ],
      "metadata": {
        "id": "HEq14T6Nl535"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import wikipedia\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level = logging.INFO)\n",
        "\n",
        "from typing import List\n",
        "from os.path import exists\n"
      ],
      "metadata": {
        "id": "spXWSv2iW9i5"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WordToVecteur:\n",
        "\n",
        "    def __init__(self, list_tags : List[str] = []):\n",
        "        self.list_tags = list_tags\n",
        "\n",
        "    def export(self, filename):\n",
        "        return NotImplementedError\n",
        "\n",
        "    def importTagList(self, filename):\n",
        "        return NotImplementedError\n",
        "\n",
        "    def convert(self):\n",
        "        return NotImplementedError\n",
        "\n",
        "\n",
        "class BERTModel(WordToVecteur):\n",
        "\n",
        "    def __init__(self, list_tag : List[str] = [], big: bool = False):\n",
        "        super(BERTModel, self).__init__(list_tag)\n",
        "        self.list_articles = {}\n",
        "\n",
        "        self.model_size = \"bert-large-uncased\" if big else \"bert-base-uncased\"\n",
        "        self.embeddings = []\n",
        "        self.cosine_sim_matrix = None\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.model_size, padding=True, truncation=True,)\n",
        "        self.model = BertModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "\n",
        "    def export(self, filename):\n",
        "        if len(self.embeddings) == 0:\n",
        "            raise Exception(\"Tags not converted yet !\")\n",
        "        \n",
        "        try:\n",
        "            f = open(filename, \"w\")\n",
        "        except OSError:\n",
        "            raise OSError(\"Could not open file\")\n",
        "\n",
        "        with f:\n",
        "            for embedding in self.embeddings:\n",
        "                line = \",\".join(list(map(lambda x: str(float(x)), embedding[1])))\n",
        "                print(embedding[0], \",\", line, sep=\"\", file=f)\n",
        "\n",
        "    def import_tag_list(self, filename):\n",
        "        self.embeddings.clear()\n",
        "        self.cosine_sim_matrix = None\n",
        "\n",
        "        try:\n",
        "            f = open(filename, \"r\")\n",
        "        except OSError:\n",
        "            return OSError(\"Could not open file\")\n",
        "\n",
        "        with f:\n",
        "            data = f.read().split(\"\\n\")\n",
        "            for item in data:\n",
        "                if item not in self.list_tags and str.strip(item) != \"\":\n",
        "                    self.list_tags.append(item)\n",
        "            print(self.list_tags)\n",
        "            logging.info(f\"Import finished : {len(self.list_tags)} elements imported.\")\n",
        "    \n",
        "    def retrieve_article(self, use_Proxy=True):\n",
        "        logging.info(\"Starting retrieveing articles...\")\n",
        "        \n",
        "        for tag in self.list_tags:\n",
        "\n",
        "            article = None\n",
        "\n",
        "            if use_Proxy and exists(tag):\n",
        "                with open(tag, \"r\") as f:\n",
        "                    article = f.read()\n",
        "            else:\n",
        "                search_result = wikipedia.search(tag)\n",
        "                article = wikipedia.page(search_result[0])\n",
        "\n",
        "                if use_Proxy:\n",
        "                    with open(tag, \"w\") as f:\n",
        "                        print(article, file=f)\n",
        "\n",
        "            self.list_article[tag] = article.summary\n",
        "\n",
        "        logging.info(\"Finished retrieving article !\")\n",
        "\n",
        "\n",
        "    def convert(self):\n",
        "        \"\"\" convert all word in their embeddings\"\"\"\n",
        "        if len(self.list_tags) == 0:\n",
        "            raise Exception(\"not tags yet !\")\n",
        "\n",
        "        if len(self.list_tags) != len(self.list_articles):\n",
        "            self.retrieve_article()\n",
        "\n",
        "        logging.info(\"Starting converting tokens...\")\n",
        "        nb_token = len(self.list_tags)\n",
        "        current_percent = 0.00\n",
        "\n",
        "        for i, tag in enumerate(self.list_tags):\n",
        "            \n",
        "            percent_completion = round((i / nb_token) * 100, 2)\n",
        "            if current_percent != percent_completion:\n",
        "                logging.info(f\"{percent_completion}% completed\")\n",
        "                current_percent = percent_completion\n",
        "            \n",
        "            tag_plus_context = tag + \". \" + self.list_articles[tag]\n",
        "            inputs = self.tokenizer(tag_plus_context, return_tensors = \"pt\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            hidden_states = outputs[2]\n",
        "\n",
        "            # log.info(f\"[{i}]\",\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
        "            # log.info(f\"[{i}]\",\"Number of batches:\", len(hidden_states[0]))\n",
        "            # logging.info(f\"[{i}] Number of tokens: {len(hidden_states[0][0]) - 2}\")\n",
        "            # log.info(f\"[{i}]\",\"Number of hidden units:\", len(hidden_states[0][0][0]))\n",
        "\n",
        "            # [# layers, # batches, # tokens, # features] ==> [# tokens, # layers, # features]\n",
        "            token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "            token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "            token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "            # apply different strategy to summarize word embeddings\n",
        "            # tokenized_text = self.tokenizer.tokenize(tag)\n",
        "            # acc = []\n",
        "            # for i, token in reversed(list(enumerate(tokenized_text))):\n",
        "\n",
        "            #     embed = self.merging_strategy.merge(token_embeddings[i+1])\n",
        "            #     if i == 0:\n",
        "            #         if len(acc) != 0:\n",
        "            #             embed = torch.mean(torch.stack([x[1] for x in acc]), dim=0)\n",
        "            #             token = tag\n",
        "            #             acc = []\n",
        "\n",
        "            #         self.embeddings.append((token, embed))\n",
        "            #     else:\n",
        "            #         acc.append((token, embed))\n",
        "            \n",
        "            # here we are just taking the [CLS] (for classification) as an embedding for the tag\n",
        "            self.embeddings.append((tag, self.merging_strategy.merge(token_embeddings[0])))\n",
        "\n",
        "\n",
        "    def compute_sim(self):\n",
        "        \"\"\" compute cosine similarity between all vectors \"\"\"\n",
        "        if len(self.embeddings) == 0:\n",
        "            raise Exception(\"Tags not converted yet !\")\n",
        "\n",
        "        logging.info(\"Computing cosine similarity, this could take some time...\")\n",
        "\n",
        "        n_tokens = len(self.embeddings)\n",
        "        self.cosine_sim_matrix = [[1 for j in range(n_tokens)] for i in range(n_tokens)]\n",
        "\n",
        "        for j, vector in enumerate(self.embeddings):\n",
        "\n",
        "            for i, other_vector in enumerate(self.embeddings):\n",
        "\n",
        "                if i == j:\n",
        "                    continue\n",
        "\n",
        "                \n",
        "                cos = torch.nn.CosineSimilarity(dim=0)\n",
        "                similarity = cos(vector[1], other_vector[1])\n",
        "\n",
        "                self.cosine_sim_matrix[i][j] = similarity\n",
        "                self.cosine_sim_matrix[j][i] = similarity\n",
        "\n",
        "    def export_sim_matrix(self, filename):\n",
        "        if self.cosine_sim_matrix == None:\n",
        "            self.compute_sim()\n",
        "        \n",
        "        try:\n",
        "            f = open(filename, \"w\")\n",
        "        except OSError:\n",
        "            raise OSError(\"Could not open file\")\n",
        "\n",
        "        with f:\n",
        "            print(\"/\", \",\".join([tag[0] for tag in self.embeddings]), sep = \",\", file = f)\n",
        "\n",
        "            for j, tag_y in enumerate(self.embeddings):\n",
        "                print(tag_y[0], \",\".join( [str(round(float(self.cosine_sim_matrix[j][i]), 3)) for i in range(len(self.embeddings))]), sep = \",\", file = f)\n",
        "\n",
        "    def sim_between(self, token1, token2):\n",
        "        if self.cosine_sim_matrix is None:\n",
        "            self.compute_co_sim()\n",
        "\n",
        "        index1 = [i for i, v in enumerate(self.embeddings) if v[0] == token1][0]\n",
        "        index2 = [i for i, v in enumerate(self.embeddings) if v[0] == token2][0]\n",
        "\n",
        "        return self.cosine_sim_matrix[index1][index2]\n",
        "\n",
        "    def get_embedding_of(self, token):\n",
        "        res = [v for v in self.embeddings if v[0] == token]\n",
        "        if len(res) == 0:\n",
        "            raise Exception(\"no such token\")\n",
        "        \n",
        "        return res[0]\n",
        "\n",
        "    def get_nearest_embedding_of(self, embedding, nb = 10):\n",
        "\n",
        "        if nb > len(self.embeddings):\n",
        "            raise Exception(\"nb too high, not enough token\")\n",
        "\n",
        "        nearest = []\n",
        "        for e in self.embeddings:\n",
        "\n",
        "            cos = torch.nn.CosineSimilarity(dim=0)\n",
        "            similarity = cos(embedding, e[1])\n",
        "\n",
        "            nearest.append((e[0], similarity))\n",
        "        \n",
        "        nearest.sort(key = lambda tup : tup[1])\n",
        "        return nearest[-nb:]\n",
        "\n",
        "    def get_class_list(self):\n",
        "        return [x[0] for x in self.embeddings]\n",
        "\n",
        "class Sum4LastLayers:\n",
        "\n",
        "    def merge(self, vector):\n",
        "        return torch.sum(vector[-4:], dim = 0)"
      ],
      "metadata": {
        "id": "UVJUSGWQorYj"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
        "imagenet_labels = np.array(open(labels_path).read().splitlines())\n",
        "imagenet_labels = list(imagenet_labels)\n",
        "len(imagenet_labels)"
      ],
      "metadata": {
        "id": "NIJ8zsk-UTuh",
        "outputId": "8f47e8ab-17bb-409c-b78c-3cf7161e4726",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt\n",
            "16384/10484 [==============================================] - 0s 0us/step\n",
            "24576/10484 [======================================================================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1001"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [\"king\", \"queen\", \"man\", \" woman\", \"splurgle\", \"pladonf\"]\n",
        "\n",
        "model = BERTModel(imagenet_labels, big = True)\n",
        "\n",
        "# model.import_tag_list(\"en-basic\")\n",
        "model.convert()\n",
        "model.export(\"imageNet-embeddeding.csv\")\n",
        "print(model.get_class_list())\n",
        "\n",
        "# model.computeCoSim()\n",
        "# model.simBetween(\"cat\", \"dog\")\n",
        "\n",
        "# man = model.get_embedding_of(\"man\")[1]\n",
        "# woman = model.get_embedding_of(\"woman\")[1]\n",
        "\n",
        "# king = model.get_embedding_of(\"king\")[1]\n",
        "\n",
        "# totest = king.sub(man).add(woman)\n",
        "# print(model.get_nearest_embedding_of(totest, 3))\n",
        "\n",
        "# model.export_sim_matrix(\"sim_matrix.csv\")"
      ],
      "metadata": {
        "id": "WOOlgG8koVD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content\""
      ],
      "metadata": {
        "id": "TxVz3vl-WxlJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}