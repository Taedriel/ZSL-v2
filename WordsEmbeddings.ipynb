{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordsEmbeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taedriel/ZSL-v2/blob/wordEmbedding/WordsEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yes | pip install transformers wget tensorflow_datasets wikipedia deprecated unzip gluonnlp mxnet --quiet\n",
        "!mkdir -p temp article"
      ],
      "metadata": {
        "id": "HEq14T6Nl535"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "import wikipedia\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import mxnet as mx\n",
        "import gluonnlp as nlp\n",
        "import traceback\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level = logging.INFO, filename = \"BERT.log\" )\n",
        "# logging.basicConfig(level = logging.INFO)\n",
        "\n",
        "from tqdm import tqdm\n",
        "from deprecated import deprecated\n",
        "from typing import List, Tuple\n",
        "from os.path import exists, join\n",
        "from enum import Enum\n",
        "\n",
        "wikipedia.set_rate_limiting(True)"
      ],
      "metadata": {
        "id": "spXWSv2iW9i5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class customWikiArticle:\n",
        "\n",
        "    def __init__(self, index : int, title : str, realtitle : str, summary : str, ambiguous : bool):\n",
        "        self.index = index\n",
        "        self.title = title\n",
        "        self.realtitle = realtitle\n",
        "        self.summary = summary\n",
        "        self.ambiguous = ambiguous\n",
        "\n",
        "class ArticleRetriever:\n",
        "\n",
        "    article_dir = \"./article\"\n",
        "\n",
        "    def __init__(self, name : str, list_title : List[str]):\n",
        "\n",
        "        self.name = name\n",
        "        self.list_title = list_title\n",
        "\n",
        "        if not exists(self.get_filename()):\n",
        "            self.articles_map = {}\n",
        "        else:\n",
        "            with open(self.get_filename(), \"rb\") as mapfile:\n",
        "                self.articles_map = pickle.load(mapfile)\n",
        "\n",
        "    def get_filename(self):\n",
        "        return join(ArticleRetriever.article_dir,self.name)\n",
        "\n",
        "    def load_article(self, title, force_reload : bool = False) -> customWikiArticle:\n",
        "        if title not in self.articles_map:\n",
        "            realtitle, summary, ambiguous = self._retrieve_article(title)\n",
        "            self.articles_map[title] = customWikiArticle(len(self.articles_map), title, realtitle, summary, ambiguous)\n",
        "\n",
        "        if title in self.articles_map and self.articles_map[title].summary == None and force_reload:\n",
        "            realtitle, summary, ambiguous = self._retrieve_article(title)\n",
        "            self.articles_map[title].summary = summary\n",
        "\n",
        "        return self.articles_map[title]\n",
        "\n",
        "    def _retrieve_article(self, title, closed_list : List = []) -> Tuple[str, str, bool]: \n",
        "        closed_list.append(title)\n",
        "        try:\n",
        "            article = wikipedia.page(title, auto_suggest=False, redirect=True)\n",
        "            return (article.title, article.summary, False)\n",
        "\n",
        "        except wikipedia.PageError as e:\n",
        "            search_result = wikipedia.search(title, suggestion = False)\n",
        "\n",
        "            logging.warning(f\"{title} misspelled or article missing. Best find is {search_result[0]}\")\n",
        "            if search_result[0] is not None and search_result[0] not in closed_list:            \n",
        "                return self._retrieve_article(search_result[0], closed_list)  \n",
        "            else: return (title, None, False)\n",
        "\n",
        "        except wikipedia.DisambiguationError as e:\n",
        "            logging.warning(f\"{title} is ambiguous, trying first {e.options[0]}\")\n",
        "            if e.options[0] is not None and e.options[0] not in closed_list:\n",
        "                res = self._retrieve_article(e.options[0], closed_list)\n",
        "                return (res[0], res[1], True)\n",
        "        \n",
        "            return (None, None, None)\n",
        "\n",
        "\n",
        "    def load_all_articles(self, force_reload : bool = False) -> None:\n",
        "        logging.info(f\"Starting loading articles...\")\n",
        "        nb_success = 0\n",
        "\n",
        "        nb_article = len(self.list_title)\n",
        "        for i, title in tqdm(enumerate(self.list_title), total=nb_article):\n",
        "            self.load_article(title, force_reload)\n",
        "\n",
        "            if self.articles_map[title].summary is not None: \n",
        "                nb_success += 1\n",
        "        logging.info(f\"Finished loading {nb_success} article(s) !\")\n",
        "\n",
        "\n",
        "    def get_article(self, title):\n",
        "        if title not in self.articles_map:\n",
        "            self.load_article(title)\n",
        "\n",
        "        return self.articles_map[title]\n",
        "        \n",
        "    def save(self):\n",
        "        with open(self.get_filename(), \"wb\") as mapfile:\n",
        "            pickle.dump(self.articles_map, mapfile)\n",
        "\n",
        "class WordToVector:\n",
        "\n",
        "    def __init__(self, list_tags : List[str] = []):\n",
        "        self.list_tags = list_tags\n",
        "        self.embeddings = {}\n",
        "\n",
        "    def export(self, filename):\n",
        "        return NotImplementedError\n",
        "\n",
        "    def convert(self):\n",
        "        return NotImplementedError\n",
        "\n",
        "\n",
        "class BERTModel(WordToVector):\n",
        "\n",
        "    temp_dir = \"./temp\"\n",
        "\n",
        "    def __init__(self, list_tag : List[str] = [], big: bool = False, window : int = 100):\n",
        "        super(BERTModel, self).__init__(list_tag)\n",
        "        self.list_articles = {}\n",
        "        self.window_size = window\n",
        "\n",
        "        self.model_size = \"bert-large-uncased\" if big else \"bert-base-uncased\"\n",
        "        self.cosine_sim_matrix = None\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.model_size, padding=True, truncation=True,)\n",
        "        self.model = BertModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "    def export(self, filename):\n",
        "        \"\"\"export all the embeddings in filename under a .csv format.\n",
        "           Raise exception if embeddings hasn't been calculed yet.\"\"\"\n",
        "\n",
        "        if len(self.embeddings) == 0:\n",
        "            raise Exception(\"Tags not converted yet !\")\n",
        "        \n",
        "        try:\n",
        "            f = open(filename, \"w\")\n",
        "        except OSError:\n",
        "            raise OSError(\"Could not open file\")\n",
        "\n",
        "        with f:\n",
        "            for tag, embedding in self.embeddings.items():\n",
        "                print(tag, *list(map(lambda x: str(float(x)), embedding)), sep=\",\", file=f)\n",
        "\n",
        "    def reset_embeddings(self):\n",
        "        self.embeddings.clear()\n",
        "\n",
        "    def convert(self, article_ret : ArticleRetriever):\n",
        "        \"\"\" convert all word in their embeddings\"\"\"\n",
        "\n",
        "        if len(self.list_tags) == 0:\n",
        "            raise Exception(\"not tags yet !\")\n",
        "\n",
        "        logging.info(\"Starting converting tokens...\")\n",
        "        nb_token = len(self.list_tags)\n",
        "        for i, tag in tqdm(enumerate(self.list_tags), total = nb_token):\n",
        "            \n",
        "            article = article_ret.get_article(tag)\n",
        "            tag_plus_context = tag\n",
        "            if article.summary is not None:\n",
        "                max_size_article = min(len(article.summary), self.window_size)\n",
        "                tag_plus_context = tag + \". \" + article.summary[:max_size_article]\n",
        "\n",
        "            inputs = self.tokenizer(tag_plus_context, return_tensors = \"pt\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            hidden_states = outputs[2]\n",
        "\n",
        "            # [# layers, # batches, # tokens, # features] ==> [# tokens, # layers, # features]\n",
        "            token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "            token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "            token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "            # apply different strategy to summarize word embeddings\n",
        "            # tokenized_text = self.tokenizer.tokenize(tag)\n",
        "            # acc = []\n",
        "            # for i, token in reversed(list(enumerate(tokenized_text))):\n",
        "\n",
        "            #     embed = self.merging_strategy.merge(token_embeddings[i+1])\n",
        "            #     if i == 0:\n",
        "            #         if len(acc) != 0:\n",
        "            #             embed = torch.mean(torch.stack([x[1] for x in acc]), dim=0)\n",
        "            #             token = tag\n",
        "            #             acc = []\n",
        "\n",
        "            #         self.embeddings.append((token, embed))\n",
        "            #     else:\n",
        "            #         acc.append((token, embed))\n",
        "            \n",
        "            # here we are just taking the [CLS] (for classification) as an embedding for the tag\n",
        "            self.embeddings[tag] = self.merging_strategy.merge(token_embeddings[0])\n",
        "\n",
        "    def get_embedding_of(self, token):\n",
        "        if token not in self.embeddings:\n",
        "            raise Exception(f\"no such token {token}\")\n",
        "        \n",
        "        return self.embeddings[token]\n",
        "\n",
        "    def get_class_list(self):\n",
        "        return self.embeddings.keys()\n",
        "\n",
        "class Sum4LastLayers:\n",
        "\n",
        "    def merge(self, vector):\n",
        "        return torch.sum(vector[-4:], dim = 0)"
      ],
      "metadata": {
        "id": "UVJUSGWQorYj"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingsLoader:\n",
        "\n",
        "    def __init__(self, filename : str):\n",
        "\n",
        "        self.file = filename\n",
        "        self.embeddings = {}\n",
        "\n",
        "        self._load_file()\n",
        "\n",
        "    def _load_file(self):\n",
        "        try:\n",
        "            with open(self.file, \"r\") as f:\n",
        "                lines = f.readlines()\n",
        "                \n",
        "            for line in lines:\n",
        "                data = line.split(\",\")\n",
        "                self.embeddings[data[0]] = torch.FloatTensor(list(map(float, data[1:])))\n",
        "\n",
        "        except IOError as e:\n",
        "            raise IOError(f\"No file {self.file}\")\n",
        "\n",
        "class SimilarityCompute(EmbeddingsLoader):\n",
        "\n",
        "    def __init__(self, embeddings):\n",
        "        super(SimilarityCompute, self).__init__(embeddings)\n",
        "\n",
        "\n",
        "    def compute_sim(self):\n",
        "        \"\"\" compute cosine similarity between all vectors \"\"\"\n",
        "        if len(self.embeddings) == 0:\n",
        "            raise Exception(\"Tags not converted yet !\")\n",
        "\n",
        "        logging.info(\"Computing cosine similarity, this could take some time...\")\n",
        "\n",
        "        if self.cosine_sim_matrix is None:\n",
        "            n_tokens = len(self.embeddings)\n",
        "            self.cosine_sim_matrix = [[1 for j in range(n_tokens)] for i in range(n_tokens)]\n",
        "\n",
        "        for j, vector in tqdm(enumerate(self.embeddings), total = len(self.embeddings)):\n",
        "\n",
        "            for i, other_vector in enumerate(self.embeddings):\n",
        "\n",
        "                if i == j:\n",
        "                    continue\n",
        "\n",
        "                cos = torch.nn.CosineSimilarity(dim=0)\n",
        "                similarity = cos(vector[1], other_vector[1])\n",
        "\n",
        "                self.cosine_sim_matrix[i][j] = similarity\n",
        "                self.cosine_sim_matrix[j][i] = similarity\n",
        "\n",
        "    def export_sim_matrix(self, filename):\n",
        "        if self.cosine_sim_matrix == None:\n",
        "            self.compute_sim()\n",
        "        \n",
        "        try:\n",
        "            f = open(filename, \"w\")\n",
        "        except OSError:\n",
        "            raise OSError(\"Could not open file\")\n",
        "\n",
        "        with f:\n",
        "            print(\"/\", \",\".join([tag[0] for tag in self.embeddings]), sep = \",\", file = f)\n",
        "\n",
        "            for j, tag_y in enumerate(self.embeddings):\n",
        "                print(tag_y[0], \",\".join( [str(round(float(self.cosine_sim_matrix[j][i]), 3)) for i in range(len(self.embeddings))]), sep = \",\", file = f)\n",
        "\n",
        "    def sim_between(self, token1, token2):\n",
        "        index1, v1 = [(i, v[1]) for i, v in enumerate(self.embeddings) if v[0] == token1][0]\n",
        "        index2, v2 = [(i, v[1]) for i, v in enumerate(self.embeddings) if v[0] == token2][0]\n",
        "\n",
        "        if self.cosine_sim_matrix is None:\n",
        "            n_tokens = len(self.embeddings)\n",
        "            self.cosine_sim_matrix = [[1 for j in range(n_tokens)] for i in range(n_tokens)]\n",
        "\n",
        "        if self.cosine_sim_matrix[index1][index2] == 0 or self.cosine_sim_matrix[index2][index1]:\n",
        "            cos = torch.nn.CosineSimilarity(dim=0)\n",
        "            similarity = cos(v1, v2)\n",
        "\n",
        "            self.cosine_sim_matrix[index1][index2] = similarity\n",
        "            self.cosine_sim_matrix[index2][index1] = similarity\n",
        "\n",
        "        return self.cosine_sim_matrix[index1][index2]\n",
        "\n",
        "class Solver(EmbeddingsLoader):\n",
        "\n",
        "    def __init__(self, embeddings):\n",
        "        super(Solver, self).__init__(embeddings)\n",
        "\n",
        "    def get_nearest_embedding_of(self, embedding, nb = 10):\n",
        "\n",
        "        if nb > len(self.embeddings):\n",
        "            raise Exception(\"nb too high, not enough token\")\n",
        "\n",
        "        nearest = []\n",
        "        for tag, e in self.embeddings.items():\n",
        "\n",
        "            cos = torch.nn.CosineSimilarity(dim=0)\n",
        "            similarity = cos(embedding, e)\n",
        "\n",
        "            nearest.append((tag, similarity))\n",
        "        \n",
        "        nearest.sort(key = lambda tup : tup[1])\n",
        "        return nearest[-1:-nb-1:-1]"
      ],
      "metadata": {
        "id": "5AxDIhr1rkca"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
        "# imagenet_labels = np.array(open(labels_path).read().splitlines())\n",
        "# imagenet_labels = list(imagenet_labels)\n",
        "# len(imagenet_labels)"
      ],
      "metadata": {
        "id": "NIJ8zsk-UTuh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sub_imagenet_labels = imagenet_labels[:200]\n",
        "# print(len(sub_imagenet_labels))"
      ],
      "metadata": {
        "id": "2jjpQvehivmw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings to word proba"
      ],
      "metadata": {
        "id": "JNFmhVBmUTgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "solver = Solver(\"animal10-embeddeding.csv\")\n",
        "\n",
        "totest = solver.embeddings[\"cat\"]\n",
        "\n",
        "print(solver.get_nearest_embedding_of(totest))"
      ],
      "metadata": {
        "id": "tD8XokPDUhkp",
        "outputId": "777a2c6b-eb71-4f14-f029-1df2b2f9d8a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('cat', tensor(1.)), ('horse', tensor(0.9067)), ('squirrel', tensor(0.8875)), ('chicken', tensor(0.8678)), ('butterfly', tensor(0.8576)), ('cow', tensor(0.8493)), ('spider', tensor(0.8481)), ('dog', tensor(0.8410)), ('elephant', tensor(0.7702)), ('sheep', tensor(0.7604))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word to embeddings"
      ],
      "metadata": {
        "id": "LN273RlCUGza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_list = [\"dog\", \"cat\", \"horse\", \"spider\", \"butterfly\", \"chicken\", \"sheep\", \"cow\", \"squirrel\", \"elephant\"]\n",
        "\n",
        "model = BERTModel(class_list, big = False, window = 100)\n",
        "\n",
        "articlesRetriever = ArticleRetriever(\"10animal.art\", class_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvIn_JeFlYfK",
        "outputId": "fac1fe0a-dc28-4d02-f431-74b0c32fec35"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articlesRetriever.load_all_articles()\n",
        "articlesRetriever.save()\n",
        "\n",
        "model.reset_embeddings()\n",
        "model.convert(articlesRetriever)\n",
        "model.export(\"animal10-embeddeding.csv\")\n",
        "print(\"\\n\", len(model.get_class_list()))"
      ],
      "metadata": {
        "id": "WOOlgG8koVD6",
        "outputId": "dd5e54a5-062f-4101-cd90-82b7cdae713e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:11<00:00,  1.17s/it]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wikipedia debug"
      ],
      "metadata": {
        "id": "7jubA6JXUMk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Article to search for { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "totest = \"rock\" #@param {type:\"string\"}\n",
        "\n",
        "result = wikipedia.search(totest, suggestion = False)\n",
        "print(result)\n",
        "print(f\"first result is: {result[0]}\")\n",
        "\n",
        "try:\n",
        "    print(wikipedia.page(totest, auto_suggest=False, redirect=True))\n",
        "    print(wikipedia.page(result[0], auto_suggest=False, redirect=True))\n",
        "except Exception as e:\n",
        "    print(f\"best option envisaged: {e.options[0]}\")\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "PeM8XAu6h5gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test part"
      ],
      "metadata": {
        "id": "ax8Aw_qVRxer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = [\"King\", \"Queen\", \"men\", \"woman\"]\n",
        "\n",
        "test_model = BERTModel(vocab, big = True, window = 0)\n",
        "articlesRetriever = ArticleRetriever(\"king2queen\", vocab)\n"
      ],
      "metadata": {
        "id": "8uNXNj_wCWhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "\n",
        "articlesRetriever.load_all_articles(force_reload = True)\n",
        "articlesRetriever.save()\n",
        "test_model.convert(articlesRetriever)\n",
        "\n",
        "men = test_model.get_embedding_of(\"men\")[1]\n",
        "woman = test_model.get_embedding_of(\"woman\")[1]\n",
        "king = test_model.get_embedding_of(\"King\")[1]\n",
        "\n",
        "totest = king.sub(men).add(woman)\n",
        "print(test_model.get_nearest_embedding_of(totest, nb=4))"
      ],
      "metadata": {
        "id": "2j1Pw0um1ofC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordsim353 = nlp.data.WordSim353('all')"
      ],
      "metadata": {
        "id": "dnkI3j0V-CkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "\n",
        "vocab = []\n",
        "\n",
        "for w1, w2, i in wordsim353:\n",
        "    if w1 not in vocab:\n",
        "        vocab.append(w1)\n",
        "    if w2 not in vocab:\n",
        "        vocab.append(w2)\n",
        "\n",
        "print(len(vocab))\n",
        "wordsim353_model = BERTModel(vocab, big = False, window = 0)\n",
        "articlesRetriever = ArticleRetriever(\"wordsim353\", vocab)\n",
        "\n",
        "try:\n",
        "    articlesRetriever.load_all_articles(force_reload = True)\n",
        "    articlesRetriever.save()\n",
        "except Exception:\n",
        "    traceback.print_exc()\n",
        "\n",
        "wordsim353_model.convert(articlesRetriever)"
      ],
      "metadata": {
        "id": "YmFXzvYYBVzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "total_comparison = 0\n",
        "sim_list = []\n",
        "i_list = []\n",
        "\n",
        "for w1, w2, i in wordsim353:\n",
        "    try:\n",
        "        # print(f\"how much {w1} is similar to {w2}:\")\n",
        "        sim = wordsim353_model.sim_between(w1, w2)\n",
        "\n",
        "        sim_list.append(sim)\n",
        "        i_list.append(i\n",
        "                      )\n",
        "        # print(f\"\\t{sim} & {i}\")\n",
        "        total_comparison += 1\n",
        "    except Exception as e:\n",
        "        print(w1, w2)\n",
        "        continue\n",
        "\n",
        "print(spearmanr(sim_list, i_list))"
      ],
      "metadata": {
        "id": "Vwy1yo7iIgnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Different result for King - men + women equation with different context window size\n",
        "\n",
        "| window size | rank of Queen | distance with first |\n",
        "|-------------|---------------|-----------|\n",
        "| 0  | 2 |  .0693 |\n",
        "| 10 | 3 |  .1191 | \n",
        "| 50 | 2 |  .1672 |\n",
        "| 100 | 2 | .1275 |\n",
        "| 150 | 3 | .1134 |\n",
        "| 200 | 3 | .1923 |\n",
        "| 300 | 3 | .0939 |\n",
        "| 400 | 3 | .1455 |"
      ],
      "metadata": {
        "id": "kY8yWSmJw6Ss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pearson correlation rank with different context window size\n",
        " \n",
        "| model | corpus | test set |window size | pearson rank correlation |\n",
        "|-------|--------|----------|------------|--------------------------|\n",
        "| bert large | article from wordsim vocab | wordsim353 | 100 | 0.2158 (5.8353e-05)|\n",
        "| bert base  | article from wordsim vocab | wordsim353 | 100 | 0.2284 (6.8215e-05)|\n",
        "| bert-base  | article from wordsim vocav | wordsim353 | 300 | 0.1326 (1.27e-02) |\n",
        "| bert-large  | article from wordsim vocav | wordsim353 | 300 | 0.2117 (6.2153e-05) |\n",
        "| bert-large  | article from wordsim vocav | wordsim353 | 0 | 0.2638 (5.1232-07) |\n",
        "| bert-base  | article from wordsim vocav | wordsim353 | 0 | 0.3721 (5.2306-13) |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-VuUxJc6zumT"
      }
    }
  ]
}