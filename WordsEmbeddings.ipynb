{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordsEmbeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taedriel/ZSL-v2/blob/wordEmbedding/WordsEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation part"
      ],
      "metadata": {
        "id": "bYtbXyM-iE1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yes | pip install transformers wget wikipedia unzip mxnet gluonnlp \"scipy>=1.7\" scikit-bio wikipedia2vec --quiet --upgrade\n",
        "!mkdir -p temp article\n",
        "%xmode Verbose"
      ],
      "metadata": {
        "id": "HEq14T6Nl535",
        "outputId": "5119c62f-4bf5-4594-9ba1-1cc3fa33242b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception reporting mode: Verbose\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as  np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, RobertaModel, RobertaTokenizer\n",
        "import tensorflow as tf\n",
        "import gluonnlp as nlp\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from wikipedia2vec import Wikipedia2Vec\n",
        "import wikipedia\n",
        "wikipedia.set_rate_limiting(True)\n",
        "\n",
        "import gc\n",
        "import traceback\n",
        "import pickle\n",
        "import logging\n",
        "logging.basicConfig(level = logging.INFO, filename = \"BERT.log\" )\n",
        "# logging.basicConfig(level = logging.INFO)\n",
        "\n",
        "from tqdm import tqdm\n",
        "from typing import List, Tuple, Dict\n",
        "from os.path import exists, join, abspath\n",
        "from os import system\n",
        "from enum import Enum\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "from scipy.stats import SpearmanRConstantInputWarning\n",
        "from skbio import DistanceMatrix\n",
        "from skbio.tree import nj\n"
      ],
      "metadata": {
        "id": "spXWSv2iW9i5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils classes"
      ],
      "metadata": {
        "id": "kjt7fI8Olf5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dict2csv(filename, embeddings : Dict[str, List[float]]):\n",
        "    try:\n",
        "        f = open(filename, \"w\")\n",
        "    except OSError:\n",
        "        raise OSError(\"Could not open file\")\n",
        "\n",
        "    dimension_number = len(next(iter(embeddings.values())))\n",
        "    with f:\n",
        "        print(\"embeddings\", *[str(i) for i in range(dimension_number)], sep=\",\", file=f)\n",
        "        for tag, embedding in embeddings.items():\n",
        "            print(tag, *list(map(lambda x: str(float(x)), embedding)), sep=\",\", file=f)\n",
        "\n",
        "def sim2dist(sim_data, inverse_function = lambda x: 1 - x, hollow_matrix = True):\n",
        "\n",
        "    inv_data = [[0 for i in range(len(sim_data[0]))] for j in range(len(sim_data))]\n",
        "\n",
        "    for i, elem in enumerate(data):\n",
        "        for j, case in enumerate(elem):\n",
        "            if i == j and hollow_matrix: \n",
        "                inv_data[i][j] = 0\n",
        "            else:\n",
        "                inv_data[i][j] = inverse_function(case)\n",
        "    \n",
        "    return inv_data\n",
        "\n",
        "\n",
        "def print_mat(data, format_function=lambda x: x):\n",
        "\n",
        "    for line in data:\n",
        "        for case in line:\n",
        "            print(f\"{format_function(case):8}\", end=\"\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "COZfnM3QNwUc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Strategy"
      ],
      "metadata": {
        "id": "-Wd47Q50gPSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTMergeStrategy:\n",
        "\n",
        "    def merge(self, vector : List[List[float]]) -> List[float]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "class Sum4LastLayers(BERTMergeStrategy):\n",
        "\n",
        "    def merge(self, vector : List[List[float]]) -> List[float]:\n",
        "        return torch.sum(vector[-4:], dim = 0)\n",
        "\n",
        "class Similarity:\n",
        "\n",
        "    def sim(self, embed1 : List[float], embed2 : List[float]) -> float:\n",
        "        raise NotImplementedError\n",
        "\n",
        "class CosineSim(Similarity):\n",
        "\n",
        "    def sim(self, embed1 : List[float], embed2 : List[float]) -> float:\n",
        "        cos = torch.nn.CosineSimilarity(dim=0)\n",
        "        return cos(embed1, embed2)\n",
        "\n",
        "class EuclidianDistSim(Similarity):\n",
        "\n",
        "    def sim(self, embed1 : List[float], embed2 : List[float]) -> float:\n",
        "        return np.linalg.norm(embed1-embed2)\n",
        "\n",
        "class ManhattanDistSim(Similarity):\n",
        "\n",
        "    def sim(self, embed1 : List[float], embed2 : List[float]) -> float:\n",
        "        return cityblock(embed1, embed2)"
      ],
      "metadata": {
        "id": "LC8LdC2ngRtx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Article"
      ],
      "metadata": {
        "id": "7G26rtCEfQny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class customWikiArticle:\n",
        "\n",
        "    def __init__(self, index : int, title : str, realtitle : str, summary : str, ambiguous : bool):\n",
        "        self.index = index\n",
        "        self.title = title\n",
        "        self.realtitle = realtitle\n",
        "        self.summary = summary\n",
        "        self.ambiguous = ambiguous\n",
        "\n",
        "class ArticleRetriever:\n",
        "\n",
        "    article_dir = \"./article\"\n",
        "\n",
        "    def __init__(self, name : str, list_title : List[str]):\n",
        "\n",
        "        self.name = name\n",
        "        self.list_title = list_title\n",
        "        self.modified = False\n",
        "\n",
        "        if not exists(self.get_filename()):\n",
        "            self.articles_map = {}\n",
        "        else:\n",
        "            with open(self.get_filename(), \"rb\") as mapfile:\n",
        "                self.articles_map = pickle.load(mapfile)\n",
        "\n",
        "    def get_filename(self):\n",
        "        return join(ArticleRetriever.article_dir,self.name)\n",
        "\n",
        "    def load_article(self, title, force_reload : bool = False) -> customWikiArticle:\n",
        "        if title not in self.articles_map:\n",
        "            self.modified = True\n",
        "            realtitle, summary, ambiguous = self._retrieve_article(title)\n",
        "            self.articles_map[title] = customWikiArticle(len(self.articles_map), title, realtitle, summary, ambiguous)\n",
        "\n",
        "        if title in self.articles_map and self.articles_map[title].summary == None and force_reload:\n",
        "            self.modified = True\n",
        "            realtitle, summary, ambiguous = self._retrieve_article(title)\n",
        "            self.articles_map[title].summary = summary\n",
        "\n",
        "        return self.articles_map[title]\n",
        "\n",
        "    def _retrieve_article(self, title, closed_list : List = []) -> Tuple[str, str, bool]: \n",
        "        closed_list.append(title)\n",
        "        try:\n",
        "            article = wikipedia.page(title, auto_suggest=False, redirect=True)\n",
        "            return (article.title, article.summary, False)\n",
        "\n",
        "        except wikipedia.PageError as e:\n",
        "            search_result = wikipedia.search(title, suggestion = False)\n",
        "\n",
        "            logging.warning(f\"{title} misspelled or article missing. Best find is {search_result[0]}\")\n",
        "            if search_result[0] is not None and search_result[0] not in closed_list:            \n",
        "                return self._retrieve_article(search_result[0], closed_list)  \n",
        "            else: return (title, None, False)\n",
        "\n",
        "        except wikipedia.DisambiguationError as e:\n",
        "            logging.warning(f\"{title} is ambiguous, trying first {e.options[0]}\")\n",
        "            if e.options[0] is not None and e.options[0] not in closed_list:\n",
        "                res = self._retrieve_article(e.options[0], closed_list)\n",
        "                return (res[0], res[1], True)\n",
        "        \n",
        "            return (None, None, None)\n",
        "\n",
        "\n",
        "    def load_all_articles(self, force_reload : bool = False) -> None:\n",
        "        logging.info(f\"Starting loading articles...\")\n",
        "        nb_success = 0\n",
        "\n",
        "        nb_article = len(self.list_title)\n",
        "        for i, title in tqdm(enumerate(self.list_title), total=nb_article):\n",
        "            self.load_article(title, force_reload)\n",
        "\n",
        "            if self.articles_map[title].summary is not None: \n",
        "                nb_success += 1\n",
        "        logging.info(f\"Finished loading {nb_success} article(s) !\")\n",
        "        return self.modified\n",
        "\n",
        "    def __call__(self, force_reload : bool = True) -> None:\n",
        "        return self.load_all_articles(force_reload)\n",
        "\n",
        "    def get_article(self, title):\n",
        "        if title not in self.articles_map:\n",
        "            self.load_article(title)\n",
        "\n",
        "        return self.articles_map[title]\n",
        "        \n",
        "    def save(self):\n",
        "        with open(self.get_filename(), \"wb\") as mapfile:\n",
        "            pickle.dump(self.articles_map, mapfile)\n",
        "\n",
        "class ArticleViewer():\n",
        "\n",
        "    def __init__(self, filename):\n",
        "        self.name = filename\n",
        "\n",
        "        if not exists(self.name):\n",
        "            raise FileNotFoundError()\n",
        "        else:\n",
        "            with open(self.name, \"rb\") as mapfile:\n",
        "                self.articles_map = pickle.load(mapfile)\n",
        "\n",
        "    def get(self, title):\n",
        "        return self.articles_map[title]\n",
        "\n",
        "    def get_all_articles(self):\n",
        "        return self.articles_map.keys()\n"
      ],
      "metadata": {
        "id": "QZiZ2Pr-lUiS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings operations "
      ],
      "metadata": {
        "id": "bw3z2xRJfeYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WordToVector:\n",
        "\n",
        "    def __init__(self, list_tags : List[str] = []):\n",
        "        self.list_tags = list_tags\n",
        "        self.embeddings = {}\n",
        "\n",
        "    def convert(self):\n",
        "        return NotImplementedError\n",
        "\n",
        "    def reset_embeddings(self):\n",
        "        self.embeddings.clear()\n",
        "\n",
        "    def get_embedding_of(self, token):\n",
        "        if token not in self.embeddings:\n",
        "            raise Exception(f\"no such token {token}\")\n",
        "        \n",
        "        return self.embeddings[token]\n",
        "\n",
        "    def get_class_list(self):\n",
        "        return self.embeddings.keys()\n",
        "\n",
        "    def export(self, filename):\n",
        "        \"\"\"export all the embeddings in filename under a .csv format.\n",
        "           Raise exception if embeddings hasn't been calculed yet.\"\"\"\n",
        "\n",
        "        if len(self.embeddings) == 0:\n",
        "            raise Exception(\"Tags not converted yet !\")\n",
        "        \n",
        "        dict2csv(filename, self.embeddings)\n",
        "\n",
        "class EmbeddingsLoader:\n",
        "\n",
        "    def __init__(self, filename : str):\n",
        "\n",
        "        self.file = filename\n",
        "        self.embeddings = {}\n",
        "\n",
        "        self._load_file()\n",
        "\n",
        "    def _load_file(self):\n",
        "        try:\n",
        "            with open(self.file, \"r\") as f:\n",
        "                lines = f.readlines()\n",
        "                \n",
        "            for line in lines[1:]:\n",
        "                data = line.split(\",\")\n",
        "                self.embeddings[data[0]] = torch.FloatTensor(list(map(float, data[1:])))\n",
        "\n",
        "        except IOError as e:\n",
        "            raise IOError(f\"No file {self.file}\")\n",
        "\n",
        "class SimilarityMatrix(EmbeddingsLoader):\n",
        "\n",
        "    def __init__(self, embeddings : Dict[str, List[float]], strategy : Similarity):\n",
        "        super(SimilarityMatrix, self).__init__(embeddings)\n",
        "        self.strategy = strategy\n",
        "        self._create_matrix()\n",
        "        self.computed : bool = False\n",
        "\n",
        "    def _create_matrix(self) -> None:\n",
        "        n_tokens = len(self.embeddings)\n",
        "        self.cosine_sim_matrix : Dict[Dict[float]] = {}\n",
        "        for tag in self.embeddings.keys():\n",
        "            self.cosine_sim_matrix[tag] = {}\n",
        "\n",
        "    def compute_sim(self) -> None:\n",
        "        \"\"\" compute cosine similarity between all vectors \"\"\"\n",
        "\n",
        "        logging.info(\"Computing cosine similarity, this could take some time...\")\n",
        "        for tag, vector in tqdm(self.embeddings.items(), total = len(self.embeddings)):\n",
        "\n",
        "            for otag, other_vector in self.embeddings.items():\n",
        "\n",
        "                if otag == tag:\n",
        "                    continue\n",
        "\n",
        "                similarity = self.strategy.sim(vector, other_vector)\n",
        "\n",
        "                self.cosine_sim_matrix[otag][tag] = similarity\n",
        "                self.cosine_sim_matrix[tag][otag] = similarity\n",
        "\n",
        "        self.computed = True\n",
        "\n",
        "    def export_sim_matrix(self, filename):\n",
        "        if not self.computed:\n",
        "            self.compute_sim()\n",
        "        \n",
        "        try:\n",
        "            f = open(filename, \"w\")\n",
        "        except OSError:\n",
        "            raise OSError(\"Could not open file\")\n",
        "\n",
        "        with f:\n",
        "            print(\"/\", *[tag for tag in self.embeddings.keys()], sep = \",\", file = f)\n",
        "\n",
        "            for tag in self.embeddings.keys():\n",
        "                print(tag, *[str(round(float(self.cosine_sim_matrix[tag][otag]), 3)) for otag in self.embeddings.keys()], sep = \",\", file = f)\n",
        "\n",
        "    def get_sim_matrix(self) -> Tuple[List[str], List[List[float]]]:\n",
        "        \"\"\"return the similarity matrix of the embeddings\n",
        "        \"\"\"\n",
        "        if not self.computed:\n",
        "            self.compute_sim()\n",
        "\n",
        "        X = len(self.embeddings)\n",
        "        matrix = [[0 for j in range(X)] for i in range(X)]\n",
        "        ids = []\n",
        "        \n",
        "        for i, tag in enumerate(self.embeddings.keys()):\n",
        "            ids.append(tag)\n",
        "            for j, otag in enumerate(self.embeddings.keys()):\n",
        "                if i == j:\n",
        "                    continue\n",
        "\n",
        "                matrix[i][j] = self.cosine_sim_matrix[tag][otag]\n",
        "                matrix[j][i] = self.cosine_sim_matrix[tag][otag]\n",
        "\n",
        "        return ids, matrix\n",
        "\n",
        "    def sim_between(self, token1 : str, token2 : str) -> float:\n",
        "        v1 = self.embeddings[token1]\n",
        "        v2 = self.embeddings[token2]\n",
        "\n",
        "        if token2 not in self.cosine_sim_matrix[token1] or token1 not in self.cosine_sim_matrix[token2]:\n",
        "            similarity = self.strategy.sim(v1, v2)\n",
        "            self.computed = True\n",
        "\n",
        "            self.cosine_sim_matrix[token1][token2] = similarity\n",
        "            self.cosine_sim_matrix[token1][token2] = similarity\n",
        "\n",
        "        return self.cosine_sim_matrix[token1][token2]\n",
        "\n",
        "class SimilarityMatrixFromDict(SimilarityMatrix):\n",
        "\n",
        "    def __init__(self, embeddings : Dict[str, List[float]], strategy : Similarity):\n",
        "        self.embeddings = embeddings\n",
        "        self._create_matrix()\n",
        "        self.strategy = strategy\n",
        "        self.computed : bool = False"
      ],
      "metadata": {
        "id": "5AxDIhr1rkca"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solver"
      ],
      "metadata": {
        "id": "0bRoDbKafoVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Solver(EmbeddingsLoader):\n",
        "\n",
        "    DEFAULT_MIN_LIST_RESULT = 10\n",
        "\n",
        "    def __init__(self, embeddings):\n",
        "        super(Solver, self).__init__(embeddings)\n",
        "\n",
        "    def get_nearest_embedding_of(self, embedding, nb = 10):\n",
        "\n",
        "        if nb > len(self.embeddings):\n",
        "            raise Exception(\"nb too high, not enough token\")\n",
        "\n",
        "        nearest = []\n",
        "        for tag, e in self.embeddings.items():\n",
        "\n",
        "            cos = torch.nn.CosineSimilarity(dim=0)\n",
        "            similarity = cos(embedding, e)\n",
        "\n",
        "            nearest.append((tag, similarity))\n",
        "        \n",
        "        nearest.sort(key = lambda tup : tup[1])\n",
        "        return nearest[-1:-nb-1:-1]\n",
        "\n",
        "    def __call__(self, embeddeding, tag=None):\n",
        "        result = self.get_nearest_embedding_of(embeddeding, min(Solver.DEFAULT_MIN_LIST_RESULT, len(self.embeddings)))\n",
        "        if tag is not None:\n",
        "            print(f\"Nearest Word for {tag}:\")\n",
        "        for i in result:\n",
        "            print(f\"\\t{i[0]:12}: {round(float(i[1]) * 100, 3)}%\")\n",
        "    \n",
        "    def score(self, embedding, target):\n",
        "        target_embeddings = self.embeddings[target]\n",
        "\n",
        "        cos = torch.nn.CosineSimilarity(dim=0)\n",
        "        return float(cos(embedding, target_embeddings))\n",
        "\n",
        "    def least_squared_score(self, embedding, target):\n",
        "        target_embeddings = self.embeddings[target]\n",
        "        return float(np.linalg.norm(target_embeddings - embedding))\n",
        "\n",
        "    def mean_squared_score(self, embedding, target):\n",
        "        target_embeddings = self.embeddings[target]\n",
        "        return float(np.square(np.subtract(embedding, target_embeddings)).mean())\n",
        "        "
      ],
      "metadata": {
        "id": "KpVPJ9IxfnC5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "nYaqmgXe1-EX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT model"
      ],
      "metadata": {
        "id": "-0g4CJtelYfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTModel(WordToVector):\n",
        "\n",
        "    temp_dir = \"./temp\"\n",
        "\n",
        "    def __init__(self, list_tag : List[str], big: bool = False, window : int = 100):\n",
        "        super(BERTModel, self).__init__(list_tag)\n",
        "        self.list_articles = {}\n",
        "        self.window_size = window\n",
        "\n",
        "        self.model_size = \"bert-large-uncased\" if big else \"bert-base-uncased\"\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.model_size, padding=True, truncation=True,)\n",
        "        self.model = BertModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "    def convert(self, article_ret : ArticleRetriever):\n",
        "        \"\"\" convert all word in their embeddings\"\"\"\n",
        "\n",
        "        if len(self.list_tags) == 0:\n",
        "            raise Exception(\"not tags yet !\")\n",
        "\n",
        "        logging.info(\"Starting converting tokens...\")\n",
        "        nb_token = len(self.list_tags)\n",
        "        for i, tag in tqdm(enumerate(self.list_tags), total = nb_token):\n",
        "            \n",
        "            if tag in self.embeddings: continue\n",
        "\n",
        "            article = article_ret.get_article(tag)\n",
        "            tag_plus_context = tag\n",
        "            if article.summary is not None:\n",
        "                max_size_article = min(len(article.summary), self.window_size)\n",
        "                tag_plus_context = tag + \". \" + article.summary[:max_size_article]\n",
        "\n",
        "            inputs = self.tokenizer(tag_plus_context, return_tensors = \"pt\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            hidden_states = outputs[2]\n",
        "\n",
        "            # [# layers, # batches, # tokens, # features] ==> [# tokens, # layers, # features]\n",
        "            token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "            token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "            token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "            self.embeddings[tag] = self.merging_strategy.merge(token_embeddings[0])"
      ],
      "metadata": {
        "id": "UVJUSGWQorYj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RoBERTa model"
      ],
      "metadata": {
        "id": "eGVvWFvtlqLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ROBERTAModel(BERTModel):\n",
        "\n",
        "    def __init__(self, list_tag : List[str], big: bool = False, window : int = 100):\n",
        "        WordToVector.__init__(self, list_tag)\n",
        "        self.window_size = window\n",
        "\n",
        "        self.model_size = \"roberta-large\" if big else \"roberta-base\"\n",
        "\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_size, padding=True, truncation=True,)\n",
        "        self.model = RobertaModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()"
      ],
      "metadata": {
        "id": "7oWZOwG-mCYI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wikipedia2Vec"
      ],
      "metadata": {
        "id": "mFf6OoBG1xVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Wiki2VecModel(WordToVector):\n",
        "\n",
        "    file = []\n",
        "\n",
        "    def __init__(self, list_tag : List[str], size : int = 300):\n",
        "        WordToVector.__init__(self, list_tag)\n",
        "        self.window_size = size\n",
        "\n",
        "        self.model_size = \"wikipedia2vec\"\n",
        "        self.zip_filename = f\"enwiki_20180420_{self.window_size}d.pkl.bz2\"\n",
        "        self.unzip_filename = self.zip_filename[:-4]\n",
        "        \n",
        "        try:\n",
        "            self._load()\n",
        "        except:\n",
        "            return\n",
        "        \n",
        "        self.model = Wikipedia2Vec.load(self.path)\n",
        "\n",
        "    def _load(self):\n",
        "\n",
        "        self.path = abspath(self.unzip_filename)\n",
        "\n",
        "        if exists(self.path):\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            system(f\"wget http://wikipedia2vec.s3.amazonaws.com/models/en/2018-04-20/{self.zip_filename}\")\n",
        "        except:\n",
        "            raise SystemError(f\"can't retrieve the file {self.zip_filename}\")\n",
        "\n",
        "        try:\n",
        "            system(f\"bunzip2 ./{self.zip_filename}\")\n",
        "        except:\n",
        "            raise SystemError(f\"can't unzip the file {self.zip_filename}\")\n",
        "\n",
        "    def token_exist(self, word):\n",
        "        try:\n",
        "            embed = self.model.get_word_vector(word)\n",
        "            return True\n",
        "        except KeyError:\n",
        "            return False\n",
        "\n",
        "    def ask_alt(self, word):\n",
        "        print(f\"Unable to find {word} in the dict, choose a replacement : \")\n",
        "        alt = str(input())\n",
        "        return alt\n",
        "    \n",
        "    def convert(self, ArticleRetriever):\n",
        "        unk = []\n",
        "\n",
        "        for word in vocab:\n",
        "            w = word.replace(\"_\", \" \")\n",
        "            try:\n",
        "                print(\"trying to retrieve\", word)\n",
        "                embed = self.model.get_word_vector(w)\n",
        "                self.embeddings[word] = torch.from_numpy(embed)\n",
        "            except:\n",
        "                logging.warning(f\"{w} cannot be retrieved.\")\n",
        "                unk.append(word)\n",
        "        \n",
        "        return unk"
      ],
      "metadata": {
        "id": "w0zukX6Zk3W5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical part"
      ],
      "metadata": {
        "id": "ue8SGNE5iKaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings to word proba"
      ],
      "metadata": {
        "id": "JNFmhVBmUTgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "solver = Solver(\"/content/animal10-roberta-base-0.csv\")\n",
        "\n",
        "totest = solver.embeddings[\"cat\"]\n",
        "\n",
        "solver(totest, \"cat\")\n",
        "print(solver.score(totest, \"dog\"))\n",
        "print(solver.mean_squared_score(totest, \"dog\"))\n",
        "print(solver.least_squared_score(totest, \"dog\"))"
      ],
      "metadata": {
        "id": "tD8XokPDUhkp",
        "outputId": "d0b06fcc-0172-4f66-eba0-b9928eb0db51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest Word for cat:\n",
            "\tcat         : 100.0%\n",
            "\tdog         : 99.998%\n",
            "\thorse       : 99.998%\n",
            "\tcow         : 99.998%\n",
            "\tsquirrel    : 99.996%\n",
            "\tspider      : 99.996%\n",
            "\tsheep       : 99.996%\n",
            "\tchicken     : 99.996%\n",
            "\tcomputer    : 99.995%\n",
            "\tbutterfly   : 99.994%\n",
            "0.9999799728393555\n",
            "0.0004379133752081543\n",
            "0.5799288749694824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word to embeddings"
      ],
      "metadata": {
        "id": "LN273RlCUGza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
        "imagenet = list(np.array(open(labels_path).read().splitlines()))\n",
        "subimagenet = imagenet[:200]"
      ],
      "metadata": {
        "id": "NIJ8zsk-UTuh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "animal10 = [\"dog\", \"cat\", \"horse\", \"spider\", \"butterfly\", \"chicken\", \"sheep\", \"cow\", \"squirrel\", \"elephant\"]\n",
        "cifar10  = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "cifar100 = [\"apple\", \"aquarium_fish\", \"baby\", \"bear\", \"beaver\", \"bed\", \"bee\", \"beetle\", \"bicycle\", \"bottle\", \"bowl\", \"boy\", \"bridge\", \"bus\", \"butterfly\", \"camel\", \"can\", \\\n",
        "            \"castle\", \"caterpillar\", \"cattle\", \"chair\", \"chimpanzee\", \"clock\", \"cloud\", \"cockroach\", \"computer_keyboard\", \"couch\", \"crab\", \"crocodile\", \"cup\", \\\n",
        "            \"dinosaur\", 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'lamp', 'lawn_mower', 'leopard', 'lion', \\\n",
        "            'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', \\\n",
        "            'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark',\\\n",
        "            'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', \\\n",
        "            'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n",
        "\n",
        "king = [\"king\", \"woman\", \"man\", \"queen\", \"boy\", \"girl\", \"male\", \"female\"]\n",
        "\n",
        "\n",
        "#@title Choose a dataset\n",
        "save_name = \"cifar100\" #@param [\"animal10\", \"cifar10\", \"cifar100\", \"king\", \"imagenet\", \"subimagenet\"]\n",
        "mapping_save_list = {\n",
        "    \"animal10\": animal10,\n",
        "    \"cifar10\" : cifar10,\n",
        "    \"cifar100\" : cifar100,\n",
        "    \"king\" : king,\n",
        "    \"imagenet\" : imagenet,\n",
        "    \"subimagenet\": subimagenet\n",
        "}\n",
        "\n",
        "vocab = mapping_save_list[save_name]"
      ],
      "metadata": {
        "id": "EYnbJc84xuh5",
        "cellView": "form"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_size  = 300 #@param [\"0\", \"100\", \"200\", \"300\", \"400\", \"500\"] {type:\"raw\"}\n",
        "is_big       = False #@param {type:\"boolean\"}\n",
        "model_choice = \"Wikipedia2Vec\" #@param [\"ROBERTA\", \"BERT\", \"Wikipedia2Vec\"]\n",
        "\n",
        "if model_choice == \"ROBERT\":\n",
        "    model = ROBERTAModel(vocab, big = is_big, window = window_size)\n",
        "elif model_choice == \"BERT\":\n",
        "    model = BERTModel(vocab, big = is_big, window = window_size)\n",
        "elif model_choice == \"Wikipedia2Vec\":\n",
        "    model = Wiki2VecModel(vocab, size = window_size)\n",
        "\n",
        "articlesRetriever = ArticleRetriever(save_name + \".art\", vocab)"
      ],
      "metadata": {
        "id": "NvIn_JeFlYfK",
        "cellView": "form"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if articlesRetriever():\n",
        "    articlesRetriever.save()\n",
        "\n",
        "model.reset_embeddings()\n",
        "\n",
        "model.convert(articlesRetriever)\n",
        "\n",
        "csv_file = f\"{save_name}-{model.model_size}-{model.window_size}.csv\"\n",
        "\n",
        "model.export(csv_file)\n",
        "\n",
        "print(\"\\n\", len(model.get_class_list()))"
      ],
      "metadata": {
        "id": "WOOlgG8koVD6",
        "outputId": "9dabb075-83bd-4e13-80e2-9e62525457c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 16/100 [00:13<00:42,  1.98it/s]/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n",
            "100%|██████████| 100/100 [01:03<00:00,  1.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trying to retrieve apple\n",
            "trying to retrieve aquarium_fish\n",
            "trying to retrieve baby\n",
            "trying to retrieve bear\n",
            "trying to retrieve beaver\n",
            "trying to retrieve bed\n",
            "trying to retrieve bee\n",
            "trying to retrieve beetle\n",
            "trying to retrieve bicycle\n",
            "trying to retrieve bottle\n",
            "trying to retrieve bowl\n",
            "trying to retrieve boy\n",
            "trying to retrieve bridge\n",
            "trying to retrieve bus\n",
            "trying to retrieve butterfly\n",
            "trying to retrieve camel\n",
            "trying to retrieve can\n",
            "trying to retrieve castle\n",
            "trying to retrieve caterpillar\n",
            "trying to retrieve cattle\n",
            "trying to retrieve chair\n",
            "trying to retrieve chimpanzee\n",
            "trying to retrieve clock\n",
            "trying to retrieve cloud\n",
            "trying to retrieve cockroach\n",
            "trying to retrieve computer_keyboard\n",
            "trying to retrieve couch\n",
            "trying to retrieve crab\n",
            "trying to retrieve crocodile\n",
            "trying to retrieve cup\n",
            "trying to retrieve dinosaur\n",
            "trying to retrieve dolphin\n",
            "trying to retrieve elephant\n",
            "trying to retrieve flatfish\n",
            "trying to retrieve forest\n",
            "trying to retrieve fox\n",
            "trying to retrieve girl\n",
            "trying to retrieve hamster\n",
            "trying to retrieve house\n",
            "trying to retrieve kangaroo\n",
            "trying to retrieve lamp\n",
            "trying to retrieve lawn_mower\n",
            "trying to retrieve leopard\n",
            "trying to retrieve lion\n",
            "trying to retrieve lizard\n",
            "trying to retrieve lobster\n",
            "trying to retrieve man\n",
            "trying to retrieve maple_tree\n",
            "trying to retrieve motorcycle\n",
            "trying to retrieve mountain\n",
            "trying to retrieve mouse\n",
            "trying to retrieve mushroom\n",
            "trying to retrieve oak_tree\n",
            "trying to retrieve orange\n",
            "trying to retrieve orchid\n",
            "trying to retrieve otter\n",
            "trying to retrieve palm_tree\n",
            "trying to retrieve pear\n",
            "trying to retrieve pickup_truck\n",
            "trying to retrieve pine_tree\n",
            "trying to retrieve plain\n",
            "trying to retrieve plate\n",
            "trying to retrieve poppy\n",
            "trying to retrieve porcupine\n",
            "trying to retrieve possum\n",
            "trying to retrieve rabbit\n",
            "trying to retrieve raccoon\n",
            "trying to retrieve ray\n",
            "trying to retrieve road\n",
            "trying to retrieve rocket\n",
            "trying to retrieve rose\n",
            "trying to retrieve sea\n",
            "trying to retrieve seal\n",
            "trying to retrieve shark\n",
            "trying to retrieve shrew\n",
            "trying to retrieve skunk\n",
            "trying to retrieve skyscraper\n",
            "trying to retrieve snail\n",
            "trying to retrieve snake\n",
            "trying to retrieve spider\n",
            "trying to retrieve squirrel\n",
            "trying to retrieve streetcar\n",
            "trying to retrieve sunflower\n",
            "trying to retrieve sweet_pepper\n",
            "trying to retrieve table\n",
            "trying to retrieve tank\n",
            "trying to retrieve telephone\n",
            "trying to retrieve television\n",
            "trying to retrieve tiger\n",
            "trying to retrieve tractor\n",
            "trying to retrieve train\n",
            "trying to retrieve trout\n",
            "trying to retrieve tulip\n",
            "trying to retrieve turtle\n",
            "trying to retrieve wardrobe\n",
            "trying to retrieve whale\n",
            "trying to retrieve willow_tree\n",
            "trying to retrieve wolf\n",
            "trying to retrieve woman\n",
            "trying to retrieve worm\n",
            "\n",
            " 90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neighboor joining Tree"
      ],
      "metadata": {
        "id": "0WZFTDaGbr1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neighboor_sim = SimilarityMatrix(csv_file, CosineSim())\n",
        "\n",
        "ids, data = neighboor_sim.get_sim_matrix()\n",
        "\n",
        "ids = [tids.replace(\" \", \"_\") for tids in ids]\n",
        "inv_data  = sim2dist(data, lambda x: 1 / x) \n",
        "\n",
        "# print()\n",
        "# print_mat(data_inv, format_function = lambda x: round(float(x), 2))\n",
        "\n",
        "dm = DistanceMatrix(inv_data, ids)\n",
        "tree = nj(dm)\n",
        "print(\"\\n\\n\", tree.ascii_art())"
      ],
      "metadata": {
        "id": "crcMR8dDuH7s",
        "outputId": "3e1ba935-6b21-46b2-d994-74828d0d4374",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [00:00<00:00, 227.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "                                                   /-ray\n",
            "                                        /--------|\n",
            "                                       |         |          /-television\n",
            "                                       |          \\--------|\n",
            "                              /--------|                    \\-fox\n",
            "                             |         |\n",
            "                             |         |          /-baby\n",
            "                             |          \\--------|\n",
            "                             |                   |          /-boy\n",
            "                             |                    \\--------|\n",
            "                    /--------|                             |          /-man\n",
            "                   |         |                              \\--------|\n",
            "                   |         |                                       |          /-woman\n",
            "                   |         |                                        \\--------|\n",
            "                   |         |                                                  \\-girl\n",
            "                   |         |\n",
            "                   |         |          /-shark\n",
            "                   |          \\--------|\n",
            "          /--------|                   |          /-whale\n",
            "         |         |                    \\--------|\n",
            "         |         |                              \\-dolphin\n",
            "         |         |\n",
            "         |         |                    /-elephant\n",
            "         |         |          /--------|\n",
            "         |         |         |         |          /-lion\n",
            "         |         |         |          \\--------|\n",
            "         |         |         |                   |          /-tiger\n",
            "         |          \\--------|                    \\--------|\n",
            "         |                   |                              \\-leopard\n",
            "         |                   |\n",
            "         |                   |          /-crocodile\n",
            "         |                    \\--------|\n",
            "         |                             |          /-dinosaur\n",
            "         |                              \\--------|\n",
            "         |                                        \\-chimpanzee\n",
            "         |\n",
            "         |                                        /-skunk\n",
            "         |                              /--------|\n",
            "         |                             |          \\-raccoon\n",
            "         |                    /--------|\n",
            "         |                   |         |          /-porcupine\n",
            "         |                   |         |         |\n",
            "         |                   |          \\--------|                    /-otter\n",
            "         |                   |                   |          /--------|\n",
            "         |                   |                   |         |          \\-beaver\n",
            "         |                   |                    \\--------|\n",
            "         |                   |                             |          /-wolf\n",
            "         |          /--------|                              \\--------|\n",
            "         |         |         |                                        \\-bear\n",
            "         |         |         |\n",
            "         |         |         |                    /-squirrel\n",
            "         |         |         |          /--------|\n",
            "         |         |         |         |         |          /-hamster\n",
            "         |         |         |         |          \\--------|\n",
            "         |         |         |         |                   |          /-shrew\n",
            "         |         |          \\--------|                    \\--------|\n",
            "         |         |                   |                              \\-mouse\n",
            "         |         |                   |\n",
            "         |         |                   |          /-rabbit\n",
            "         |         |                    \\--------|\n",
            "         |         |                             |          /-possum\n",
            "         |         |                              \\--------|\n",
            "         |         |                                        \\-kangaroo\n",
            "         |         |\n",
            "         |         |                    /-snake\n",
            "         |         |                   |\n",
            "         |---------|          /--------|          /-lizard\n",
            "         |         |         |         |         |\n",
            "         |         |         |          \\--------|          /-spider\n",
            "         |         |         |                   |         |\n",
            "         |         |         |                    \\--------|          /-cockroach\n",
            "         |         |         |                             |         |\n",
            "         |         |         |                              \\--------|          /-bee\n",
            "         |         |         |                                       |         |\n",
            "         |         |         |                                        \\--------|                    /-orchid\n",
            "         |         |         |                                                 |          /--------|\n",
            "         |         |         |                                                 |         |          \\-butterfly\n",
            "         |         |         |                                                  \\--------|\n",
            "         |         |         |                                                           |          /-snail\n",
            "         |         |         |                                                            \\--------|\n",
            "         |         |         |                                                                      \\-beetle\n",
            "         |         |         |\n",
            "         |         |         |                              /-mushroom\n",
            "         |         |         |                             |\n",
            "         |         |         |                             |                    /-pear\n",
            "         |         |         |                             |          /--------|\n",
            "         |          \\--------|                    /--------|         |         |          /-apple\n",
            "         |                   |                   |         |         |          \\--------|\n",
            "         |                   |                   |         |         |                   |          /-orange\n",
            "         |                   |                   |         |         |                    \\--------|\n",
            "         |                   |                   |          \\--------|                             |          /-tulip\n",
            "         |                   |                   |                   |                              \\--------|\n",
            "         |                   |          /--------|                   |                                        \\-rose\n",
            "         |                   |         |         |                   |\n",
            "         |                   |         |         |                   |          /-sunflower\n",
            "         |                   |         |         |                    \\--------|\n",
            "         |                   |         |         |                              \\-poppy\n",
            "         |                   |         |         |\n",
            "---------|                   |         |         |          /-crab\n",
            "         |                   |         |          \\--------|\n",
            "         |                   |         |                   |          /-lobster\n",
            "         |                   |         |                    \\--------|\n",
            "         |                   |         |                             |          /-trout\n",
            "         |                   |         |                              \\--------|\n",
            "         |                   |         |                                        \\-flatfish\n",
            "         |                   |         |\n",
            "         |                   |         |                    /-worm\n",
            "         |                    \\--------|                   |\n",
            "         |                             |          /--------|          /-caterpillar\n",
            "         |                             |         |         |         |\n",
            "         |                             |         |         |         |                    /-rocket\n",
            "         |                             |         |          \\--------|          /--------|\n",
            "         |                             |         |                   |         |         |          /-tractor\n",
            "         |                             |         |                   |         |          \\--------|\n",
            "         |                             |         |                   |         |                    \\-tank\n",
            "         |                             |         |                    \\--------|\n",
            "         |                             |         |                             |                    /-motorcycle\n",
            "         |                             |         |                             |          /--------|\n",
            "         |                             |         |                             |         |          \\-bicycle\n",
            "         |                             |         |                              \\--------|\n",
            "         |                             |         |                                       |          /-train\n",
            "         |                             |         |                                        \\--------|\n",
            "         |                             |         |                                                 |          /-telephone\n",
            "         |                              \\--------|                                                  \\--------|\n",
            "         |                                       |                                                           |          /-bus\n",
            "         |                                       |                                                            \\--------|\n",
            "         |                                       |                                                                     |          /-road\n",
            "         |                                       |                                                                      \\--------|\n",
            "         |                                       |                                                                               |          /-skyscraper\n",
            "         |                                       |                                                                                \\--------|\n",
            "         |                                       |                                                                                          \\-bridge\n",
            "         |                                       |\n",
            "         |                                       |                    /-cattle\n",
            "         |                                       |          /--------|\n",
            "         |                                       |         |          \\-camel\n",
            "         |                                       |         |\n",
            "         |                                       |         |                    /-sea\n",
            "         |                                       |         |          /--------|\n",
            "         |                                        \\--------|         |         |          /-mountain\n",
            "         |                                                 |         |          \\--------|\n",
            "         |                                                 |         |                   |          /-forest\n",
            "         |                                                 |         |                    \\--------|\n",
            "         |                                                 |         |                              \\-cloud\n",
            "         |                                                 |         |\n",
            "         |                                                  \\--------|                    /-can\n",
            "         |                                                           |          /--------|\n",
            "         |                                                           |         |         |          /-plain\n",
            "         |                                                           |         |          \\--------|\n",
            "         |                                                           |         |                   |          /-table\n",
            "         |                                                           |         |                    \\--------|\n",
            "         |                                                           |         |                             |          /-plate\n",
            "         |                                                           |         |                              \\--------|\n",
            "         |                                                            \\--------|                                       |          /-cup\n",
            "         |                                                                     |                                        \\--------|\n",
            "         |                                                                     |                                                  \\-bowl\n",
            "         |                                                                     |\n",
            "         |                                                                     |                    /-couch\n",
            "         |                                                                     |          /--------|\n",
            "         |                                                                     |         |          \\-bed\n",
            "         |                                                                     |         |\n",
            "         |                                                                      \\--------|          /-bottle\n",
            "         |                                                                               |         |\n",
            "         |                                                                               |         |                    /-wardrobe\n",
            "         |                                                                               |         |          /--------|\n",
            "         |                                                                                \\--------|         |         |          /-castle\n",
            "         |                                                                                         |         |          \\--------|\n",
            "         |                                                                                         |         |                   |          /-house\n",
            "         |                                                                                         |         |                    \\--------|\n",
            "         |                                                                                          \\--------|                              \\-chair\n",
            "         |                                                                                                   |\n",
            "         |                                                                                                   |                    /-streetcar\n",
            "         |                                                                                                   |          /--------|\n",
            "         |                                                                                                   |         |          \\-seal\n",
            "         |                                                                                                    \\--------|\n",
            "         |                                                                                                             |          /-lamp\n",
            "         |                                                                                                              \\--------|\n",
            "         |                                                                                                                        \\-clock\n",
            "         |\n",
            "          \\-turtle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## similarity test"
      ],
      "metadata": {
        "id": "Eu9U-_PfeXSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"/content/cifar100-roberta-base-0.csv\"\n",
        "\n",
        "\n",
        "cosine_sim = SimilarityMatrix(filename, CosineSim())\n",
        "euclidian_dist_sim = SimilarityMatrix(filename, EuclidianDistSim())\n",
        "manhattan_dist_sim = SimilarityMatrix(filename, ManhattanDistSim())\n",
        "\n",
        "ids, cosine_mat = cosine_sim.get_sim_matrix()\n",
        "_  , euclid_mat = euclidian_dist_sim.get_sim_matrix()\n",
        "_  , manhat_mat = manhattan_dist_sim.get_sim_matrix()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JUE701weWzy",
        "outputId": "7924611d-d2bc-49d2-a8e4-8432919594e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 159.52it/s]\n",
            "100%|██████████| 100/100 [00:00<00:00, 540.59it/s]\n",
            "100%|██████████| 100/100 [00:00<00:00, 694.61it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(max(list(map(max, euclid_mat))))\n",
        "print(min(list(map(min, euclid_mat))))\n",
        "\n",
        "print(max(list(map(max, manhat_mat))))\n",
        "print(min(list(map(min, manhat_mat))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oMgdFgEC_hj",
        "outputId": "00129af5-49fb-40dc-fed5-ae05f80b2d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.722381\n",
            "0\n",
            "122.34671\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim_list = []\n",
        "euclid_sim_list = []\n",
        "manhat_sim_list = []\n",
        "\n",
        "closed_list = []\n",
        "\n",
        "# cos_thresold = 0.5\n",
        "# euc_thresold = 5\n",
        "# man_thresold = 60\n",
        "\n",
        "for i, idsa in tqdm(enumerate(ids)):\n",
        "    for j, idsb in enumerate(ids):\n",
        "\n",
        "        if i == j: continue \n",
        "        if (i, j) in closed_list or (j, i) in closed_list: continue\n",
        "\n",
        "        cos_val = cosine_mat[i][j]\n",
        "        euc_val = euclid_mat[i][j]\n",
        "        man_val = manhat_mat[i][j]\n",
        "\n",
        "        # if cos_val >= cos_thresold:\n",
        "        cosine_sim_list.append((idsa, idsb, cos_val))\n",
        "        \n",
        "        # if euc_val <= euc_thresold:\n",
        "        euclid_sim_list.append((idsa, idsb, euc_val))\n",
        "\n",
        "        # if man_val <= man_thresold:\n",
        "        manhat_sim_list.append((idsa, idsb, man_val))\n",
        "\n",
        "        closed_list.append((i, j))\n",
        "        closed_list.append((j, i))\n",
        "\n",
        "cosine_sim_list.sort(key=lambda x: x[2], reverse = True)\n",
        "euclid_sim_list.sort(key=lambda x: x[2], reverse = False)\n",
        "manhat_sim_list.sort(key=lambda x: x[2], reverse = False)\n",
        "\n",
        "def print_list(listed):\n",
        "    print(f\"{'word':20}{'target':20}{'similarity':10}\")\n",
        "    print(\"=\"*50)\n",
        "    for ida, idb, sim in listed:\n",
        "        print(f\"{ida:20}{idb:20}{round(float(sim), 6):6}\")\n",
        "\n",
        "print(\" Cosine similarity \")\n",
        "print_list(cosine_sim_list[0:10])\n",
        "print()\n",
        "\n",
        "print(\" Euclidian distance \")\n",
        "print_list(euclid_sim_list[0:10])\n",
        "print()\n",
        "\n",
        "print(\" Manhattan distance \")\n",
        "print_list(manhat_sim_list[0:10])\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kiq-UPEApL0U",
        "outputId": "631fcd5f-be86-40f5-fbb5-bd61b7618e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100it [00:02, 35.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Cosine similarity \n",
            "word                target              similarity\n",
            "==================================================\n",
            "plain               woman               0.999996\n",
            "snail               snake               0.999995\n",
            "bee                 plain               0.999995\n",
            "chair               ray                 0.999995\n",
            "castle              train               0.999994\n",
            "bowl                woman               0.999994\n",
            "chair               clock               0.999994\n",
            "bee                 bridge              0.999994\n",
            "clock               train               0.999994\n",
            "plain               sea                 0.999994\n",
            "\n",
            " Euclidian distance \n",
            "word                target              similarity\n",
            "==================================================\n",
            "plain               woman               0.270051\n",
            "snail               snake               0.292886\n",
            "chair               ray                 0.29943\n",
            "bee                 plain               0.303731\n",
            "castle              train               0.310533\n",
            "bowl                woman               0.318508\n",
            "chair               clock               0.319932\n",
            "bee                 sea                 0.324723\n",
            "bowl                castle              0.325598\n",
            "bee                 bridge              0.325666\n",
            "\n",
            " Manhattan distance \n",
            "word                target              similarity\n",
            "==================================================\n",
            "plain               woman               5.932135\n",
            "bowl                woman               6.365982\n",
            "chair               ray                 6.422032\n",
            "snail               snake               6.453013\n",
            "castle              train               6.534059\n",
            "bee                 plain               6.568179\n",
            "bowl                plain               6.733428\n",
            "bowl                castle               6.905\n",
            "castle              plate               6.940557\n",
            "chair               clock               6.945336\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wikipedia debug"
      ],
      "metadata": {
        "id": "7jubA6JXUMk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Article to search for { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "totest = \"great white shark\" #@param {type:\"string\"}\n",
        "\n",
        "result = wikipedia.search(totest, suggestion = False)\n",
        "print(result)\n",
        "print(f\"first result is: {result[0]}\")\n",
        "\n",
        "try:\n",
        "    print(wikipedia.page(totest, auto_suggest=False, redirect=True))\n",
        "    print(wikipedia.page(result[0], auto_suggest=False, redirect=True))\n",
        "except Exception as e:\n",
        "    print(f\"best option envisaged: {e.options[0]}\")\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "PeM8XAu6h5gD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94bdb970-b474-4fa7-9184-19556e700efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Great white shark', 'Deep Blue (great white shark)', 'Great White Shark (character)', 'Megalodon', 'Great White (2021 film)', 'List of fatal shark attacks in the United States', 'Shark', 'Jersey Shore shark attacks of 1916', 'Shark attack', 'Rosie the Shark']\n",
            "first result is: Great white shark\n",
            "<WikipediaPage 'Great white shark'>\n",
            "<WikipediaPage 'Great white shark'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articleviewer = ArticleViewer(\"/content/article/cifar100.art\")\n",
        "print(articleviewer.get_all_articles())\n",
        "articleviewer.get(\"king\").summary"
      ],
      "metadata": {
        "id": "O5sAnE0nuhD_",
        "outputId": "3c5f78a6-a6b8-4753-aec0-dcc36af8648b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'computer_keyboard', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-18eff26b951e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0marticleviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArticleViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/article/cifar100.art\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticleviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0marticleviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"king\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-42125d763f18>\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, title)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marticles_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_all_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'king'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test part"
      ],
      "metadata": {
        "id": "ax8Aw_qVRxer"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Different result for King - men + women equation with different context window size"
      ],
      "metadata": {
        "id": "uUU3kl9sbmg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "\n",
        "save_name = \"king-test\"\n",
        "vocab = [\"king\", \"queen\", \"men\", \"woman\"]\n",
        "\n",
        "# king_test_model = ROBERTAModel(vocab, big = False, window = 0)\n",
        "try:\n",
        "    king_test_model = Wiki2VecModel(vocab)\n",
        "except:\n",
        "    print(\"haha\")\n",
        "\n",
        "king_test_articlesRetriever = ArticleRetriever(save_name + \".art\", vocab)\n",
        "if king_test_articlesRetriever():\n",
        "    king_test_articlesRetriever.save()\n",
        "\n",
        "result = king_test_model.convert(king_test_articlesRetriever)\n",
        "\n",
        "if result is not None:\n",
        "    print(result)\n",
        "\n",
        "king_test_model.export(save_name + \".csv\")\n",
        "king_test_solver = Solver(save_name + \".csv\")\n",
        "\n",
        "men = king_test_model.get_embedding_of(\"men\")\n",
        "woman = king_test_model.get_embedding_of(\"woman\")\n",
        "king = king_test_model.get_embedding_of(\"king\")\n",
        "\n",
        "totest = king.sub(men).add(woman)\n",
        "print()\n",
        "king_test_solver(totest, \"King - man + woman\")"
      ],
      "metadata": {
        "id": "8uNXNj_wCWhf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "602df47a-9f5c-4ea5-9b26-42b65ea67cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 26132.74it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordsim353 = nlp.data.WordSim353('all')"
      ],
      "metadata": {
        "id": "dnkI3j0V-CkE",
        "outputId": "fa5fcc29-215a-4190-f9e7-2a7ac01297c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading /root/.mxnet/datasets/wordsim353/ws353simrel.tar.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/ws353/ws353simrel.tar.gz...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| model | window size | rank of Queen | distance with first |\n",
        "|-------|-------------|---------------|-----------|\n",
        "| bert-large | 0  | 2 |  .0693 |\n",
        "| bert-large | 10 | 3 |  .1191 | \n",
        "| bert-large | 50 | 2 |  .1672 |\n",
        "| bert-large | 100 | 2 | .1275 |\n",
        "| bert-large | 150 | 3 | .1134 |\n",
        "| bert-large | 200 | 3 | .1923 |\n",
        "| bert-large | 300 | 3 | .0939 |\n",
        "| bert-large | 400 | 3 | .1455 |\n",
        "| roberta-large | 0 | 2 | .0001 |\n",
        "| roberta-large | 10 | 3 | .0011 |\n",
        "| roberta-large | 50 | 2 | .0029 |\n",
        "| roberta-large | 100 | 4 | .0061 |\n",
        "| roberta-large | 150 | 3 | .0023 |\n",
        "| roberta-large | 200 | 4 | .0055 |\n",
        "| roberta-large | 300 | 4 | .0127 |\n",
        "| roberta-large | 400 | 4 | .0045 |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kY8yWSmJw6Ss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pearson correlation rank with different context window size"
      ],
      "metadata": {
        "id": "0hjQnEodbcAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "\n",
        "save_name = \"wordsim353\"\n",
        "vocab = []\n",
        "\n",
        "for w1, w2, i in wordsim353:\n",
        "    if w1 not in vocab:\n",
        "        vocab.append(w1)\n",
        "    if w2 not in vocab:\n",
        "        vocab.append(w2)\n",
        "\n",
        "print(len(vocab))\n",
        "\n",
        "# wordsim353_model = ROBERTAModel(vocab, big = False, window = 0)\n",
        "# wordsim353_model = BERTModel(vocab, big = False, window = 0)\n",
        "wordsim353_model = ROBERTAModel(vocab, big = False, window = 0)\n",
        "\n",
        "articlesRetriever = ArticleRetriever(save_name + \".art\", vocab)\n",
        "\n",
        "if articlesRetriever(force_reload = True):\n",
        "    articlesRetriever.save()\n",
        "\n",
        "wordsim353_model.convert(articlesRetriever)\n",
        "wordsim353_model.export(save_name + \".csv\")"
      ],
      "metadata": {
        "id": "YmFXzvYYBVzf",
        "outputId": "c57990fa-255b-48d5-e6c2-9cc87998d94c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "  0%|          | 0/437 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n",
            "100%|██████████| 437/437 [01:17<00:00,  5.63it/s]\n",
            "100%|██████████| 437/437 [00:31<00:00, 13.83it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "total_comparison = 0\n",
        "sim_list = []\n",
        "i_list = []\n",
        "\n",
        "sim_computer = SimilarityMatrix(save_name + \".csv\")\n",
        "\n",
        "for w1, w2, i in wordsim353:\n",
        "    # print(f\"how much {w1} is similar to {w2}:\")\n",
        "    sim = sim_computer.sim_between(w1, w2)\n",
        "\n",
        "    sim_list.append(sim)\n",
        "    i_list.append(i)\n",
        "\n",
        "    # print(f\"\\t{sim} & {i}\")\n",
        "    total_comparison += 1\n",
        "\n",
        "\n",
        "print(spearmanr(sim_list, i_list))"
      ],
      "metadata": {
        "id": "Vwy1yo7iIgnd",
        "outputId": "02c96d7a-1a43-44b7-d1da-676b541e8cee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpearmanrResult(correlation=0.20122918054751185, pvalue=0.00014416169395114014)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "| model | corpus | test set |window size | pearson rank correlation |\n",
        "|-------|--------|----------|------------|--------------------------|\n",
        "| bert large | article from wordsim vocab | wordsim353 | 100 | 0.2158 (5.8353e-05)|\n",
        "| bert base  | article from wordsim vocab | wordsim353 | 100 | 0.2284 (6.8215e-05)|\n",
        "| bert-base  | article from wordsim vocav | wordsim353 | 300 | 0.1326 (1.27e-02) |\n",
        "| bert-large  | article from wordsim vocav | wordsim353 | 300 | 0.2117 (6.2153e-05) |\n",
        "| bert-large  | article from wordsim vocav | wordsim353 | 0 | 0.2638 (5.1232-07) |\n",
        "| bert-base  | article from wordsim vocav | wordsim353 | 0 | 0.3721 (5.2306-13) |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-VuUxJc6zumT"
      }
    }
  ]
}