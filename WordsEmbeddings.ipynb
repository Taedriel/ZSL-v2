{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordsEmbeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taedriel/ZSL-v2/blob/wordEmbedding/WordsEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yes | pip install transformers wget tensorflow_datasets wikipedia unzip mxnet gluonnlp scipy>=1.7 scikit-bio --quiet --upgrade\n",
        "!mkdir -p temp article"
      ],
      "metadata": {
        "id": "HEq14T6Nl535"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "import wikipedia\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "\n",
        "import gluonnlp as nlp\n",
        "import traceback\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, RobertaModel, RobertaTokenizer\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level = logging.INFO, filename = \"BERT.log\" )\n",
        "# logging.basicConfig(level = logging.INFO)\n",
        "\n",
        "from tqdm import tqdm\n",
        "from typing import List, Tuple\n",
        "from os.path import exists, join\n",
        "from enum import Enum\n",
        "\n",
        "wikipedia.set_rate_limiting(True)"
      ],
      "metadata": {
        "id": "spXWSv2iW9i5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils classes"
      ],
      "metadata": {
        "id": "kjt7fI8Olf5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class customWikiArticle:\n",
        "\n",
        "    def __init__(self, index : int, title : str, realtitle : str, summary : str, ambiguous : bool):\n",
        "        self.index = index\n",
        "        self.title = title\n",
        "        self.realtitle = realtitle\n",
        "        self.summary = summary\n",
        "        self.ambiguous = ambiguous\n",
        "\n",
        "class ArticleRetriever:\n",
        "\n",
        "    article_dir = \"./article\"\n",
        "\n",
        "    def __init__(self, name : str, list_title : List[str]):\n",
        "\n",
        "        self.name = name\n",
        "        self.list_title = list_title\n",
        "        self.modified = False\n",
        "\n",
        "        if not exists(self.get_filename()):\n",
        "            self.articles_map = {}\n",
        "        else:\n",
        "            with open(self.get_filename(), \"rb\") as mapfile:\n",
        "                self.articles_map = pickle.load(mapfile)\n",
        "\n",
        "    def get_filename(self):\n",
        "        return join(ArticleRetriever.article_dir,self.name)\n",
        "\n",
        "    def load_article(self, title, force_reload : bool = False) -> customWikiArticle:\n",
        "        if title not in self.articles_map:\n",
        "            self.modified = True\n",
        "            realtitle, summary, ambiguous = self._retrieve_article(title)\n",
        "            self.articles_map[title] = customWikiArticle(len(self.articles_map), title, realtitle, summary, ambiguous)\n",
        "\n",
        "        if title in self.articles_map and self.articles_map[title].summary == None and force_reload:\n",
        "            self.modified = True\n",
        "            realtitle, summary, ambiguous = self._retrieve_article(title)\n",
        "            self.articles_map[title].summary = summary\n",
        "\n",
        "        return self.articles_map[title]\n",
        "\n",
        "    def _retrieve_article(self, title, closed_list : List = []) -> Tuple[str, str, bool]: \n",
        "        closed_list.append(title)\n",
        "        try:\n",
        "            article = wikipedia.page(title, auto_suggest=False, redirect=True)\n",
        "            return (article.title, article.summary, False)\n",
        "\n",
        "        except wikipedia.PageError as e:\n",
        "            search_result = wikipedia.search(title, suggestion = False)\n",
        "\n",
        "            logging.warning(f\"{title} misspelled or article missing. Best find is {search_result[0]}\")\n",
        "            if search_result[0] is not None and search_result[0] not in closed_list:            \n",
        "                return self._retrieve_article(search_result[0], closed_list)  \n",
        "            else: return (title, None, False)\n",
        "\n",
        "        except wikipedia.DisambiguationError as e:\n",
        "            logging.warning(f\"{title} is ambiguous, trying first {e.options[0]}\")\n",
        "            if e.options[0] is not None and e.options[0] not in closed_list:\n",
        "                res = self._retrieve_article(e.options[0], closed_list)\n",
        "                return (res[0], res[1], True)\n",
        "        \n",
        "            return (None, None, None)\n",
        "\n",
        "\n",
        "    def load_all_articles(self, force_reload : bool = False) -> None:\n",
        "        logging.info(f\"Starting loading articles...\")\n",
        "        nb_success = 0\n",
        "\n",
        "        nb_article = len(self.list_title)\n",
        "        for i, title in tqdm(enumerate(self.list_title), total=nb_article):\n",
        "            self.load_article(title, force_reload)\n",
        "\n",
        "            if self.articles_map[title].summary is not None: \n",
        "                nb_success += 1\n",
        "        logging.info(f\"Finished loading {nb_success} article(s) !\")\n",
        "        return self.modified\n",
        "\n",
        "    def __call__(self, force_reload : bool = True) -> None:\n",
        "        return self.load_all_articles(force_reload)\n",
        "\n",
        "    def get_article(self, title):\n",
        "        if title not in self.articles_map:\n",
        "            self.load_article(title)\n",
        "\n",
        "        return self.articles_map[title]\n",
        "        \n",
        "    def save(self):\n",
        "        with open(self.get_filename(), \"wb\") as mapfile:\n",
        "            pickle.dump(self.articles_map, mapfile)\n",
        "\n",
        "class WordToVector:\n",
        "\n",
        "    def __init__(self, list_tags : List[str] = []):\n",
        "        self.list_tags = list_tags\n",
        "        self.embeddings = {}\n",
        "\n",
        "    def export(self, filename):\n",
        "        return NotImplementedError\n",
        "\n",
        "    def convert(self):\n",
        "        return NotImplementedError\n",
        "\n",
        "class Sum4LastLayers:\n",
        "\n",
        "    def merge(self, vector):\n",
        "        return torch.sum(vector[-4:], dim = 0)\n"
      ],
      "metadata": {
        "id": "QZiZ2Pr-lUiS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingsLoader:\n",
        "\n",
        "    def __init__(self, filename : str):\n",
        "\n",
        "        self.file = filename\n",
        "        self.embeddings = {}\n",
        "\n",
        "        self._load_file()\n",
        "\n",
        "    def _load_file(self):\n",
        "        try:\n",
        "            with open(self.file, \"r\") as f:\n",
        "                lines = f.readlines()\n",
        "                \n",
        "            for line in lines[1:]:\n",
        "                data = line.split(\",\")\n",
        "                self.embeddings[data[0]] = torch.FloatTensor(list(map(float, data[1:])))\n",
        "\n",
        "        except IOError as e:\n",
        "            raise IOError(f\"No file {self.file}\")\n",
        "\n",
        "class SimilarityCompute(EmbeddingsLoader):\n",
        "\n",
        "    def __init__(self, embeddings):\n",
        "        super(SimilarityCompute, self).__init__(embeddings)\n",
        "        self._create_matrix()\n",
        "        self.computed : bool = False\n",
        "\n",
        "    def _create_matrix(self) -> None:\n",
        "        n_tokens = len(self.embeddings)\n",
        "        self.cosine_sim_matrix : Dict[Dict[float]] = {}\n",
        "        for tag in self.embeddings.keys():\n",
        "            self.cosine_sim_matrix[tag] = {}\n",
        "\n",
        "    def compute_sim(self) -> None:\n",
        "        \"\"\" compute cosine similarity between all vectors \"\"\"\n",
        "\n",
        "        logging.info(\"Computing cosine similarity, this could take some time...\")\n",
        "        for tag, vector in tqdm(self.embeddings.items(), total = len(self.embeddings)):\n",
        "\n",
        "            for otag, other_vector in self.embeddings.items():\n",
        "\n",
        "                if otag == tag:\n",
        "                    continue\n",
        "\n",
        "                cos = torch.nn.CosineSimilarity(dim=0)\n",
        "                similarity = cos(vector, other_vector)\n",
        "\n",
        "                self.cosine_sim_matrix[otag][tag] = similarity\n",
        "                self.cosine_sim_matrix[tag][otag] = similarity\n",
        "\n",
        "        self.computed = True\n",
        "\n",
        "    def export_sim_matrix(self, filename):\n",
        "        if not self.computed:\n",
        "            self.compute_sim()\n",
        "        \n",
        "        try:\n",
        "            f = open(filename, \"w\")\n",
        "        except OSError:\n",
        "            raise OSError(\"Could not open file\")\n",
        "\n",
        "        with f:\n",
        "            print(\"/\", *[tag for tag in self.embeddings.keys()], sep = \",\", file = f)\n",
        "\n",
        "            for tag in self.embeddings.keys():\n",
        "                print(tag, *[str(round(float(self.cosine_sim_matrix[tag][otag]), 3)) for otag in self.embeddings.keys()], sep = \",\", file = f)\n",
        "\n",
        "    def get_sim_matrix(self) -> Tuple[List[str], List[List[float]]]:\n",
        "        \"\"\"return the similarity matrix of the embeddings\n",
        "        \"\"\"\n",
        "        if not self.computed:\n",
        "            self.compute_sim()\n",
        "\n",
        "        X = len(self.embeddings)\n",
        "        matrix = [[0 for j in range(X)] for i in range(X)]\n",
        "        ids = []\n",
        "        \n",
        "        for i, tag in enumerate(self.embeddings.keys()):\n",
        "            ids.append(tag)\n",
        "            for j, otag in enumerate(self.embeddings.keys()):\n",
        "                if i == j:\n",
        "                    continue\n",
        "\n",
        "                matrix[i][j] = self.cosine_sim_matrix[tag][otag]\n",
        "                matrix[j][i] = self.cosine_sim_matrix[tag][otag]\n",
        "\n",
        "        return ids, matrix\n",
        "\n",
        "    def sim_between(self, token1 : str, token2 : str) -> float:\n",
        "        v1 = self.embeddings[token1]\n",
        "        v2 = self.embeddings[token2]\n",
        "\n",
        "        if token2 not in self.cosine_sim_matrix[token1] or token1 not in self.cosine_sim_matrix[token2]:\n",
        "            cos = torch.nn.CosineSimilarity(dim=0)\n",
        "            similarity = cos(v1, v2)\n",
        "\n",
        "            self.cosine_sim_matrix[token1][token2] = similarity\n",
        "            self.cosine_sim_matrix[token1][token2] = similarity\n",
        "\n",
        "        return self.cosine_sim_matrix[token1][token2]\n",
        "\n",
        "class Solver(EmbeddingsLoader):\n",
        "\n",
        "    DEFAULT_MIN_LIST_RESULT = 10\n",
        "\n",
        "    def __init__(self, embeddings):\n",
        "        super(Solver, self).__init__(embeddings)\n",
        "\n",
        "    def get_nearest_embedding_of(self, embedding, nb = 10):\n",
        "\n",
        "        if nb > len(self.embeddings):\n",
        "            raise Exception(\"nb too high, not enough token\")\n",
        "\n",
        "        nearest = []\n",
        "        for tag, e in self.embeddings.items():\n",
        "\n",
        "            cos = torch.nn.CosineSimilarity(dim=0)\n",
        "            similarity = cos(embedding, e)\n",
        "\n",
        "            nearest.append((tag, similarity))\n",
        "        \n",
        "        nearest.sort(key = lambda tup : tup[1])\n",
        "        return nearest[-1:-nb-1:-1]\n",
        "\n",
        "    def __call__(self, embeddeding):\n",
        "        return self.get_nearest_embedding_of(embeddeding, min(Solver.DEFAULT_MIN_LIST_RESULT, len(self.embeddings)))"
      ],
      "metadata": {
        "id": "5AxDIhr1rkca"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "nYaqmgXe1-EX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT model"
      ],
      "metadata": {
        "id": "-0g4CJtelYfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTModel(WordToVector):\n",
        "\n",
        "    temp_dir = \"./temp\"\n",
        "\n",
        "    def __init__(self, list_tag : List[str], big: bool = False, window : int = 100):\n",
        "        super(BERTModel, self).__init__(list_tag)\n",
        "        self.list_articles = {}\n",
        "        self.window_size = window\n",
        "\n",
        "        self.model_size = \"bert-large-uncased\" if big else \"bert-base-uncased\"\n",
        "        self.cosine_sim_matrix = None\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.model_size, padding=True, truncation=True,)\n",
        "        self.model = BertModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "    def export(self, filename):\n",
        "        \"\"\"export all the embeddings in filename under a .csv format.\n",
        "           Raise exception if embeddings hasn't been calculed yet.\"\"\"\n",
        "\n",
        "        if len(self.embeddings) == 0:\n",
        "            raise Exception(\"Tags not converted yet !\")\n",
        "        \n",
        "        try:\n",
        "            f = open(filename, \"w\")\n",
        "        except OSError:\n",
        "            raise OSError(\"Could not open file\")\n",
        "\n",
        "        dimension_number = len(next(iter(self.embeddings.values())))\n",
        "        with f:\n",
        "            print(\"embeddings\", *[str(i) for i in range(dimension_number)], sep=\",\", file=f)\n",
        "            for tag, embedding in self.embeddings.items():\n",
        "                print(tag, *list(map(lambda x: str(float(x)), embedding)), sep=\",\", file=f)\n",
        "\n",
        "    def reset_embeddings(self):\n",
        "        self.embeddings.clear()\n",
        "\n",
        "    def convert(self, article_ret : ArticleRetriever):\n",
        "        \"\"\" convert all word in their embeddings\"\"\"\n",
        "\n",
        "        if len(self.list_tags) == 0:\n",
        "            raise Exception(\"not tags yet !\")\n",
        "\n",
        "        logging.info(\"Starting converting tokens...\")\n",
        "        nb_token = len(self.list_tags)\n",
        "        for i, tag in tqdm(enumerate(self.list_tags), total = nb_token):\n",
        "            \n",
        "            article = article_ret.get_article(tag)\n",
        "            tag_plus_context = tag\n",
        "            if article.summary is not None:\n",
        "                max_size_article = min(len(article.summary), self.window_size)\n",
        "                tag_plus_context = tag + \". \" + article.summary[:max_size_article]\n",
        "\n",
        "            inputs = self.tokenizer(tag_plus_context, return_tensors = \"pt\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            hidden_states = outputs[2]\n",
        "\n",
        "            # [# layers, # batches, # tokens, # features] ==> [# tokens, # layers, # features]\n",
        "            token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "            token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "            token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "            # apply different strategy to summarize word embeddings\n",
        "            # tokenized_text = self.tokenizer.tokenize(tag)\n",
        "            # acc = []\n",
        "            # for i, token in reversed(list(enumerate(tokenized_text))):\n",
        "\n",
        "            #     embed = self.merging_strategy.merge(token_embeddings[i+1])\n",
        "            #     if i == 0:\n",
        "            #         if len(acc) != 0:\n",
        "            #             embed = torch.mean(torch.stack([x[1] for x in acc]), dim=0)\n",
        "            #             token = tag\n",
        "            #             acc = []\n",
        "\n",
        "            #         self.embeddings.append((token, embed))\n",
        "            #     else:\n",
        "            #         acc.append((token, embed))\n",
        "            \n",
        "            # here we are just taking the [CLS] (for classification) as an embedding for the tag\n",
        "            self.embeddings[tag] = self.merging_strategy.merge(token_embeddings[0])\n",
        "\n",
        "    def get_embedding_of(self, token):\n",
        "        if token not in self.embeddings:\n",
        "            raise Exception(f\"no such token {token}\")\n",
        "        \n",
        "        return self.embeddings[token]\n",
        "\n",
        "    def get_class_list(self):\n",
        "        return self.embeddings.keys()"
      ],
      "metadata": {
        "id": "UVJUSGWQorYj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RoBERTa model"
      ],
      "metadata": {
        "id": "eGVvWFvtlqLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ROBERTAModel(BERTModel):\n",
        "\n",
        "    def __init__(self, list_tag : List[str], big: bool = False, window : int = 100):\n",
        "        WordToVector.__init__(self, list_tag)\n",
        "        self.window_size = window\n",
        "\n",
        "        self.model_size = \"roberta-large\" if big else \"roberta-base\"\n",
        "        self.cosine_sim_matrix = None\n",
        "\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_size, padding=True, truncation=True,)\n",
        "        self.model = RobertaModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()"
      ],
      "metadata": {
        "id": "7oWZOwG-mCYI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wikipedia2Vec"
      ],
      "metadata": {
        "id": "mFf6OoBG1xVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://wikipedia2vec.s3.amazonaws.com/models/en/2018-04-20/enwiki_20180420_win10_500d.txt.bz2"
      ],
      "metadata": {
        "id": "tB7Odt1813if",
        "outputId": "ba392880-5c66-4590-b858-d5ab5e9afb0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-25 16:11:31--  http://wikipedia2vec.s3.amazonaws.com/models/en/2018-04-20/enwiki_20180420_win10_500d.txt.bz2\n",
            "Resolving wikipedia2vec.s3.amazonaws.com (wikipedia2vec.s3.amazonaws.com)... 52.219.17.21\n",
            "Connecting to wikipedia2vec.s3.amazonaws.com (wikipedia2vec.s3.amazonaws.com)|52.219.17.21|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4149376202 (3.9G) [application/x-bzip2]\n",
            "Saving to: ‘enwiki_20180420_win10_500d.txt.bz2’\n",
            "\n",
            "enwiki_20180420_win 100%[===================>]   3.86G  18.4MB/s    in 3m 39s  \n",
            "\n",
            "2022-05-25 16:15:11 (18.1 MB/s) - ‘enwiki_20180420_win10_500d.txt.bz2’ saved [4149376202/4149376202]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings to word proba"
      ],
      "metadata": {
        "id": "JNFmhVBmUTgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "solver = Solver(\"animal10-embeddeding.csv\")\n",
        "\n",
        "totest = solver.embeddings[\"cat\"]\n",
        "\n",
        "print(solver.get_nearest_embedding_of(totest))"
      ],
      "metadata": {
        "id": "tD8XokPDUhkp",
        "outputId": "777a2c6b-eb71-4f14-f029-1df2b2f9d8a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('cat', tensor(1.)), ('horse', tensor(0.9067)), ('squirrel', tensor(0.8875)), ('chicken', tensor(0.8678)), ('butterfly', tensor(0.8576)), ('cow', tensor(0.8493)), ('spider', tensor(0.8481)), ('dog', tensor(0.8410)), ('elephant', tensor(0.7702)), ('sheep', tensor(0.7604))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
        "# imagenet_labels = np.array(open(labels_path).read().splitlines())\n",
        "# imagenet_labels = list(imagenet_labels)\n",
        "# sub_imagenet_labels = imagenet_labels[:200]"
      ],
      "metadata": {
        "id": "NIJ8zsk-UTuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word to embeddings"
      ],
      "metadata": {
        "id": "LN273RlCUGza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "animal10 = [\"dog\", \"cat\", \"horse\", \"spider\", \"butterfly\", \"chicken\", \"sheep\", \"cow\", \"squirrel\", \"elephant\"]\n",
        "cifar10  = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "cifar100 = [\"apple\", \"aquarium_fish\", \"baby\", \"bear\", \"beaver\", \"bed\", \"bee\", \"beetle\", \"bicycle\", \"bottle\", \"bowl\", \"boy\", \"bridge\", \"bus\", \"butterfly\", \"camel\", \"can\", \\\n",
        "            \"castle\", \"caterpillar\", \"cattle\", \"chair\", \"chimpanzee\", \"clock\", \"cloud\", \"cockroach\", \"computer_keyboard\", \"couch\", \"crab\", \"crocodile\", \"cup\", \\\n",
        "            \"dinosaur\", 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'lamp', 'lawn_mower', 'leopard', 'lion', \\\n",
        "            'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', \\\n",
        "            'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark',\\\n",
        "            'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', \\\n",
        "            'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']"
      ],
      "metadata": {
        "id": "EYnbJc84xuh5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_name = \"cifar10\"\n",
        "vocab = cifar10\n",
        "\n",
        "# model = BERTModel(model, big = True, window = 100)\n",
        "model = ROBERTAModel(vocab, big = True, window = 100)\n",
        "\n",
        "articlesRetriever = ArticleRetriever(save_name + \".art\", vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvIn_JeFlYfK",
        "outputId": "a15db180-d394-4abe-b487-6f75ec401fe9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if articlesRetriever():\n",
        "    articlesRetriever.save()\n",
        "\n",
        "model.reset_embeddings()\n",
        "model.convert(articlesRetriever)\n",
        "model.export(f\"{save_name}-{model.model_size}-{model.window_size}.csv\")\n",
        "print(\"\\n\", len(model.get_class_list()))"
      ],
      "metadata": {
        "id": "WOOlgG8koVD6",
        "outputId": "8c26c0f0-14e4-441c-f1b0-7795aa4bca25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:03<00:00,  3.16it/s]\n",
            "100%|██████████| 10/10 [00:05<00:00,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neighboor joining Tree"
      ],
      "metadata": {
        "id": "0WZFTDaGbr1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import SpearmanRConstantInputWarning\n",
        "from skbio import DistanceMatrix\n",
        "from skbio.tree import nj"
      ],
      "metadata": {
        "id": "SPQXoPQEbrWS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neighboor_sim = SimilarityCompute(\"/content/cifar10-roberta-large-100.csv\")\n",
        "\n",
        "ids, data = neighboor_sim.get_sim_matrix()\n",
        "\n",
        "dm = DistanceMatrix(data, ids)\n",
        "tree = nj(dm)\n",
        "print(\"\\n\\n\", tree.ascii_art())"
      ],
      "metadata": {
        "id": "crcMR8dDuH7s",
        "outputId": "0d4a62fc-2be8-48a8-d3b4-08a96def3a9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 1240.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "                     /-ship\n",
            "          /--------|\n",
            "         |         |          /-frog\n",
            "         |          \\--------|\n",
            "         |                    \\-automobile\n",
            "         |\n",
            "         |                    /-truck\n",
            "         |          /--------|\n",
            "         |         |          \\-horse\n",
            "---------|---------|\n",
            "         |         |          /-dog\n",
            "         |          \\--------|\n",
            "         |                   |          /-deer\n",
            "         |                    \\--------|\n",
            "         |                              \\-airplane\n",
            "         |\n",
            "         |          /-cat\n",
            "          \\--------|\n",
            "                    \\-bird\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wikipedia debug"
      ],
      "metadata": {
        "id": "7jubA6JXUMk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Article to search for { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "totest = \"king\" #@param {type:\"string\"}\n",
        "\n",
        "result = wikipedia.search(totest, suggestion = False)\n",
        "print(result)\n",
        "print(f\"first result is: {result[0]}\")\n",
        "\n",
        "try:\n",
        "    print(wikipedia.page(totest, auto_suggest=False, redirect=True))\n",
        "    print(wikipedia.page(result[0], auto_suggest=False, redirect=True))\n",
        "except Exception as e:\n",
        "    print(f\"best option envisaged: {e.options[0]}\")\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "PeM8XAu6h5gD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3d0a8cf-88bf-4da4-a111-1bc05d416cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['King', 'King (disambiguation)', 'Martin Luther King Jr.', 'Stephen King', 'King & King', 'George VI', 'The Lion King', 'Burger King', 'King King', 'List of King of the Hill episodes']\n",
            "first result is: King\n",
            "<WikipediaPage 'King'>\n",
            "<WikipediaPage 'King'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test part"
      ],
      "metadata": {
        "id": "ax8Aw_qVRxer"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Different result for King - men + women equation with different context window size"
      ],
      "metadata": {
        "id": "uUU3kl9sbmg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "\n",
        "save_name = \"king-test\"\n",
        "vocab = [\"King\", \"Queen\", \"men\", \"woman\"]\n",
        "\n",
        "king_test_model = ROBERTAModel(vocab, big = False, window = 10)\n",
        "\n",
        "king_test_articlesRetriever = ArticleRetriever(save_name + \".art\", vocab)\n",
        "if king_test_articlesRetriever():\n",
        "    king_test_articlesRetriever.save()\n",
        "\n",
        "\n",
        "king_test_model.convert(articlesRetriever)\n",
        "king_test_model.export(save_name + \".csv\")\n",
        "king_test_solver = Solver(save_name + \".csv\")\n",
        "\n",
        "\n",
        "men = king_test_model.get_embedding_of(\"men\")\n",
        "woman = king_test_model.get_embedding_of(\"woman\")\n",
        "king = king_test_model.get_embedding_of(\"King\")\n",
        "\n",
        "totest = king.sub(men).add(woman)\n",
        "print(\"\\n\", king_test_solver(totest))"
      ],
      "metadata": {
        "id": "8uNXNj_wCWhf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21dc0cda-90a6-42e6-9c5c-9ba4e8909a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 4/4 [00:00<00:00, 23730.15it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00,  8.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " [('woman', tensor(0.9997)), ('King', tensor(0.9996)), ('Queen', tensor(0.9996)), ('men', tensor(0.9986))]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordsim353 = nlp.data.WordSim353('all')"
      ],
      "metadata": {
        "id": "dnkI3j0V-CkE",
        "outputId": "fa5fcc29-215a-4190-f9e7-2a7ac01297c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading /root/.mxnet/datasets/wordsim353/ws353simrel.tar.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/ws353/ws353simrel.tar.gz...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| model | window size | rank of Queen | distance with first |\n",
        "|-------|-------------|---------------|-----------|\n",
        "| bert-large | 0  | 2 |  .0693 |\n",
        "| bert-large | 10 | 3 |  .1191 | \n",
        "| bert-large | 50 | 2 |  .1672 |\n",
        "| bert-large | 100 | 2 | .1275 |\n",
        "| bert-large | 150 | 3 | .1134 |\n",
        "| bert-large | 200 | 3 | .1923 |\n",
        "| bert-large | 300 | 3 | .0939 |\n",
        "| bert-large | 400 | 3 | .1455 |\n",
        "| roberta-large | 0 | 2 | .0001 |\n",
        "| roberta-large | 10 | 3 | .0011 |\n",
        "| roberta-large | 50 | 2 | .0029 |\n",
        "| roberta-large | 100 | 4 | .0061 |\n",
        "| roberta-large | 150 | 3 | .0023 |\n",
        "| roberta-large | 200 | 4 | .0055 |\n",
        "| roberta-large | 300 | 4 | .0127 |\n",
        "| roberta-large | 400 | 4 | .0045 |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kY8yWSmJw6Ss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pearson correlation rank with different context window size"
      ],
      "metadata": {
        "id": "0hjQnEodbcAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "\n",
        "save_name = \"wordsim353\"\n",
        "vocab = []\n",
        "\n",
        "for w1, w2, i in wordsim353:\n",
        "    if w1 not in vocab:\n",
        "        vocab.append(w1)\n",
        "    if w2 not in vocab:\n",
        "        vocab.append(w2)\n",
        "\n",
        "print(len(vocab))\n",
        "wordsim353_model = ROBERTAModel(vocab, big = False, window = 0)\n",
        "articlesRetriever = ArticleRetriever(save_name + \".art\", vocab)\n",
        "\n",
        "if articlesRetriever(force_reload = True):\n",
        "    articlesRetriever.save()\n",
        "\n",
        "wordsim353_model.convert(articlesRetriever)\n",
        "wordsim353_model.export(save_name + \".csv\")"
      ],
      "metadata": {
        "id": "YmFXzvYYBVzf",
        "outputId": "c57990fa-255b-48d5-e6c2-9cc87998d94c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "  0%|          | 0/437 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n",
            "100%|██████████| 437/437 [01:17<00:00,  5.63it/s]\n",
            "100%|██████████| 437/437 [00:31<00:00, 13.83it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "total_comparison = 0\n",
        "sim_list = []\n",
        "i_list = []\n",
        "\n",
        "sim_computer = SimilarityCompute(save_name + \".csv\")\n",
        "\n",
        "for w1, w2, i in wordsim353:\n",
        "    # print(f\"how much {w1} is similar to {w2}:\")\n",
        "    sim = sim_computer.sim_between(w1, w2)\n",
        "\n",
        "    sim_list.append(sim)\n",
        "    i_list.append(i)\n",
        "\n",
        "    # print(f\"\\t{sim} & {i}\")\n",
        "    total_comparison += 1\n",
        "\n",
        "\n",
        "print(spearmanr(sim_list, i_list))"
      ],
      "metadata": {
        "id": "Vwy1yo7iIgnd",
        "outputId": "02c96d7a-1a43-44b7-d1da-676b541e8cee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpearmanrResult(correlation=0.20122918054751185, pvalue=0.00014416169395114014)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "| model | corpus | test set |window size | pearson rank correlation |\n",
        "|-------|--------|----------|------------|--------------------------|\n",
        "| bert large | article from wordsim vocab | wordsim353 | 100 | 0.2158 (5.8353e-05)|\n",
        "| bert base  | article from wordsim vocab | wordsim353 | 100 | 0.2284 (6.8215e-05)|\n",
        "| bert-base  | article from wordsim vocav | wordsim353 | 300 | 0.1326 (1.27e-02) |\n",
        "| bert-large  | article from wordsim vocav | wordsim353 | 300 | 0.2117 (6.2153e-05) |\n",
        "| bert-large  | article from wordsim vocav | wordsim353 | 0 | 0.2638 (5.1232-07) |\n",
        "| bert-base  | article from wordsim vocav | wordsim353 | 0 | 0.3721 (5.2306-13) |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-VuUxJc6zumT"
      }
    }
  ]
}