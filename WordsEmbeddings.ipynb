{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordsEmbeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taedriel/ZSL-v2/blob/wordEmbedding/WordsEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation part"
      ],
      "metadata": {
        "id": "bYtbXyM-iE1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import stuff\n"
      ],
      "metadata": {
        "id": "7fqDjGPZE1hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yes | pip install transformers wget wikipedia unzip mxnet gluonnlp \"scipy>=1.7\" scikit-bio wikipedia2vec --quiet --upgrade\n",
        "!mkdir -p temp article\n",
        "%xmode Verbose"
      ],
      "metadata": {
        "id": "HEq14T6Nl535",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fbb1dfb-c5a5-4e66-8276-d04381c1682e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception reporting mode: Verbose\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as  np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, RobertaModel, RobertaTokenizer\n",
        "import tensorflow as tf\n",
        "import gluonnlp as nlp\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from wikipedia2vec import Wikipedia2Vec\n",
        "import wikipedia\n",
        "wikipedia.set_rate_limiting(True)\n",
        "\n",
        "import gc\n",
        "import traceback\n",
        "import pickle\n",
        "import json\n",
        "import logging\n",
        "logging.basicConfig(level = logging.INFO, filename = \"BERT.log\" )\n",
        "# logging.basicConfig(level = logging.INFO)\n",
        "\n",
        "from tqdm import tqdm\n",
        "from typing import List, Tuple, Dict, Callable\n",
        "from os.path import exists, join, abspath\n",
        "from os import system\n",
        "from enum import Enum\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "from scipy.stats import SpearmanRConstantInputWarning\n",
        "from scipy.stats import spearmanr\n",
        "from skbio import DistanceMatrix\n",
        "from skbio.tree import nj"
      ],
      "metadata": {
        "id": "spXWSv2iW9i5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils classes"
      ],
      "metadata": {
        "id": "kjt7fI8Olf5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usefull functions ✅"
      ],
      "metadata": {
        "id": "W3-C-5ztMDZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dict2csv(filename : str, embeddings : Dict[str, List[float]]) -> None:\n",
        "    \"\"\" write a dict of embeddings under a .CSV file\n",
        "\n",
        "    the .CSV file is construct with a header looking like this :\n",
        "    \\tembeddings\\t | 0 | 1 | 2 | 3 | ...\n",
        "    where each line contain an embeddings for the word in the first row\n",
        "    Args:\n",
        "        filename (str) : a path to the file where the .csv is to be written\n",
        "        embeddings (Dict[str, List[float]]): a dictionnary of embeddings\n",
        "\n",
        "    \"\"\"\n",
        "    try:\n",
        "        f = open(filename, \"w\")\n",
        "    except OSError:\n",
        "        raise OSError(\"Could not open file\")\n",
        "\n",
        "    dimension_number = len(next(iter(embeddings.values())))\n",
        "    with f:\n",
        "        print(\"embeddings\", *[str(i) for i in range(dimension_number)], sep=\",\", file=f)\n",
        "        for tag, embedding in embeddings.items():\n",
        "            print(tag, *list(map(lambda x: str(float(x)), embedding)), sep=\",\", file=f)\n",
        "\n",
        "def sim2dist(mat : List[List[float]], func : Callable[[float], float] \\\n",
        "             = lambda x: 1 - x, hollow : bool = True) -> List[List[float]]:\n",
        "    \"\"\" map the function func to each elements in the matrix\n",
        "\n",
        "    apply the lambda function func to each element of the matrix. if hollow is set \n",
        "    to True, set the diagonal of the matrix to 0.\n",
        "    Args:\n",
        "        mat (List[List[float]]) : a matrix of number\n",
        "        func (Callable[[float], float]) : a simple function to apply to each elem of the matrix\n",
        "        hollow (bool) : whether to consider the diagonal of the matrix or not\n",
        "    \n",
        "    \"\"\"\n",
        "    inv_data = [[0 for i in range(len(mat[0]))] for j in range(len(mat))]\n",
        "\n",
        "    for i, elem in enumerate(mat):\n",
        "        for j, case in enumerate(elem):\n",
        "            if i == j and hollow: \n",
        "                inv_data[i][j] = 0\n",
        "            else:\n",
        "                inv_data[i][j] = func(case)\n",
        "    \n",
        "    return inv_data\n",
        "\n",
        "def print_mat(mat : List[List[float]], format_function : Callable[[float], str]=lambda x: x) -> None:\n",
        "    \"\"\" print a matrice on stdout\n",
        "\n",
        "    format each number in the matrice using the format_function\n",
        "    Args:\n",
        "        mat (List[List[float]]) : a matrix of number\n",
        "        format_function (Callable[[float], str]) : a simple format function to display numbers froms the matrix\n",
        "    \"\"\"\n",
        "    for line in mat:\n",
        "        for case in line:\n",
        "            print(f\"{format_function(case):8}\", end=\"\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "COZfnM3QNwUc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Strategy ✅\n",
        "\n"
      ],
      "metadata": {
        "id": "-Wd47Q50gPSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTMergeStrategy:\n",
        "    \"\"\" strategy to extract BERT embeddings\n",
        "    \n",
        "    different approach exist, \n",
        "    see https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/BERT/bert-feature-extraction-contextualized-embeddings.png\n",
        "    for more possible strategy\n",
        "    \"\"\"\n",
        "\n",
        "    def merge(self, vector : List[List[float]]) -> List[float]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "class Sum4LastLayers(BERTMergeStrategy):\n",
        "\n",
        "    def merge(self, vector : List[List[float]]) -> List[float]:\n",
        "        return torch.sum(vector[-4:], dim = 0)\n",
        "\n",
        "class Concat4LastLayer(BERTMergeStrategy):\n",
        "\n",
        "    def merge(self, vector : List[List[float]]) -> List[float]:\n",
        "        return torch.concat(vector[-4:], dim = 0)\n",
        "\n",
        "class Similarity:\n",
        "    \"\"\"strategy to compute similarity between embeddings. Cosine similarity should \n",
        "    be the only valid one in word embeddings, other aren't relevant\n",
        "    \"\"\"\n",
        "\n",
        "    def sim(self, embed1 : List[float], embed2 : List[float]) -> float:\n",
        "        raise NotImplementedError\n",
        "\n",
        "class CosineSim(Similarity):\n",
        "\n",
        "    def sim(self, embed1 : List[float], embed2 : List[float]) -> float:\n",
        "        cos = torch.nn.CosineSimilarity(dim=0)\n",
        "        return cos(embed1, embed2)\n",
        "\n",
        "class EuclidianDistSim(Similarity):\n",
        "\n",
        "    def sim(self, embed1 : List[float], embed2 : List[float]) -> float:\n",
        "        return np.linalg.norm(embed1-embed2)\n",
        "\n",
        "class ManhattanDistSim(Similarity):\n",
        "\n",
        "    def sim(self, embed1 : List[float], embed2 : List[float]) -> float:\n",
        "        return cityblock(embed1, embed2)"
      ],
      "metadata": {
        "id": "LC8LdC2ngRtx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Article"
      ],
      "metadata": {
        "id": "7G26rtCEfQny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class customWikiArticle:\n",
        "\n",
        "    def __init__(self, index : int, title : str, realtitle : str, summary : str, ambiguous : bool):\n",
        "        self.index = index\n",
        "        self.title = title\n",
        "        self.realtitle = realtitle\n",
        "        self.summary = summary\n",
        "        self.ambiguous = ambiguous\n",
        "\n",
        "class ArticleRetriever:\n",
        "\n",
        "    article_dir = \"./article\"\n",
        "\n",
        "    def __init__(self, name : str, list_title : List[str]):\n",
        "\n",
        "        self.name = name\n",
        "        self.list_title = list_title\n",
        "        self.modified = False\n",
        "\n",
        "        if not exists(self.get_filename()):\n",
        "            self.articles_map = {}\n",
        "        else:\n",
        "            with open(self.get_filename(), \"rb\") as mapfile:\n",
        "                self.articles_map = pickle.load(mapfile)\n",
        "\n",
        "    def get_filename(self):\n",
        "        return join(ArticleRetriever.article_dir,self.name)\n",
        "\n",
        "    def load_article(self, title, force_reload : bool = False) -> customWikiArticle:\n",
        "        if title not in self.articles_map:\n",
        "            self.modified = True\n",
        "            realtitle, summary, ambiguous = self._retrieve_article(title)\n",
        "            self.articles_map[title] = customWikiArticle(len(self.articles_map), title, realtitle, summary, ambiguous)\n",
        "\n",
        "        if title in self.articles_map and self.articles_map[title].summary == None and force_reload:\n",
        "            self.modified = True\n",
        "            realtitle, summary, ambiguous = self._retrieve_article(title)\n",
        "            self.articles_map[title].summary = summary\n",
        "\n",
        "        return self.articles_map[title]\n",
        "\n",
        "    def _retrieve_article(self, title, closed_list : List = []) -> Tuple[str, str, bool]: \n",
        "        closed_list.append(title)\n",
        "        try:\n",
        "            article = wikipedia.page(title, auto_suggest=False, redirect=True)\n",
        "            return (article.title, article.summary, False)\n",
        "\n",
        "        except wikipedia.PageError as e:\n",
        "            search_result = wikipedia.search(title, suggestion = False)\n",
        "\n",
        "            logging.warning(f\"{title} misspelled or article missing. Best find is {search_result[0]}\")\n",
        "            if search_result[0] is not None and search_result[0] not in closed_list:            \n",
        "                return self._retrieve_article(search_result[0], closed_list)  \n",
        "            else: return (title, None, False)\n",
        "\n",
        "        except wikipedia.DisambiguationError as e:\n",
        "            logging.warning(f\"{title} is ambiguous, trying first {e.options[0]}\")\n",
        "            if e.options[0] is not None and e.options[0] not in closed_list:\n",
        "                res = self._retrieve_article(e.options[0], closed_list)\n",
        "                return (res[0], res[1], True)\n",
        "        \n",
        "            return (None, None, None)\n",
        "\n",
        "\n",
        "    def load_all_articles(self, force_reload : bool = False) -> None:\n",
        "        logging.info(f\"Starting loading articles...\")\n",
        "        nb_success = 0\n",
        "\n",
        "        nb_article = len(self.list_title)\n",
        "        for i, title in tqdm(enumerate(self.list_title), total=nb_article):\n",
        "            self.load_article(title, force_reload)\n",
        "\n",
        "            if self.articles_map[title].summary is not None: \n",
        "                nb_success += 1\n",
        "        logging.info(f\"Finished loading {nb_success} article(s) !\")\n",
        "        return self.modified\n",
        "\n",
        "    def __call__(self, force_reload : bool = True) -> None:\n",
        "        return self.load_all_articles(force_reload)\n",
        "\n",
        "    def get_article(self, title):\n",
        "        if title not in self.articles_map:\n",
        "            self.load_article(title)\n",
        "\n",
        "        return self.articles_map[title]\n",
        "        \n",
        "    def save(self):\n",
        "        with open(self.get_filename(), \"wb\") as mapfile:\n",
        "            pickle.dump(self.articles_map, mapfile)\n",
        "\n",
        "class ArticleViewer():\n",
        "\n",
        "    def __init__(self, filename):\n",
        "        self.name = filename\n",
        "\n",
        "        if not exists(self.name):\n",
        "            raise FileNotFoundError()\n",
        "        else:\n",
        "            with open(self.name, \"rb\") as mapfile:\n",
        "                self.articles_map = pickle.load(mapfile)\n",
        "\n",
        "    def get(self, title):\n",
        "        return self.articles_map[title]\n",
        "\n",
        "    def get_all_articles(self):\n",
        "        return self.articles_map.keys()\n"
      ],
      "metadata": {
        "id": "QZiZ2Pr-lUiS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings operations "
      ],
      "metadata": {
        "id": "bw3z2xRJfeYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WordToVector:\n",
        "\n",
        "    def __init__(self, list_tags : List[str] = []):\n",
        "        self.list_tags = list_tags\n",
        "        self.embeddings = {}\n",
        "\n",
        "    def convert(self):\n",
        "        return NotImplementedError\n",
        "\n",
        "    def reset_embeddings(self):\n",
        "        self.embeddings.clear()\n",
        "\n",
        "    def get_embedding_of(self, token):\n",
        "        if token not in self.embeddings:\n",
        "            raise Exception(f\"no such token {token}\")\n",
        "        \n",
        "        return self.embeddings[token]\n",
        "\n",
        "    def get_class_list(self):\n",
        "        return self.embeddings.keys()\n",
        "\n",
        "    def ask_alt(self, word):\n",
        "        print(f\"Unable to find {word} in the dict, choose a replacement : \")\n",
        "        alt = str(input())\n",
        "        return alt\n",
        "\n",
        "    def export(self, filename):\n",
        "        \"\"\"export all the embeddings in filename under a .csv format.\n",
        "           Raise exception if embeddings hasn't been calculed yet.\"\"\"\n",
        "\n",
        "        if len(self.embeddings) == 0:\n",
        "            raise Exception(\"Tags not converted yet !\")\n",
        "        \n",
        "        dict2csv(filename, self.embeddings)\n",
        "\n",
        "class EmbeddingsLoader:\n",
        "\n",
        "    def __init__(self, filename : str):\n",
        "\n",
        "        self.file = filename\n",
        "        self.embeddings = {}\n",
        "\n",
        "        self._load_file()\n",
        "\n",
        "    def _load_file(self):\n",
        "        try:\n",
        "            with open(self.file, \"r\") as f:\n",
        "                lines = f.readlines()\n",
        "                \n",
        "            for line in lines[1:]:\n",
        "                data = line.split(\",\")\n",
        "                self.embeddings[data[0]] = torch.FloatTensor(list(map(float, data[1:])))\n",
        "\n",
        "        except IOError as e:\n",
        "            raise IOError(f\"No file {self.file}\")\n",
        "\n",
        "class SimilarityMatrix(EmbeddingsLoader):\n",
        "\n",
        "    def __init__(self, embeddings : Dict[str, List[float]], strategy : Similarity):\n",
        "        super(SimilarityMatrix, self).__init__(embeddings)\n",
        "        self.strategy = strategy\n",
        "        self._create_matrix()\n",
        "        self.computed : bool = False\n",
        "\n",
        "    def _create_matrix(self) -> None:\n",
        "        n_tokens = len(self.embeddings)\n",
        "        self.cosine_sim_matrix : Dict[Dict[float]] = {}\n",
        "        for tag in self.embeddings.keys():\n",
        "            self.cosine_sim_matrix[tag] = {}\n",
        "\n",
        "    def compute_sim(self) -> None:\n",
        "        \"\"\" compute cosine similarity between all vectors \"\"\"\n",
        "\n",
        "        logging.info(\"Computing cosine similarity, this could take some time...\")\n",
        "        for tag, vector in tqdm(self.embeddings.items(), total = len(self.embeddings)):\n",
        "\n",
        "            for otag, other_vector in self.embeddings.items():\n",
        "\n",
        "                if otag == tag:\n",
        "                    continue\n",
        "\n",
        "                similarity = self.strategy.sim(vector, other_vector)\n",
        "\n",
        "                self.cosine_sim_matrix[otag][tag] = similarity\n",
        "                self.cosine_sim_matrix[tag][otag] = similarity\n",
        "\n",
        "        self.computed = True\n",
        "\n",
        "    def export_sim_matrix(self, filename):\n",
        "        if not self.computed:\n",
        "            self.compute_sim()\n",
        "        \n",
        "        try:\n",
        "            f = open(filename, \"w\")\n",
        "        except OSError:\n",
        "            raise OSError(\"Could not open file\")\n",
        "\n",
        "        with f:\n",
        "            print(\"/\", *[tag for tag in self.embeddings.keys()], sep = \",\", file = f)\n",
        "\n",
        "            for tag in self.embeddings.keys():\n",
        "                print(tag, *[str(round(float(self.cosine_sim_matrix[tag][otag]), 3)) for otag in self.embeddings.keys()], sep = \",\", file = f)\n",
        "\n",
        "    def get_sim_matrix(self) -> Tuple[List[str], List[List[float]]]:\n",
        "        \"\"\"return the similarity matrix of the embeddings\n",
        "        \"\"\"\n",
        "        if not self.computed:\n",
        "            self.compute_sim()\n",
        "\n",
        "        X = len(self.embeddings)\n",
        "        matrix = [[0 for j in range(X)] for i in range(X)]\n",
        "        ids = []\n",
        "        \n",
        "        for i, tag in enumerate(self.embeddings.keys()):\n",
        "            ids.append(tag)\n",
        "            for j, otag in enumerate(self.embeddings.keys()):\n",
        "                if i == j:\n",
        "                    continue\n",
        "\n",
        "                matrix[i][j] = self.cosine_sim_matrix[tag][otag]\n",
        "                matrix[j][i] = self.cosine_sim_matrix[tag][otag]\n",
        "\n",
        "        return ids, matrix\n",
        "\n",
        "    def sim_between(self, token1 : str, token2 : str) -> float:\n",
        "        v1 = self.embeddings[token1]\n",
        "        v2 = self.embeddings[token2]\n",
        "\n",
        "        if token2 not in self.cosine_sim_matrix[token1] or token1 not in self.cosine_sim_matrix[token2]:\n",
        "            similarity = self.strategy.sim(v1, v2)\n",
        "            self.computed = True\n",
        "\n",
        "            self.cosine_sim_matrix[token1][token2] = similarity\n",
        "            self.cosine_sim_matrix[token1][token2] = similarity\n",
        "\n",
        "        return self.cosine_sim_matrix[token1][token2]\n",
        "\n",
        "class SimilarityMatrixFromDict(SimilarityMatrix):\n",
        "\n",
        "    def __init__(self, embeddings : Dict[str, List[float]], strategy : Similarity):\n",
        "        self.embeddings = embeddings\n",
        "        self._create_matrix()\n",
        "        self.strategy = strategy\n",
        "        self.computed : bool = False"
      ],
      "metadata": {
        "id": "5AxDIhr1rkca"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solver"
      ],
      "metadata": {
        "id": "0bRoDbKafoVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Solver(EmbeddingsLoader):\n",
        "\n",
        "    DEFAULT_MIN_LIST_RESULT = 10\n",
        "\n",
        "    def __init__(self, embeddings):\n",
        "        super(Solver, self).__init__(embeddings)\n",
        "\n",
        "    def get_nearest_embedding_of(self, embedding, nb = 10):\n",
        "\n",
        "        if nb > len(self.embeddings):\n",
        "            raise Exception(\"nb too high, not enough token\")\n",
        "\n",
        "        nearest = []\n",
        "        for tag, e in self.embeddings.items():\n",
        "\n",
        "            cos = torch.nn.CosineSimilarity(dim=0)\n",
        "            similarity = cos(embedding, e)\n",
        "\n",
        "            nearest.append((tag, similarity))\n",
        "        \n",
        "        nearest.sort(key = lambda tup : tup[1])\n",
        "        return nearest[-1:-nb-1:-1]\n",
        "\n",
        "    def __call__(self, embeddeding, tag=None):\n",
        "        result = self.get_nearest_embedding_of(embeddeding, min(Solver.DEFAULT_MIN_LIST_RESULT, len(self.embeddings)))\n",
        "        if tag is not None:\n",
        "            print(f\"Nearest Word for {tag}:\")\n",
        "        for i in result:\n",
        "            print(f\"\\t{i[0]:12}: {round(float(i[1]) * 100, 3)}%\")\n",
        "    \n",
        "    def score(self, embedding, target):\n",
        "        target_embeddings = self.embeddings[target]\n",
        "\n",
        "        cos = torch.nn.CosineSimilarity(dim=0)\n",
        "        return float(cos(embedding, target_embeddings))\n",
        "\n",
        "    def least_squared_score(self, embedding, target):\n",
        "        target_embeddings = self.embeddings[target]\n",
        "        return float(np.linalg.norm(target_embeddings - embedding))\n",
        "\n",
        "    def mean_squared_score(self, embedding, target):\n",
        "        target_embeddings = self.embeddings[target]\n",
        "        return float(np.square(np.subtract(embedding, target_embeddings)).mean())\n",
        "        "
      ],
      "metadata": {
        "id": "KpVPJ9IxfnC5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "nYaqmgXe1-EX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT model"
      ],
      "metadata": {
        "id": "-0g4CJtelYfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTModel(WordToVector):\n",
        "\n",
        "    temp_dir = \"./temp\"\n",
        "\n",
        "    def __init__(self, list_tag : List[str], big: bool = False, window : int = 100):\n",
        "        super(BERTModel, self).__init__(list_tag)\n",
        "        self.window_size = window\n",
        "\n",
        "        self.model_size = \"bert-large-uncased\" if big else \"bert-base-uncased\"\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.model_size, padding=True, truncation=True)\n",
        "        self.model = BertModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "    def convert(self, article_ret : ArticleRetriever):\n",
        "        \"\"\" convert all word in their embeddings\"\"\"\n",
        "\n",
        "        if len(self.list_tags) == 0:\n",
        "            raise Exception(\"no tags yet !\")\n",
        "\n",
        "        logging.info(\"Starting converting tokens...\")\n",
        "        nb_token = len(self.list_tags)\n",
        "        for i, tag in tqdm(enumerate(self.list_tags), total = nb_token):\n",
        "            \n",
        "            if tag in self.embeddings: continue\n",
        "\n",
        "            article = article_ret.get_article(tag)\n",
        "            tag_plus_context = tag\n",
        "            if article.summary is not None:\n",
        "                max_size_article = min(len(article.summary), self.window_size)\n",
        "                tag_plus_context = tag + \". \" + article.summary[:max_size_article]\n",
        "\n",
        "            inputs = self.tokenizer(tag_plus_context, return_tensors = \"pt\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            hidden_states = outputs[2]\n",
        "\n",
        "            # [# layers, # batches, # tokens, # features] ==> [# tokens, # layers, # features]\n",
        "            token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "            token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "            token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "            self.embeddings[tag] = self.merging_strategy.merge(token_embeddings[0])"
      ],
      "metadata": {
        "id": "UVJUSGWQorYj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RoBERTa model"
      ],
      "metadata": {
        "id": "eGVvWFvtlqLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ROBERTAModel(BERTModel):\n",
        "\n",
        "    def __init__(self, list_tag : List[str], big: bool = False, window : int = 100):\n",
        "        WordToVector.__init__(self, list_tag)\n",
        "        self.window_size = window\n",
        "\n",
        "        self.model_size = \"roberta-large\" if big else \"roberta-base\"\n",
        "\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_size, padding=True, truncation=True)\n",
        "        self.model = RobertaModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()"
      ],
      "metadata": {
        "id": "7oWZOwG-mCYI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DocBERT model"
      ],
      "metadata": {
        "id": "GqQ1HgNyz-zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DocBERT(BERTModel):\n",
        "\n",
        "    def __init__(self, list_tag : List[str], big : bool = False):\n",
        "        WordToVector.__init__(self, list_tag)\n",
        "        self.window_size = \"document\"\n",
        "\n",
        "        self.model_size = \"bert-large-uncased\" if big else \"bert-base-uncased\"\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.model_size, padding=True, truncation=True)\n",
        "        self.model = BertModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.max_size = 300\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "    def _one_pass(self, subinputs):\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(subinputs[\"input_ids\"], attention_mask=subinputs[\"attention_mask\"])\n",
        "\n",
        "        hidden_states = outputs[2]\n",
        "\n",
        "        # [# layers, # batches, # tokens, # features] ==> [# tokens, # layers, # features]\n",
        "        token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "        token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "        return self.merging_strategy.merge(token_embeddings[0])\n",
        "\n",
        "\n",
        "    def convert(self, article_ret):\n",
        "        \"\"\" convert all word in their embeddings\"\"\"\n",
        "\n",
        "        if len(self.list_tags) == 0:\n",
        "            raise Exception(\"no tags yet !\")\n",
        "\n",
        "        logging.info(\"Starting converting tokens...\")\n",
        "        nb_token = len(self.list_tags)\n",
        "        for i, tag in tqdm(enumerate(self.list_tags), total = nb_token):\n",
        "            \n",
        "            if tag in self.embeddings: continue\n",
        "\n",
        "            article = article_ret.get_article(tag)\n",
        "            tag_plus_context = tag\n",
        "            if article.summary is not None:\n",
        "\n",
        "                torch_cls = []\n",
        "\n",
        "                ids = self.tokenizer.encode(article.summary)\n",
        "                nb_token = len(ids)\n",
        "                nb_pass = nb_token // self.max_size\n",
        "                \n",
        "                nb_pass = max(1, nb_token // self.max_size)\n",
        "                logging.info(f\"{tag} is {nb_pass} pass\")\n",
        "\n",
        "                for i in range(nb_pass):\n",
        "                    start = i * self.max_size\n",
        "                    stop = start + self.max_size\n",
        "                    sub_ids = ids[start:min(nb_token, stop)]\n",
        "                    print(sub_ids)\n",
        "\n",
        "                    subinputs = {\"input_ids\": torch.FloatTensor(sub_ids), \\\n",
        "                                 \"token_type_ids\": torch.FloatTensor([0 for i in range(len(sub_ids))]), \\\n",
        "                                 \"attention_mask\": torch.FloatTensor([1 for i in range(len(sub_ids))])  }\n",
        "                    torch_cls.append(self._one_pass(subinputs))\n",
        "\n",
        "                self.embeddings[tag] = torch.mean(torch.stack(tuple(t for t in torch_cls)), axis=0)\n",
        "            else:\n",
        "                logging.warning(f\"no article for {tag}\")\n",
        "                self.embeddings[tag] = self._one_pass(tag)\n",
        "\n"
      ],
      "metadata": {
        "id": "nnq0gSYxiIQd"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DocBERTA model"
      ],
      "metadata": {
        "id": "mRkUxi2g0FT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DocBERTA(DocBERT):\n",
        "\n",
        "    def __init__(self, list_tag : List[str], big : bool = False):\n",
        "        WordToVector.__init__(self, list_tag)\n",
        "        self.window_size = \"document\"\n",
        "\n",
        "        self.model_size = \"roberta-large\" if big else \"roberta-base\"\n",
        "\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_size, padding=True, truncation=True)\n",
        "        self.model = RobertaModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.max_size = self.tokenizer.model_max_length\n",
        "\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()"
      ],
      "metadata": {
        "id": "FHdFrESi0Joa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wikipedia2Vec"
      ],
      "metadata": {
        "id": "mFf6OoBG1xVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Wiki2VecModel(WordToVector):\n",
        "\n",
        "    file = []\n",
        "\n",
        "    def __init__(self, list_tag : List[str], size : int = 300):\n",
        "        WordToVector.__init__(self, list_tag)\n",
        "        self.window_size = size\n",
        "\n",
        "        self.model_size = \"wikipedia2vec\"\n",
        "        assert size in [100, 300, 500], f\"size should be one of this value (100, 300, 500)\"\n",
        "        self.zip_filename = f\"enwiki_20180420_{self.window_size}d.pkl.bz2\"\n",
        "        self.unzip_filename = self.zip_filename[:-4]\n",
        "        \n",
        "        try:\n",
        "            self._load()\n",
        "        except:\n",
        "            return\n",
        "        \n",
        "        self.model = Wikipedia2Vec.load(self.path)\n",
        "\n",
        "    def _load(self):\n",
        "\n",
        "        self.path = abspath(self.unzip_filename)\n",
        "\n",
        "        if exists(self.path):\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            system(f\"wget http://wikipedia2vec.s3.amazonaws.com/models/en/2018-04-20/{self.zip_filename}\")\n",
        "        except:\n",
        "            raise SystemError(f\"can't retrieve the file {self.zip_filename}\")\n",
        "\n",
        "        try:\n",
        "            system(f\"bunzip2 ./{self.zip_filename}\")\n",
        "        except:\n",
        "            raise SystemError(f\"can't unzip the file {self.zip_filename}\")\n",
        "\n",
        "    def _retrieve(self, word):\n",
        "        try:\n",
        "            return self.model.get_word_vector(word)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            return self.model.get_word_vector(word.capitalize())\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            return self.model.get_entity_vector(word)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            return self.model.get_entity_vector(word.capitalize())\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        return None\n",
        "    \n",
        "    def _one_turn(self, resolve_dict = {}):\n",
        "        unk = []\n",
        "\n",
        "        for word in vocab:\n",
        "            w = word.replace(\"_\", \" \")\n",
        "            if w in self.embeddings: continue\n",
        "\n",
        "            if w in resolve_dict:\n",
        "                embed = self._retrieve(resolve_dict[w])\n",
        "            else:\n",
        "                embed = self._retrieve(w)\n",
        "\n",
        "            if embed is None:\n",
        "                logging.warning(f\"{w} cannot be retrieved.\")\n",
        "                unk.append(word)\n",
        "            else:\n",
        "                self.embeddings[w] = torch.from_numpy(embed)\n",
        "        \n",
        "        return unk\n",
        "\n",
        "    def convert(self, ar):\n",
        "        resolve_filename = f\"./temp/{ar.name[:-4]}_resolve.json\"\n",
        "        resolve = {}\n",
        "        while True: \n",
        "            unk_list = self._one_turn(resolve)\n",
        "        \n",
        "            if unk_list is None or len(unk_list) == 0:\n",
        "                break\n",
        "\n",
        "            print(len(unk_list), \"items haven't been found, resolve mode.\")\n",
        "\n",
        "            with open(resolve_filename, 'w') as f:\n",
        "                resolve_dict = {word: \"\" for word in unk_list}\n",
        "                json.dump(resolve_dict, f)\n",
        "\n",
        "            input(\"press enter to resume resolve\")\n",
        "\n",
        "            resolve = {}\n",
        "            with open(resolve_filename, 'r') as f:\n",
        "                resolve = json.load(f)\n",
        "                assert(type(resolve) == type(dict()))"
      ],
      "metadata": {
        "id": "w0zukX6Zk3W5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "44KuFxQOG0WX",
        "outputId": "66db86d8-e2fa-4b53-b081-5c5a740658f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical part"
      ],
      "metadata": {
        "id": "ue8SGNE5iKaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings to word proba"
      ],
      "metadata": {
        "id": "JNFmhVBmUTgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "solver = Solver(\"/content/animal10-roberta-base-0.csv\")\n",
        "\n",
        "totest = solver.embeddings[\"cat\"]\n",
        "\n",
        "solver(totest, \"cat\")\n",
        "print(solver.score(totest, \"dog\"))\n",
        "print(solver.mean_squared_score(totest, \"dog\"))\n",
        "print(solver.least_squared_score(totest, \"dog\"))"
      ],
      "metadata": {
        "id": "tD8XokPDUhkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word to embeddings"
      ],
      "metadata": {
        "id": "LN273RlCUGza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
        "imagenet = list(np.array(open(labels_path).read().splitlines()))\n",
        "subimagenet = imagenet[:200]"
      ],
      "metadata": {
        "id": "NIJ8zsk-UTuh",
        "outputId": "41de02e5-050e-4181-e181-7957c15e8744",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt\n",
            "16384/10484 [==============================================] - 0s 0us/step\n",
            "24576/10484 [======================================================================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "animal10 = [\"dog\", \"cat\", \"horse\", \"spider\", \"butterfly\", \"chicken\", \"sheep\", \"cow\", \"squirrel\", \"elephant\"]\n",
        "cifar10  = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "cifar100 = [\"apple\", \"aquarium_fish\", \"baby\", \"bear\", \"beaver\", \"bed\", \"bee\", \"beetle\", \"bicycle\", \"bottle\", \"bowl\", \"boy\", \"bridge\", \"bus\", \"butterfly\", \"camel\", \"can\", \\\n",
        "            \"castle\", \"caterpillar\", \"cattle\", \"chair\", \"chimpanzee\", \"clock\", \"cloud\", \"cockroach\", \"computer_keyboard\", \"couch\", \"crab\", \"crocodile\", \"cup\", \\\n",
        "            \"dinosaur\", 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'lamp', 'lawn_mower', 'leopard', 'lion', \\\n",
        "            'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', \\\n",
        "            'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark',\\\n",
        "            'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', \\\n",
        "            'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n",
        "\n",
        "king = [\"king\", \"woman\", \"man\", \"queen\", \"boy\", \"girl\", \"male\", \"female\"]\n",
        "custom = []\n",
        "\n",
        "#@title Choose a dataset\n",
        "save_name = \"cifar100\" #@param [\"animal10\", \"cifar10\", \"cifar100\", \"king\", \"imagenet\", \"subimagenet\", \"custom\"]\n",
        "mapping_save_list = {\n",
        "    \"animal10\": animal10,\n",
        "    \"cifar10\" : cifar10,\n",
        "    \"cifar100\" : cifar100,\n",
        "    \"king\" : king,\n",
        "    \"imagenet\" : imagenet,\n",
        "    \"subimagenet\": subimagenet,\n",
        "    \"custom\" : custom\n",
        "}\n",
        "\n",
        "vocab = mapping_save_list[save_name]"
      ],
      "metadata": {
        "id": "EYnbJc84xuh5",
        "cellView": "form"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose a model and params\n",
        "window_size  = 300 #@param [\"0\", \"100\", \"200\", \"300\", \"400\", \"500\"] {type:\"raw\"}\n",
        "is_big       = True #@param {type:\"boolean\"}\n",
        "model_choice = \"Wikipedia2Vec\" #@param [\"ROBERTA\", \"BERT\", \"Wikipedia2Vec\", \"DocBERT\", \"DocBERTA\"]\n",
        "\n",
        "if model_choice == \"ROBERT\":\n",
        "    model = ROBERTAModel(vocab, big = is_big, window = window_size)\n",
        "elif model_choice == \"BERT\":\n",
        "    model = BERTModel(vocab, big = is_big, window = window_size)\n",
        "elif model_choice == \"Wikipedia2Vec\":\n",
        "    model = Wiki2VecModel(vocab, size = window_size)\n",
        "elif model_choice == \"DocBERT\":\n",
        "    model = DocBERT(vocab, big = is_big)\n",
        "elif model_choice == \"DocBERTA\":\n",
        "    model = DocBERTA(vocab, big = is_big)\n",
        "    \n",
        "articlesRetriever = ArticleRetriever(save_name + \".art\", vocab)"
      ],
      "metadata": {
        "id": "NvIn_JeFlYfK",
        "cellView": "form"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if articlesRetriever():\n",
        "    articlesRetriever.save()\n",
        "\n",
        "model.reset_embeddings()\n",
        "\n",
        "model.convert(articlesRetriever)\n",
        "\n",
        "csv_file = f\"{save_name}-{model.model_size}-{model.window_size}.csv\"\n",
        "\n",
        "model.export(csv_file)\n",
        "\n",
        "print(\"\\n\", len(model.get_class_list()))"
      ],
      "metadata": {
        "id": "WOOlgG8koVD6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d274dbfd-a9ff-44bb-9c3e-e0b4156bfd58"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 201069.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wiki2vec_dict = model.model.dictionary\n",
        "\n",
        "# with open(\"./temp/wiki_article.txt\", \"w\") as f:\n",
        "#     for word in wiki2vec_dict.words():\n",
        "#         print(word.text, file=f)\n",
        "\n",
        "#     for ent in wiki2vec_dict.entities():\n",
        "#         print(ent.title, file=f)"
      ],
      "metadata": {
        "id": "z0qnv2UbfqZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neighboor joining Tree"
      ],
      "metadata": {
        "id": "0WZFTDaGbr1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neighboor_sim = SimilarityMatrix(csv_file, CosineSim())\n",
        "\n",
        "ids, data = neighboor_sim.get_sim_matrix()\n",
        "\n",
        "ids = [tids.replace(\" \", \"_\") for tids in ids]\n",
        "inv_data  = sim2dist(data, lambda x: 1 - x) \n",
        "\n",
        "# print()\n",
        "# print_mat(data_inv, format_function = lambda x: round(float(x), 2))"
      ],
      "metadata": {
        "id": "crcMR8dDuH7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f180c39-e898-462a-c85b-03f2b98547bf"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 122.41it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dm = DistanceMatrix(inv_data, ids)\n",
        "tree = nj(dm)\n",
        "\n",
        "with open(f\"{save_name}-{model_choice}-.tree\", \"w\") as f:\n",
        "    print(tree, file = f)\n",
        "\n",
        "print(\"\\n\\n\", tree.ascii_art())"
      ],
      "metadata": {
        "id": "_J431TiosB8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## similarity test"
      ],
      "metadata": {
        "id": "Eu9U-_PfeXSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"/content/cifar100-bert-base-uncased-document.csv\"\n",
        "\n",
        "\n",
        "cosine_sim = SimilarityMatrix(filename, CosineSim())\n",
        "euclidian_dist_sim = SimilarityMatrix(filename, EuclidianDistSim())\n",
        "manhattan_dist_sim = SimilarityMatrix(filename, ManhattanDistSim())\n",
        "\n",
        "ids, cosine_mat = cosine_sim.get_sim_matrix()\n",
        "_  , euclid_mat = euclidian_dist_sim.get_sim_matrix()\n",
        "_  , manhat_mat = manhattan_dist_sim.get_sim_matrix()"
      ],
      "metadata": {
        "id": "4JUE701weWzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69b86455-a6e7-4f9c-c943-47a655c8b76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 172.79it/s]\n",
            "100%|██████████| 100/100 [00:00<00:00, 605.13it/s]\n",
            "100%|██████████| 100/100 [00:00<00:00, 668.22it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(max(list(map(max, euclid_mat))))\n",
        "print(min(list(map(min, euclid_mat))))\n",
        "\n",
        "print(max(list(map(max, manhat_mat))))\n",
        "print(min(list(map(min, manhat_mat))))"
      ],
      "metadata": {
        "id": "5oMgdFgEC_hj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99e31f49-5952-45f2-ca7b-b77b11da7c60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45.33593\n",
            "0\n",
            "1004.4331\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim_list = []\n",
        "euclid_sim_list = []\n",
        "manhat_sim_list = []\n",
        "\n",
        "closed_list = []\n",
        "\n",
        "# cos_thresold = 0.5\n",
        "# euc_thresold = 5\n",
        "# man_thresold = 60\n",
        "\n",
        "for i, idsa in tqdm(enumerate(ids)):\n",
        "    for j, idsb in enumerate(ids):\n",
        "\n",
        "        if i == j: continue \n",
        "        if (i, j) in closed_list or (j, i) in closed_list: continue\n",
        "\n",
        "        cos_val = cosine_mat[i][j]\n",
        "        euc_val = euclid_mat[i][j]\n",
        "        man_val = manhat_mat[i][j]\n",
        "\n",
        "        # if cos_val >= cos_thresold:\n",
        "        cosine_sim_list.append((idsa, idsb, cos_val))\n",
        "        \n",
        "        # if euc_val <= euc_thresold:\n",
        "        euclid_sim_list.append((idsa, idsb, euc_val))\n",
        "\n",
        "        # if man_val <= man_thresold:\n",
        "        manhat_sim_list.append((idsa, idsb, man_val))\n",
        "\n",
        "        closed_list.append((i, j))\n",
        "        closed_list.append((j, i))\n",
        "\n",
        "cosine_sim_list.sort(key=lambda x: x[2], reverse = True)\n",
        "euclid_sim_list.sort(key=lambda x: x[2], reverse = False)\n",
        "manhat_sim_list.sort(key=lambda x: x[2], reverse = False)\n",
        "\n",
        "def print_list(listed):\n",
        "    print(f\"{'word':20}{'target':20}{'similarity':10}\")\n",
        "    print(\"=\"*50)\n",
        "    for ida, idb, sim in listed:\n",
        "        print(f\"{ida:20}{idb:20}{round(float(sim), 6):6}\")\n",
        "\n",
        "print(\" Cosine similarity \")\n",
        "print_list(cosine_sim_list[0:40])\n",
        "print()\n",
        "\n",
        "# print(\" Euclidian distance \")\n",
        "# print_list(euclid_sim_list[0:10])\n",
        "# print()\n",
        "\n",
        "# print(\" Manhattan distance \")\n",
        "# print_list(manhat_sim_list[0:10])\n",
        "# print()\n"
      ],
      "metadata": {
        "id": "Kiq-UPEApL0U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dc39b50-1eb0-4f03-eb99-fb43b624091e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100it [00:02, 39.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Cosine similarity \n",
            "word                target              similarity\n",
            "==================================================\n",
            "seal                snake               0.982592\n",
            "shark               snake               0.982201\n",
            "snake               spider              0.981459\n",
            "seal                spider              0.979495\n",
            "bear                seal                0.978866\n",
            "seal                shark               0.978803\n",
            "porcupine           possum              0.97794\n",
            "butterfly           spider              0.977711\n",
            "beaver              wolf                0.976449\n",
            "beetle              spider              0.976221\n",
            "dolphin             whale               0.975683\n",
            "beetle              snake               0.975592\n",
            "dolphin             seal                0.975553\n",
            "beetle              butterfly           0.975295\n",
            "television          train               0.974783\n",
            "bee                 beetle              0.974143\n",
            "bee                 spider              0.974133\n",
            "beetle              seal                0.973985\n",
            "dolphin             elephant            0.973689\n",
            "bowl                plate               0.973083\n",
            "cockroach           snake               0.973034\n",
            "bear                snake               0.972928\n",
            "bear                wolf                0.972592\n",
            "crocodile           porcupine           0.972452\n",
            "raccoon             wolf                0.972393\n",
            "shark               turtle              0.972279\n",
            "snake               turtle              0.972119\n",
            "elephant            seal                0.972075\n",
            "shark               spider              0.971849\n",
            "telephone           television          0.971748\n",
            "chimpanzee          lion                0.97168\n",
            "lion                wolf                0.971644\n",
            "beetle              cockroach           0.971399\n",
            "beaver              kangaroo            0.971362\n",
            "bear                beaver              0.971294\n",
            "kangaroo            seal                0.971148\n",
            "bear                tiger               0.970911\n",
            "leopard             wolf                0.970904\n",
            "beaver              seal                0.970902\n",
            "kangaroo            snake               0.970683\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wikipedia debug"
      ],
      "metadata": {
        "id": "7jubA6JXUMk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Article to search for { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "totest = \"buck\" #@param {type:\"string\"}\n",
        "\n",
        "result = wikipedia.search(totest, suggestion = False)\n",
        "print(result)\n",
        "print(f\"first result is: {result[0]}\")\n",
        "\n",
        "try:\n",
        "    print(wikipedia.page(totest, auto_suggest=False, redirect=True))\n",
        "    print(wikipedia.page(result[0], auto_suggest=False, redirect=True))\n",
        "except Exception as e:\n",
        "    print(f\"best option envisaged: {e.options[0]}\")\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "PeM8XAu6h5gD",
        "outputId": "147aec66-0c3d-4062-e7f3-c46a0e508c0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Buck', 'Buck buck', 'Joe Buck', 'Young Buck', 'Buck Owens', 'Buck Rogers', 'Buck converter', 'Buck Showalter', 'Uncle Buck', 'Bucking']\n",
            "first result is: Buck\n",
            "best option envisaged: dollar\n",
            "\"buck\" may refer to: \n",
            "dollar\n",
            "List of animal names\n",
            "Derby shoes\n",
            "Buck (nickname)\n",
            "Buck Pierce\n",
            "Buck (surname)\n",
            "Buck 65\n",
            "Buck Angel\n",
            "Buck Dharma\n",
            "Buck Ellison\n",
            "Buck Henry\n",
            "Buck Jones\n",
            "Buck Owens\n",
            "Young Buck\n",
            "David Paul Grove\n",
            "Bunkhouse Buck\n",
            "Buck Quartermain\n",
            "Buck Zumhofe\n",
            "Buck Creek (disambiguation)\n",
            "Buck, Pennsylvania\n",
            "Buck Township, Hardin County, Ohio\n",
            "Buck Township, Luzerne County, Pennsylvania\n",
            "The High Chaparral\n",
            "Chicken Little\n",
            "Buck Frobisher\n",
            "Adventures of Huckleberry Finn\n",
            "Beverly Hills Teens\n",
            "Best in Show\n",
            "Buck Mulligan\n",
            "Chuck & Buck\n",
            "Buck Rogers\n",
            "Uncle Buck\n",
            "Time Squad\n",
            "Dr. Strangelove\n",
            "Cameron \"Buck\" Williams\n",
            "The Call of the Wild\n",
            "Married... with Children\n",
            "Home on the Range\n",
            "187 Ride or Die\n",
            "Wonder Pets\n",
            "Ice Age 3: Dawn of the Dinosaurs\n",
            "The Good Dinosaur\n",
            "Halo 3: ODST\n",
            "Midnight Cowboy\n",
            "USS Buck\n",
            "Buck: A Memoir\n",
            "Buck (cocktail)\n",
            "Buck (crater)\n",
            "Buck (design company)\n",
            "Buck (film)\n",
            "Buck (human resources consulting company)\n",
            "Buck Knives\n",
            "Buck (magazine)\n",
            "Buck (software)\n",
            "Buck (video game)\n",
            "brony\n",
            "krump\n",
            "Buck converter\n",
            "Bucking\n",
            "Bucks (disambiguation)\n",
            "Bucky (disambiguation)\n",
            "Log bucking\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articleviewer = ArticleViewer(\"/content/article/cifar100.art\")\n",
        "print(articleviewer.get_all_articles())\n",
        "articleviewer.get(\"king\").summary"
      ],
      "metadata": {
        "id": "O5sAnE0nuhD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test part"
      ],
      "metadata": {
        "id": "ax8Aw_qVRxer"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Different result for King - men + women equation with different context window size"
      ],
      "metadata": {
        "id": "uUU3kl9sbmg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "\n",
        "save_name = \"king-test\"\n",
        "vocab = [\"king\", \"queen\", \"men\", \"woman\"]\n",
        "\n",
        "# king_test_model = ROBERTAModel(vocab, big = False, window = 0)\n",
        "try:\n",
        "    king_test_model = Wiki2VecModel(vocab)\n",
        "except:\n",
        "    print(\"haha\")\n",
        "\n",
        "king_test_articlesRetriever = ArticleRetriever(save_name + \".art\", vocab)\n",
        "if king_test_articlesRetriever():\n",
        "    king_test_articlesRetriever.save()\n",
        "\n",
        "result = king_test_model.convert(king_test_articlesRetriever)\n",
        "\n",
        "if result is not None and len(result) > 0:\n",
        "    print(result)\n",
        "king_test_model.export(save_name + \".csv\")\n",
        "king_test_solver = Solver(save_name + \".csv\")\n",
        "\n",
        "men = king_test_model.get_embedding_of(\"men\")\n",
        "woman = king_test_model.get_embedding_of(\"woman\")\n",
        "king = king_test_model.get_embedding_of(\"king\")\n",
        "\n",
        "print(type(men), type(woman), type(king))\n",
        "#totest = king - men + woman\n",
        "input(\"debug\")\n",
        "print()\n",
        "king_test_solver(totest, \"King - man + woman\")"
      ],
      "metadata": {
        "id": "8uNXNj_wCWhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| model | window size | rank of Queen | distance with first |\n",
        "|-------|-------------|---------------|-----------|\n",
        "| bert-large | 0  | 2 |  .0693 |\n",
        "| bert-large | 10 | 3 |  .1191 | \n",
        "| bert-large | 50 | 2 |  .1672 |\n",
        "| bert-large | 100 | 2 | .1275 |\n",
        "| bert-large | 150 | 3 | .1134 |\n",
        "| bert-large | 200 | 3 | .1923 |\n",
        "| bert-large | 300 | 3 | .0939 |\n",
        "| bert-large | 400 | 3 | .1455 |\n",
        "| roberta-large | 0 | 2 | .0001 |\n",
        "| roberta-large | 10 | 3 | .0011 |\n",
        "| roberta-large | 50 | 2 | .0029 |\n",
        "| roberta-large | 100 | 4 | .0061 |\n",
        "| roberta-large | 150 | 3 | .0023 |\n",
        "| roberta-large | 200 | 4 | .0055 |\n",
        "| roberta-large | 300 | 4 | .0127 |\n",
        "| roberta-large | 400 | 4 | .0045 |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kY8yWSmJw6Ss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pearson correlation rank with different context window size"
      ],
      "metadata": {
        "id": "0hjQnEodbcAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordsim353_vocab():\n",
        "    vocab = []\n",
        "\n",
        "    wordsim353_vocab = nlp.data.WordSim353('all')\n",
        "    for w1, w2, i in wordsim353_vocab:\n",
        "        if w1 not in vocab:\n",
        "            vocab.append(w1)\n",
        "        if w2 not in vocab:\n",
        "            vocab.append(w2)\n",
        "    \n",
        "    return wordsim353_vocab,vocab\n",
        "\n",
        "def wordsim_test(wordsim_model, wordsim_vocab, wordsim_pair):\n",
        "    save_name = \"wordsim353\"\n",
        "\n",
        "    articlesRetriever = ArticleRetriever(save_name + \".art\", vocab)\n",
        "    if articlesRetriever(force_reload = True):\n",
        "        articlesRetriever.save()\n",
        "\n",
        "    save_file = f\"{save_name}-{wordsim353_model.model_size}.csv\"\n",
        "    wordsim_model.convert(articlesRetriever)\n",
        "    wordsim_model.export(save_file)\n",
        "\n",
        "    total_comparison = 0\n",
        "    sim_list = []\n",
        "    i_list = []\n",
        "\n",
        "    sim_computer = SimilarityMatrix(save_file, CosineSim())\n",
        "\n",
        "    for w1, w2, i in wordsim_pair:\n",
        "        sim = sim_computer.sim_between(w1, w2)\n",
        "\n",
        "        sim_list.append(sim)\n",
        "        i_list.append(i)\n",
        "\n",
        "        total_comparison += 1\n",
        "\n",
        "    result = spearmanr(sim_list, i_list)\n",
        "    return (result.correlation, result.pvalue)"
      ],
      "metadata": {
        "id": "YmFXzvYYBVzf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "\n",
        "pair, vocab = get_wordsim353_vocab()\n",
        "\n",
        "\n",
        "# wordsim353_model = ROBERTAModel(vocab, big = True, window = 75)\n",
        "# wordsim353_model = BERTModel(vocab, big = True, window = 75)\n",
        "# wordsim353_model = Wiki2VecModel(vocab, size = 300)\n",
        "wordsim353_model = DocBERT(vocab, big = True)\n",
        "# wordsim353_model = DocBERTA(vocab, big = False)\n",
        "\n",
        "result = wordsim_test(wordsim353_model, vocab, pair)\n",
        "\n",
        "print(f\"{round(result[0], 4)} ({result[1]:.4E})\")\n"
      ],
      "metadata": {
        "id": "k9u-3ByDYjFl",
        "outputId": "99d4c435-80ce-43ae-8e5a-74b573394d7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 437/437 [00:00<00:00, 439441.58it/s]\n",
            "  0%|          | 0/437 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (764 > 512). Running this sequence through the model will result in indexing errors\n",
            "  0%|          | 0/437 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 12619, 19935, 2884, 14364, 19935, 2884, 10546, 16093, 2632, 1011, 24209, 2094, 4213, 2632, 1011, 16543, 2072, 1006, 1018, 1013, 2484, 2257, 4612, 1516, 2340, 2281, 2432, 1007, 1010, 16071, 2124, 2004, 8038, 18116, 19027, 27753, 1006, 12098, 2099, 1011, 1114, 1011, 6638, 1010, 2036, 2149, 1024, 12098, 1011, 1114, 1011, 6904, 11039, 1025, 5640, 1024, 1295, 29820, 22192, 15394, 1300, 25573, 29824, 17149, 1288, 29816, 15394, 1270, 23673, 17149, 29820, 22192, 15915, 1288, 29816, 15394, 1270, 23673, 17149, 29836, 29836, 29833, 1288, 17149, 29833, 25573, 29817, 1270, 23673, 29834, 15394, 29836, 19433, 1270, 23673, 29820, 29824, 14498, 15915, 14498, 1025, 5640, 1024, 1300, 25573, 29824, 17149, 1288, 17149, 29833, 25573, 29817, 1010, 7651, 1024, 8038, 29481, 1148, 5400, 27753, 1007, 2030, 2011, 2010, 28919, 3148, 8273, 2572, 7849, 1006, 5640, 1024, 1270, 29816, 29836, 1288, 22192, 25573, 17149, 1010, 7651, 1024, 1147, 7875, 2226, 1148, 3286, 7849, 1007, 1010, 2001, 1037, 9302, 2576, 3003, 1012, 2002, 2001, 3472, 1997, 1996, 8976, 7931, 3029, 1006, 20228, 2080, 1007, 2013, 3440, 2000, 2432, 1998, 2343, 1997, 1996, 9302, 2120, 3691, 1006, 1052, 2532, 1007, 2013, 2807, 2000, 2432, 1012, 17859, 2135, 2019, 5424, 8986, 1010, 2002, 2001, 1037, 4889, 2266, 1997, 1996, 6638, 4430, 2576, 2283, 1010, 2029, 2002, 2419, 2013, 3851, 2127, 2432, 1012, 19027, 27753, 2001, 2141, 2000, 9302, 3008, 1999, 11096, 1010, 5279, 1010, 2073, 2002, 2985, 2087, 1997, 2010, 3360, 1998, 3273, 2012, 1996, 2118, 1997, 2332, 11865, 4215, 1045, 1012, 2096, 1037, 3076, 1010, 2002, 14218, 5424, 8986, 1998, 3424, 1011, 21379, 4784, 1012, 4941, 2000, 1996, 3882, 4325, 1997, 1996, 2110, 1997, 3956, 1010, 2002, 4061, 4077, 1996, 5152, 12865, 2076, 1996, 3882, 5424, 1516, 5611, 2162, 1012, 2206, 1996, 4154, 1997, 5424, 2749, 1010, 19027, 27753, 2513, 2000, 11096, 1998, 2366, 2004, 2343]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-9eb41c8832ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# wordsim353_model = DocBERTA(vocab, big = False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordsim_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordsim353_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mresult\u001b[0m \u001b[0;34m= (0.6901393357395735, 4.2646194280196177e-51)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mwordsim_test\u001b[0m \u001b[0;34m= <function wordsim_test at 0x7fd3553309e0>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mwordsim353_model\u001b[0m \u001b[0;34m= <__main__.DocBERT object at 0x7fd3552f0750>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mvocab\u001b[0m \u001b[0;34m= ['Arafat', 'Jackson', 'peace', 'terror', 'FBI', 'fingerprint', 'investigation', 'Harvard', 'Yale', 'Japanese', 'American', 'Jerusalem', 'Israel', 'Palestinian', 'Maradona', 'football', 'Mars', 'scientist', 'water', 'Mexico', 'Brazil', 'OPEC', 'country', 'oil', 'Wednesday', 'news', 'admission', 'ticket', 'alcohol', 'chemistry', 'aluminum', 'metal', 'announcement', 'effort', 'production', 'warning', 'architecture', 'century', 'arrangement', 'accommodation', 'arrival', 'hotel', 'asylum', 'madhouse', 'atmosphere', 'landscape', 'attempt', 'baby', 'mother', 'bank', 'money', 'baseball', 'season', 'bed', 'closet', 'benchmark', 'index', 'bird', 'cock', 'crane', 'bishop', 'rabbi', 'board', 'recommendation', 'book', 'library', 'paper', 'boxing', 'round', 'boy', 'lad', 'bread', 'butter', 'brother', 'monk', 'calculation', 'computation', 'canyon', 'car', 'automobile', 'flight', 'cell', 'phone', 'cemetery', 'woodland', 'nation', 'year', 'championship', 'tournament', 'chance', 'credibility', 'change', 'attitude', 'chord', 'smile', 'clothes', 'coast', 'forest', 'hill', 'shore', 'company', 'stock', 'competition', 'price', 'computer', 'internet', 'keyboard', 'laboratory', 'software', 'concert', 'virtuoso', 'consumer', 'confidence', 'energy', 'citizen', 'implement', 'credit', 'card', 'information', 'cucumber', 'potato', 'cup', 'article', 'artifact', 'coffee', 'drink', 'entity', 'food', 'liquid', 'object', 'substance', 'tableware', 'curre...\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mpair\u001b[0m \u001b[0;34m= <gluonnlp.data.word_embedding_evaluation.WordSim353 object at 0x7fd206a87ed0>\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{round(result[0], 4)} ({result[1]:.4E})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-2eb4b8ba60ea>\u001b[0m in \u001b[0;36mwordsim_test\u001b[0;34m(wordsim_model=<__main__.DocBERT object>, wordsim_vocab=['Arafat', 'Jackson', 'peace', 'terror', 'FBI', 'fingerprint', 'investigation', 'Harvard', 'Yale', 'Japanese', 'American', 'Jerusalem', 'Israel', 'Palestinian', 'Maradona', 'football', 'Mars', 'scientist', 'water', 'Mexico', ...], wordsim_pair=<gluonnlp.data.word_embedding_evaluation.WordSim353 object>)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0msave_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{save_name}-{wordsim353_model.model_size}.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mwordsim_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticlesRetriever\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mwordsim_model.convert\u001b[0m \u001b[0;34m= <bound method DocBERT.convert of <__main__.DocBERT object at 0x7fd3552f0750>>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36marticlesRetriever\u001b[0m \u001b[0;34m= <__main__.ArticleRetriever object at 0x7fd3551bbed0>\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mwordsim_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-8853a9bfa79e>\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self=<__main__.DocBERT object>, article_ret=<__main__.ArticleRetriever object>)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                     \u001b[0msubinputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                  \u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                  \u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                     \u001b[0mtorch_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_one_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubinputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mtorch_cls.append\u001b[0m \u001b[0;34m= <built-in method append of list object at 0x7fd3545c2410>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mself._one_pass\u001b[0m \u001b[0;34m= <bound method DocBERT._one_pass of <__main__.DocBERT object at 0x7fd3552f0750>>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36msubinputs\u001b[0m \u001b[0;34m= {'input_ids': tensor([  101., 12619., 19935.,  2884., 14364., 19935.,  2884., 10546., 16093.,\n         2632.,  1011., 24209.,  2094.,  4213.,  2632.,  1011., 16543.,  2072.,\n         1006.,  1018.,  1013.,  2484.,  2257.,  4612.,  1516.,  2340.,  2281.,\n         2432.,  1007.,  1010., 16071.,  2124.,  2004.,  8038., 18116., 19027.,\n        27753.,  1006., 12098.,  2099.,  1011.,  1114.,  1011.,  6638.,  1010.,\n         2036.,  2149.,  1024., 12098.,  1011.,  1114.,  1011.,  6904., 11039.,\n         1025.,  5640.,  1024.,  1295., 29820., 22192., 15394.,  1300., 25573.,\n        29824., 17149.,  1288., 29816., 15394.,  1270., 23673., 17149., 29820.,\n        22192., 15915.,  1288., 29816., 15394.,  1270., 23673., 17149., 29836.,\n        29836., 29833.,  1288., 17149., 29833., 25573., 29817.,  1270., 23673.,\n        29834., 15394., 29836., 19433.,  1270., 23673., 29820., 29824., 14498.,\n        15915., 14498.,  1025.,  5640.,  1024.,  1300., 25573., 29824., 17149.,\n         1288., 17149., 29833., 25573., 29817.,  1010.,  7651.,  1024.,  8038.,\n        29481.,  1148.,  5400., 27753.,  1007.,  2030.,  2011.,  2010., 28919.,\n         3148.,  8273.,  2572.,  7849.,  1006.,  5640.,  1024.,  1270., 29816.,\n        29836.,  1288., 22192., 25573., 17149.,  1010.,  7651.,  1024.,  1147.,\n         7875.,  2226.,  1148.,  3286.,  7849.,  1007.,  1010.,  2001.,  1037.,\n         9302.,  2576.,  3003.,  1012.,  2002.,  2001.,  3472.,  1997.,  1996.,\n         8976.,  7931.,  3029.,  1006., 20228.,  2080.,  1007.,  2013.,  3440.,\n         2000.,  2432.,  1998.,  2343.,  1997.,  1996.,  9302.,  2120.,  3691.,\n         1006.,  1052.,  2532.,  1007.,  2013.,  2807.,  2000.,  2432.,  1012.,\n        17859.,  2135.,  2019.,  5424.,  8986.,  1010.,  2002.,  2001.,  1037.,\n         4889.,  2266.,  1997.,  1996.,  6638.,  4430.,  2576.,  2283.,  1010.,\n         2029.,  2002.,  2419.,  2013.,  3851.,  2127.,  2432.,  1012., 19027.,\n        27753.,  2001.,  2141.,  2000.,  9302.,  3008.,  1999., 11096.,  1010.,\n         5279.,  1010.,  2073.,  2002.,  2985.,  2087.,  1997.,  2010.,  3360.,\n         1998.,  3273.,  2012.,  1996.,  2118.,  1997.,  2332., 11865.,  4215.,\n         1045.,  1012.,  2096.,  1037.,  3076.,  1010.,  2002., 14218.,  5424.,\n         8986.,  1998.,  3424.,  1011., 21379.,  4784.,  1012.,  4941.,  2000.,\n         1996.,  3882.,  4325.,  1997.,  1996.,  2110.,  1997.,  3956.,  1010.,\n         2002.,  4061.,  4077.,  1996.,  5152., 12865.,  2076.,  1996.,  3882.,\n         5424.,  1516.,  5611.,  2162.,  1012.,  2206.,  1996.,  4154.,  1997.,\n         5424.,  2749.,  1010., 19027., 27753.,  2513.,  2000., 11096.,  1998.,\n         2366.,  2004.,  2343.]), 'token_type_ids': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-8853a9bfa79e>\u001b[0m in \u001b[0;36m_one_pass\u001b[0;34m(self=<__main__.DocBERT object>, subinputs={'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., ... 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'input_ids': tensor([  101., 12619., 19935.,  2884., 14364., ... 11096.,  1998.,\n         2366.,  2004.,  2343.]), 'token_type_ids': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])})\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_one_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubinputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubinputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubinputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36moutputs\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mself.model\u001b[0m \u001b[0;34m= BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n    (position_embeddings): Embedding(512, 1024)\n    (token_type_embeddings): Embedding(2, 1024)\n    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (12): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (13): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (14): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (15): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (16): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (17): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (18): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (19): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (20): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (21): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (22): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (23): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (activation): Tanh()\n  )\n)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36msubinputs\u001b[0m \u001b[0;34m= {'input_ids': tensor([  101., 12619., 19935.,  2884., 14364., 19935.,  2884., 10546., 16093.,\n         2632.,  1011., 24209.,  2094.,  4213.,  2632.,  1011., 16543.,  2072.,\n         1006.,  1018.,  1013.,  2484.,  2257.,  4612.,  1516.,  2340.,  2281.,\n         2432.,  1007.,  1010., 16071.,  2124.,  2004.,  8038., 18116., 19027.,\n        27753.,  1006., 12098.,  2099.,  1011.,  1114.,  1011.,  6638.,  1010.,\n         2036.,  2149.,  1024., 12098.,  1011.,  1114.,  1011.,  6904., 11039.,\n         1025.,  5640.,  1024.,  1295., 29820., 22192., 15394.,  1300., 25573.,\n        29824., 17149.,  1288., 29816., 15394.,  1270., 23673., 17149., 29820.,\n        22192., 15915.,  1288., 29816., 15394.,  1270., 23673., 17149., 29836.,\n        29836., 29833.,  1288., 17149., 29833., 25573., 29817.,  1270., 23673.,\n        29834., 15394., 29836., 19433.,  1270., 23673., 29820., 29824., 14498.,\n        15915., 14498.,  1025.,  5640.,  1024.,  1300., 25573., 29824., 17149.,\n         1288., 17149., 29833., 25573., 29817.,  1010.,  7651.,  1024.,  8038.,\n        29481.,  1148.,  5400., 27753.,  1007.,  2030.,  2011.,  2010., 28919.,\n         3148.,  8273.,  2572.,  7849.,  1006.,  5640.,  1024.,  1270., 29816.,\n        29836.,  1288., 22192., 25573., 17149.,  1010.,  7651.,  1024.,  1147.,\n         7875.,  2226.,  1148.,  3286.,  7849.,  1007.,  1010.,  2001.,  1037.,\n         9302.,  2576.,  3003.,  1012.,  2002.,  2001.,  3472.,  1997.,  1996.,\n         8976.,  7931.,  3029.,  1006., 20228.,  2080.,  1007.,  2013.,  3440.,\n         2000.,  2432.,  1998.,  2343.,  1997.,  1996.,  9302.,  2120.,  3691.,\n         1006.,  1052.,  2532.,  1007.,  2013.,  2807.,  2000.,  2432.,  1012.,\n        17859.,  2135.,  2019.,  5424.,  8986.,  1010.,  2002.,  2001.,  1037.,\n         4889.,  2266.,  1997.,  1996.,  6638.,  4430.,  2576.,  2283.,  1010.,\n         2029.,  2002.,  2419.,  2013.,  3851.,  2127.,  2432.,  1012., 19027.,\n        27753.,  2001.,  2141.,  2000.,  9302.,  3008.,  1999., 11096.,  1010.,\n         5279.,  1010.,  2073.,  2002.,  2985.,  2087.,  1997.,  2010.,  3360.,\n         1998.,  3273.,  2012.,  1996.,  2118.,  1997.,  2332., 11865.,  4215.,\n         1045.,  1012.,  2096.,  1037.,  3076.,  1010.,  2002., 14218.,  5424.,\n         8986.,  1998.,  3424.,  1011., 21379.,  4784.,  1012.,  4941.,  2000.,\n         1996.,  3882.,  4325.,  1997.,  1996.,  2110.,  1997.,  3956.,  1010.,\n         2002.,  4061.,  4077.,  1996.,  5152., 12865.,  2076.,  1996.,  3882.,\n         5424.,  1516.,  5611.,  2162.,  1012.,  2206.,  1996.,  4154.,  1997.,\n         5424.,  2749.,  1010., 19027., 27753.,  2513.,  2000., 11096.,  1998.,\n         2366.,  2004.,  2343.]), 'token_type_ids': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mattention_mask\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self=BertModel(\n  (embeddings): BertEmbeddings(\n    (...s=1024, bias=True)\n    (activation): Tanh()\n  )\n), *input=(tensor([  101., 12619., 19935.,  2884., 14364., ... 11096.,  1998.,\n         2366.,  2004.,  2343.]),), **kwargs={'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., ... 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])})\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mforward_call\u001b[0m \u001b[0;34m= <bound method BertModel.forward of BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n    (position_embeddings): Embedding(512, 1024)\n    (token_type_embeddings): Embedding(2, 1024)\n    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (12): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (13): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (14): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (15): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (16): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (17): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (18): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (19): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (20): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (21): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (22): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (23): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (activation): Tanh()\n  )\n)>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36minput\u001b[0m \u001b[0;34m= (tensor([  101., 12619., 19935.,  2884., 14364., 19935.,  2884., 10546., 16093.,\n         2632.,  1011., 24209.,  2094.,  4213.,  2632.,  1011., 16543.,  2072.,\n         1006.,  1018.,  1013.,  2484.,  2257.,  4612.,  1516.,  2340.,  2281.,\n         2432.,  1007.,  1010., 16071.,  2124.,  2004.,  8038., 18116., 19027.,\n        27753.,  1006., 12098.,  2099.,  1011.,  1114.,  1011.,  6638.,  1010.,\n         2036.,  2149.,  1024., 12098.,  1011.,  1114.,  1011.,  6904., 11039.,\n         1025.,  5640.,  1024.,  1295., 29820., 22192., 15394.,  1300., 25573.,\n        29824., 17149.,  1288., 29816., 15394.,  1270., 23673., 17149., 29820.,\n        22192., 15915.,  1288., 29816., 15394.,  1270., 23673., 17149., 29836.,\n        29836., 29833.,  1288., 17149., 29833., 25573., 29817.,  1270., 23673.,\n        29834., 15394., 29836., 19433.,  1270., 23673., 29820., 29824., 14498.,\n        15915., 14498.,  1025.,  5640.,  1024.,  1300., 25573., 29824., 17149.,\n         1288., 17149., 29833., 25573., 29817.,  1010.,  7651.,  1024.,  8038.,\n        29481.,  1148.,  5400., 27753.,  1007.,  2030.,  2011.,  2010., 28919.,\n         3148.,  8273.,  2572.,  7849.,  1006.,  5640.,  1024.,  1270., 29816.,\n        29836.,  1288., 22192., 25573., 17149.,  1010.,  7651.,  1024.,  1147.,\n         7875.,  2226.,  1148.,  3286.,  7849.,  1007.,  1010.,  2001.,  1037.,\n         9302.,  2576.,  3003.,  1012.,  2002.,  2001.,  3472.,  1997.,  1996.,\n         8976.,  7931.,  3029.,  1006., 20228.,  2080.,  1007.,  2013.,  3440.,\n         2000.,  2432.,  1998.,  2343.,  1997.,  1996.,  9302.,  2120.,  3691.,\n         1006.,  1052.,  2532.,  1007.,  2013.,  2807.,  2000.,  2432.,  1012.,\n        17859.,  2135.,  2019.,  5424.,  8986.,  1010.,  2002.,  2001.,  1037.,\n         4889.,  2266.,  1997.,  1996.,  6638.,  4430.,  2576.,  2283.,  1010.,\n         2029.,  2002.,  2419.,  2013.,  3851.,  2127.,  2432.,  1012., 19027.,\n        27753.,  2001.,  2141.,  2000.,  9302.,  3008.,  1999., 11096.,  1010.,\n         5279.,  1010.,  2073.,  2002.,  2985.,  2087.,  1997.,  2010.,  3360.,\n         1998.,  3273.,  2012.,  1996.,  2118.,  1997.,  2332., 11865.,  4215.,\n         1045.,  1012.,  2096.,  1037.,  3076.,  1010.,  2002., 14218.,  5424.,\n         8986.,  1998.,  3424.,  1011., 21379.,  4784.,  1012.,  4941.,  2000.,\n         1996.,  3882.,  4325.,  1997.,  1996.,  2110.,  1997.,  3956.,  1010.,\n         2002.,  4061.,  4077.,  1996.,  5152., 12865.,  2076.,  1996.,  3882.,\n         5424.,  1516.,  5611.,  2162.,  1012.,  2206.,  1996.,  4154.,  1997.,\n         5424.,  2749.,  1010., 19027., 27753.,  2513.,  2000., 11096.,  1998.,\n         2366.,  2004.,  2343.]),)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mkwargs\u001b[0m \u001b[0;34m= {'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self=BertModel(\n  (embeddings): BertEmbeddings(\n    (...s=1024, bias=True)\n    (activation): Tanh()\n  )\n), input_ids=tensor([  101., 12619., 19935.,  2884., 14364., ... 11096.,  1998.,\n         2366.,  2004.,  2343.]), attention_mask=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., ... 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=False, output_attentions=False, output_hidden_states=True, return_dict=True)\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to specify either input_ids or inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mbatch_size\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mseq_length\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36minput_shape\u001b[0m \u001b[0;34m= torch.Size([300])\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ]
    }
  ]
}