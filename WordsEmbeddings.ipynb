{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordsEmbeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taedriel/ZSL-v2/blob/wordEmbedding/WordsEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yes | pip install transformers wget tensorflow_datasets wikipedia deprecated unzip gluonnlp mxnet --quiet\n",
        "!mkdir -p temp article"
      ],
      "metadata": {
        "id": "HEq14T6Nl535"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "import wikipedia\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import mxnet as mx\n",
        "import gluonnlp as nlp\n",
        "import traceback\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, RobertaModel, RobertaTokenizer\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level = logging.INFO, filename = \"BERT.log\" )\n",
        "# logging.basicConfig(level = logging.INFO)\n",
        "\n",
        "from tqdm import tqdm\n",
        "from deprecated import deprecated\n",
        "from typing import List, Tuple\n",
        "from os.path import exists, join\n",
        "from enum import Enum\n",
        "\n",
        "wikipedia.set_rate_limiting(True)"
      ],
      "metadata": {
        "id": "spXWSv2iW9i5"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils classes"
      ],
      "metadata": {
        "id": "kjt7fI8Olf5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class customWikiArticle:\n",
        "\n",
        "    def __init__(self, index : int, title : str, realtitle : str, summary : str, ambiguous : bool):\n",
        "        self.index = index\n",
        "        self.title = title\n",
        "        self.realtitle = realtitle\n",
        "        self.summary = summary\n",
        "        self.ambiguous = ambiguous\n",
        "\n",
        "class ArticleRetriever:\n",
        "\n",
        "    article_dir = \"./article\"\n",
        "\n",
        "    def __init__(self, name : str, list_title : List[str]):\n",
        "\n",
        "        self.name = name\n",
        "        self.list_title = list_title\n",
        "        self.modified = False\n",
        "\n",
        "        if not exists(self.get_filename()):\n",
        "            self.articles_map = {}\n",
        "        else:\n",
        "            with open(self.get_filename(), \"rb\") as mapfile:\n",
        "                self.articles_map = pickle.load(mapfile)\n",
        "\n",
        "    def get_filename(self):\n",
        "        return join(ArticleRetriever.article_dir,self.name)\n",
        "\n",
        "    def load_article(self, title, force_reload : bool = False) -> customWikiArticle:\n",
        "        if title not in self.articles_map:\n",
        "            self.modified = True\n",
        "            realtitle, summary, ambiguous = self._retrieve_article(title)\n",
        "            self.articles_map[title] = customWikiArticle(len(self.articles_map), title, realtitle, summary, ambiguous)\n",
        "\n",
        "        if title in self.articles_map and self.articles_map[title].summary == None and force_reload:\n",
        "            self.modified = True\n",
        "            realtitle, summary, ambiguous = self._retrieve_article(title)\n",
        "            self.articles_map[title].summary = summary\n",
        "\n",
        "        return self.articles_map[title]\n",
        "\n",
        "    def _retrieve_article(self, title, closed_list : List = []) -> Tuple[str, str, bool]: \n",
        "        closed_list.append(title)\n",
        "        try:\n",
        "            article = wikipedia.page(title, auto_suggest=False, redirect=True)\n",
        "            return (article.title, article.summary, False)\n",
        "\n",
        "        except wikipedia.PageError as e:\n",
        "            search_result = wikipedia.search(title, suggestion = False)\n",
        "\n",
        "            logging.warning(f\"{title} misspelled or article missing. Best find is {search_result[0]}\")\n",
        "            if search_result[0] is not None and search_result[0] not in closed_list:            \n",
        "                return self._retrieve_article(search_result[0], closed_list)  \n",
        "            else: return (title, None, False)\n",
        "\n",
        "        except wikipedia.DisambiguationError as e:\n",
        "            logging.warning(f\"{title} is ambiguous, trying first {e.options[0]}\")\n",
        "            if e.options[0] is not None and e.options[0] not in closed_list:\n",
        "                res = self._retrieve_article(e.options[0], closed_list)\n",
        "                return (res[0], res[1], True)\n",
        "        \n",
        "            return (None, None, None)\n",
        "\n",
        "\n",
        "    def load_all_articles(self, force_reload : bool = False) -> None:\n",
        "        logging.info(f\"Starting loading articles...\")\n",
        "        nb_success = 0\n",
        "\n",
        "        nb_article = len(self.list_title)\n",
        "        for i, title in tqdm(enumerate(self.list_title), total=nb_article):\n",
        "            self.load_article(title, force_reload)\n",
        "\n",
        "            if self.articles_map[title].summary is not None: \n",
        "                nb_success += 1\n",
        "        logging.info(f\"Finished loading {nb_success} article(s) !\")\n",
        "        return self.modified\n",
        "\n",
        "    def __call__(self, force_reload : bool = True) -> None:\n",
        "        return self.load_all_articles(force_reload)\n",
        "\n",
        "    def get_article(self, title):\n",
        "        if title not in self.articles_map:\n",
        "            self.load_article(title)\n",
        "\n",
        "        return self.articles_map[title]\n",
        "        \n",
        "    def save(self):\n",
        "        with open(self.get_filename(), \"wb\") as mapfile:\n",
        "            pickle.dump(self.articles_map, mapfile)\n",
        "\n",
        "class WordToVector:\n",
        "\n",
        "    def __init__(self, list_tags : List[str] = []):\n",
        "        self.list_tags = list_tags\n",
        "        self.embeddings = {}\n",
        "\n",
        "    def export(self, filename):\n",
        "        return NotImplementedError\n",
        "\n",
        "    def convert(self):\n",
        "        return NotImplementedError\n",
        "\n",
        "class Sum4LastLayers:\n",
        "\n",
        "    def merge(self, vector):\n",
        "        return torch.sum(vector[-4:], dim = 0)\n"
      ],
      "metadata": {
        "id": "QZiZ2Pr-lUiS"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingsLoader:\n",
        "\n",
        "    def __init__(self, filename : str):\n",
        "\n",
        "        self.file = filename\n",
        "        self.embeddings = {}\n",
        "\n",
        "        self._load_file()\n",
        "\n",
        "    def _load_file(self):\n",
        "        try:\n",
        "            with open(self.file, \"r\") as f:\n",
        "                lines = f.readlines()\n",
        "                \n",
        "            for line in lines[1:]:\n",
        "                data = line.split(\",\")\n",
        "                self.embeddings[data[0]] = torch.FloatTensor(list(map(float, data[1:])))\n",
        "\n",
        "        except IOError as e:\n",
        "            raise IOError(f\"No file {self.file}\")\n",
        "\n",
        "class SimilarityCompute(EmbeddingsLoader):\n",
        "\n",
        "    def __init__(self, embeddings):\n",
        "        super(SimilarityCompute, self).__init__(embeddings)\n",
        "\n",
        "\n",
        "    def compute_sim(self):\n",
        "        \"\"\" compute cosine similarity between all vectors \"\"\"\n",
        "        if len(self.embeddings) == 0:\n",
        "            raise Exception(\"Tags not converted yet !\")\n",
        "\n",
        "        logging.info(\"Computing cosine similarity, this could take some time...\")\n",
        "\n",
        "        if self.cosine_sim_matrix is None:\n",
        "            n_tokens = len(self.embeddings)\n",
        "            self.cosine_sim_matrix = [[1 for j in range(n_tokens)] for i in range(n_tokens)]\n",
        "\n",
        "        for j, vector in tqdm(enumerate(self.embeddings), total = len(self.embeddings)):\n",
        "\n",
        "            for i, other_vector in enumerate(self.embeddings):\n",
        "\n",
        "                if i == j:\n",
        "                    continue\n",
        "\n",
        "                cos = torch.nn.CosineSimilarity(dim=0)\n",
        "                similarity = cos(vector[1], other_vector[1])\n",
        "\n",
        "                self.cosine_sim_matrix[i][j] = similarity\n",
        "                self.cosine_sim_matrix[j][i] = similarity\n",
        "\n",
        "    def export_sim_matrix(self, filename):\n",
        "        if self.cosine_sim_matrix == None:\n",
        "            self.compute_sim()\n",
        "        \n",
        "        try:\n",
        "            f = open(filename, \"w\")\n",
        "        except OSError:\n",
        "            raise OSError(\"Could not open file\")\n",
        "\n",
        "        with f:\n",
        "            print(\"/\", \",\".join([tag[0] for tag in self.embeddings]), sep = \",\", file = f)\n",
        "\n",
        "            for j, tag_y in enumerate(self.embeddings):\n",
        "                print(tag_y[0], \",\".join( [str(round(float(self.cosine_sim_matrix[j][i]), 3)) for i in range(len(self.embeddings))]), sep = \",\", file = f)\n",
        "\n",
        "    def sim_between(self, token1, token2):\n",
        "        index1, v1 = [(i, v[1]) for i, v in enumerate(self.embeddings) if v[0] == token1][0]\n",
        "        index2, v2 = [(i, v[1]) for i, v in enumerate(self.embeddings) if v[0] == token2][0]\n",
        "\n",
        "        if self.cosine_sim_matrix is None:\n",
        "            n_tokens = len(self.embeddings)\n",
        "            self.cosine_sim_matrix = [[1 for j in range(n_tokens)] for i in range(n_tokens)]\n",
        "\n",
        "        if self.cosine_sim_matrix[index1][index2] == 0 or self.cosine_sim_matrix[index2][index1]:\n",
        "            cos = torch.nn.CosineSimilarity(dim=0)\n",
        "            similarity = cos(v1, v2)\n",
        "\n",
        "            self.cosine_sim_matrix[index1][index2] = similarity\n",
        "            self.cosine_sim_matrix[index2][index1] = similarity\n",
        "\n",
        "        return self.cosine_sim_matrix[index1][index2]\n",
        "\n",
        "class Solver(EmbeddingsLoader):\n",
        "\n",
        "    DEFAULT_MIN_LIST_RESULT = 10\n",
        "\n",
        "    def __init__(self, embeddings):\n",
        "        super(Solver, self).__init__(embeddings)\n",
        "\n",
        "    def get_nearest_embedding_of(self, embedding, nb = 10):\n",
        "\n",
        "        if nb > len(self.embeddings):\n",
        "            raise Exception(\"nb too high, not enough token\")\n",
        "\n",
        "        nearest = []\n",
        "        for tag, e in self.embeddings.items():\n",
        "\n",
        "            cos = torch.nn.CosineSimilarity(dim=0)\n",
        "            similarity = cos(embedding, e)\n",
        "\n",
        "            nearest.append((tag, similarity))\n",
        "        \n",
        "        nearest.sort(key = lambda tup : tup[1])\n",
        "        return nearest[-1:-nb-1:-1]\n",
        "\n",
        "    def __call__(self, embeddeding):\n",
        "        return self.get_nearest_embedding_of(embeddeding, min(Solver.DEFAULT_MIN_LIST_RESULT, len(self.embeddings)))"
      ],
      "metadata": {
        "id": "5AxDIhr1rkca"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT model"
      ],
      "metadata": {
        "id": "-0g4CJtelYfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTModel(WordToVector):\n",
        "\n",
        "    temp_dir = \"./temp\"\n",
        "\n",
        "    def __init__(self, list_tag : List[str], big: bool = False, window : int = 100):\n",
        "        super(BERTModel, self).__init__(list_tag)\n",
        "        self.list_articles = {}\n",
        "        self.window_size = window\n",
        "\n",
        "        self.model_size = \"bert-large-uncased\" if big else \"bert-base-uncased\"\n",
        "        self.cosine_sim_matrix = None\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.model_size, padding=True, truncation=True,)\n",
        "        self.model = BertModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "    def export(self, filename):\n",
        "        \"\"\"export all the embeddings in filename under a .csv format.\n",
        "           Raise exception if embeddings hasn't been calculed yet.\"\"\"\n",
        "\n",
        "        if len(self.embeddings) == 0:\n",
        "            raise Exception(\"Tags not converted yet !\")\n",
        "        \n",
        "        try:\n",
        "            f = open(filename, \"w\")\n",
        "        except OSError:\n",
        "            raise OSError(\"Could not open file\")\n",
        "\n",
        "        dimension_number = len(next(iter(self.embeddings.values())))\n",
        "        with f:\n",
        "            print(\"embeddings\", *[str(i) for i in range(dimension_number)], sep=\",\", file=f)\n",
        "            for tag, embedding in self.embeddings.items():\n",
        "                print(tag, *list(map(lambda x: str(float(x)), embedding)), sep=\",\", file=f)\n",
        "\n",
        "    def reset_embeddings(self):\n",
        "        self.embeddings.clear()\n",
        "\n",
        "    def convert(self, article_ret : ArticleRetriever):\n",
        "        \"\"\" convert all word in their embeddings\"\"\"\n",
        "\n",
        "        if len(self.list_tags) == 0:\n",
        "            raise Exception(\"not tags yet !\")\n",
        "\n",
        "        logging.info(\"Starting converting tokens...\")\n",
        "        nb_token = len(self.list_tags)\n",
        "        for i, tag in tqdm(enumerate(self.list_tags), total = nb_token):\n",
        "            \n",
        "            article = article_ret.get_article(tag)\n",
        "            tag_plus_context = tag\n",
        "            if article.summary is not None:\n",
        "                max_size_article = min(len(article.summary), self.window_size)\n",
        "                tag_plus_context = tag + \". \" + article.summary[:max_size_article]\n",
        "\n",
        "            inputs = self.tokenizer(tag_plus_context, return_tensors = \"pt\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            hidden_states = outputs[2]\n",
        "\n",
        "            # [# layers, # batches, # tokens, # features] ==> [# tokens, # layers, # features]\n",
        "            token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "            token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "            token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "            # apply different strategy to summarize word embeddings\n",
        "            # tokenized_text = self.tokenizer.tokenize(tag)\n",
        "            # acc = []\n",
        "            # for i, token in reversed(list(enumerate(tokenized_text))):\n",
        "\n",
        "            #     embed = self.merging_strategy.merge(token_embeddings[i+1])\n",
        "            #     if i == 0:\n",
        "            #         if len(acc) != 0:\n",
        "            #             embed = torch.mean(torch.stack([x[1] for x in acc]), dim=0)\n",
        "            #             token = tag\n",
        "            #             acc = []\n",
        "\n",
        "            #         self.embeddings.append((token, embed))\n",
        "            #     else:\n",
        "            #         acc.append((token, embed))\n",
        "            \n",
        "            # here we are just taking the [CLS] (for classification) as an embedding for the tag\n",
        "            self.embeddings[tag] = self.merging_strategy.merge(token_embeddings[0])\n",
        "\n",
        "    def get_embedding_of(self, token):\n",
        "        if token not in self.embeddings:\n",
        "            raise Exception(f\"no such token {token}\")\n",
        "        \n",
        "        return self.embeddings[token]\n",
        "\n",
        "    def get_class_list(self):\n",
        "        return self.embeddings.keys()"
      ],
      "metadata": {
        "id": "UVJUSGWQorYj"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RoBERTa model"
      ],
      "metadata": {
        "id": "eGVvWFvtlqLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ROBERTAModel(BERTModel):\n",
        "\n",
        "    def __init__(self, list_tag : List[str], big: bool = False, window : int = 100):\n",
        "        WordToVector.__init__(self, list_tag)\n",
        "        self.window_size = window\n",
        "\n",
        "        self.model_size = \"roberta-large\" if big else \"roberta-base\"\n",
        "        self.cosine_sim_matrix = None\n",
        "\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_size, padding=True, truncation=True,)\n",
        "        self.model = RobertaModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()"
      ],
      "metadata": {
        "id": "7oWZOwG-mCYI"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings to word proba"
      ],
      "metadata": {
        "id": "JNFmhVBmUTgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "solver = Solver(\"animal10-embeddeding.csv\")\n",
        "\n",
        "totest = solver.embeddings[\"cat\"]\n",
        "\n",
        "print(solver.get_nearest_embedding_of(totest))"
      ],
      "metadata": {
        "id": "tD8XokPDUhkp",
        "outputId": "777a2c6b-eb71-4f14-f029-1df2b2f9d8a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('cat', tensor(1.)), ('horse', tensor(0.9067)), ('squirrel', tensor(0.8875)), ('chicken', tensor(0.8678)), ('butterfly', tensor(0.8576)), ('cow', tensor(0.8493)), ('spider', tensor(0.8481)), ('dog', tensor(0.8410)), ('elephant', tensor(0.7702)), ('sheep', tensor(0.7604))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
        "# imagenet_labels = np.array(open(labels_path).read().splitlines())\n",
        "# imagenet_labels = list(imagenet_labels)\n",
        "# sub_imagenet_labels = imagenet_labels[:200]"
      ],
      "metadata": {
        "id": "NIJ8zsk-UTuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word to embeddings"
      ],
      "metadata": {
        "id": "LN273RlCUGza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "animal10 = [\"dog\", \"cat\", \"horse\", \"spider\", \"butterfly\", \"chicken\", \"sheep\", \"cow\", \"squirrel\", \"elephant\"]\n",
        "cifar10  = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "cifar100 = [\"apple\", \"aquarium_fish\", \"baby\", \"bear\", \"beaver\", \"bed\", \"bee\", \"beetle\", \"bicycle\", \"bottle\", \"bowl\", \"boy\", \"bridge\", \"bus\", \"butterfly\", \"camel\", \"can\", \\\n",
        "            \"castle\", \"caterpillar\", \"cattle\", \"chair\", \"chimpanzee\", \"clock\", \"cloud\", \"cockroach\", \"computer_keyboard\", \"couch\", \"crab\", \"crocodile\", \"cup\", \\\n",
        "            \"dinosaur\", 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'lamp', 'lawn_mower', 'leopard', 'lion', \\\n",
        "            'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', \\\n",
        "            'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark',\\\n",
        "            'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', \\\n",
        "            'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']"
      ],
      "metadata": {
        "id": "EYnbJc84xuh5"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_name = \"cifar100\"\n",
        "vocab = cifar100\n",
        "\n",
        "# model = BERTModel(model, big = True, window = 100)\n",
        "model = ROBERTAModel(vocab, big = True, window = 100)\n",
        "\n",
        "articlesRetriever = ArticleRetriever(save_name + \".art\", vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvIn_JeFlYfK",
        "outputId": "cfe69af6-78c1-4ed6-f4e2-2c914fc4a306"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if articlesRetriever():\n",
        "    articlesRetriever.save()\n",
        "\n",
        "model.reset_embeddings()\n",
        "model.convert(articlesRetriever)\n",
        "model.export(f\"{save_name}-{model.model_size}-{model.window_size}.csv\")\n",
        "print(\"\\n\", len(model.get_class_list()))"
      ],
      "metadata": {
        "id": "WOOlgG8koVD6",
        "outputId": "9d49d839-306c-4935-83e1-01b3b5eef3fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 97338.22it/s]\n",
            "100%|██████████| 100/100 [00:52<00:00,  1.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wikipedia debug"
      ],
      "metadata": {
        "id": "7jubA6JXUMk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Article to search for { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "totest = \"king\" #@param {type:\"string\"}\n",
        "\n",
        "result = wikipedia.search(totest, suggestion = False)\n",
        "print(result)\n",
        "print(f\"first result is: {result[0]}\")\n",
        "\n",
        "try:\n",
        "    print(wikipedia.page(totest, auto_suggest=False, redirect=True))\n",
        "    print(wikipedia.page(result[0], auto_suggest=False, redirect=True))\n",
        "except Exception as e:\n",
        "    print(f\"best option envisaged: {e.options[0]}\")\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "PeM8XAu6h5gD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3d0a8cf-88bf-4da4-a111-1bc05d416cce"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['King', 'King (disambiguation)', 'Martin Luther King Jr.', 'Stephen King', 'King & King', 'George VI', 'The Lion King', 'Burger King', 'King King', 'List of King of the Hill episodes']\n",
            "first result is: King\n",
            "<WikipediaPage 'King'>\n",
            "<WikipediaPage 'King'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test part"
      ],
      "metadata": {
        "id": "ax8Aw_qVRxer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "\n",
        "save_name = \"king-test\"\n",
        "vocab = [\"King\", \"Queen\", \"men\", \"woman\"]\n",
        "\n",
        "king_test_model = ROBERTAModel(vocab, big = False, window = 10)\n",
        "\n",
        "king_test_articlesRetriever = ArticleRetriever(save_name + \".art\", vocab)\n",
        "if king_test_articlesRetriever():\n",
        "    king_test_articlesRetriever.save()\n",
        "\n",
        "\n",
        "king_test_model.convert(articlesRetriever)\n",
        "king_test_model.export(save_name + \".csv\")\n",
        "king_test_solver = Solver(save_name + \".csv\")\n",
        "\n",
        "\n",
        "men = king_test_model.get_embedding_of(\"men\")\n",
        "woman = king_test_model.get_embedding_of(\"woman\")\n",
        "king = king_test_model.get_embedding_of(\"King\")\n",
        "\n",
        "totest = king.sub(men).add(woman)\n",
        "print(\"\\n\", king_test_solver(totest))"
      ],
      "metadata": {
        "id": "8uNXNj_wCWhf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21dc0cda-90a6-42e6-9c5c-9ba4e8909a75"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 4/4 [00:00<00:00, 23730.15it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00,  8.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " [('woman', tensor(0.9997)), ('King', tensor(0.9996)), ('Queen', tensor(0.9996)), ('men', tensor(0.9986))]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordsim353 = nlp.data.WordSim353('all')"
      ],
      "metadata": {
        "id": "dnkI3j0V-CkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Different result for King - men + women equation with different context window size\n",
        "\n",
        "| model | window size | rank of Queen | distance with first |\n",
        "|-------|-------------|---------------|-----------|\n",
        "| bert-large | 0  | 2 |  .0693 |\n",
        "| bert-large | 10 | 3 |  .1191 | \n",
        "| bert-large | 50 | 2 |  .1672 |\n",
        "| bert-large | 100 | 2 | .1275 |\n",
        "| bert-large | 150 | 3 | .1134 |\n",
        "| bert-large | 200 | 3 | .1923 |\n",
        "| bert-large | 300 | 3 | .0939 |\n",
        "| bert-large | 400 | 3 | .1455 |\n",
        "| roberta-large | 0 | 2 | .0001 |\n",
        "| roberta-large | 10 | 3 | .0011 |\n",
        "| roberta-large | 50 | 2 | .0029 |\n",
        "| roberta-large | 100 | 4 | .0061 |\n",
        "| roberta-large | 150 | 3 | .0023 |\n",
        "| roberta-large | 200 | 4 | .0055 |\n",
        "| roberta-large | 300 | 4 | .0127 |\n",
        "| roberta-large | 400 | 4 | .0045 |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kY8yWSmJw6Ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "\n",
        "vocab = []\n",
        "\n",
        "for w1, w2, i in wordsim353:\n",
        "    if w1 not in vocab:\n",
        "        vocab.append(w1)\n",
        "    if w2 not in vocab:\n",
        "        vocab.append(w2)\n",
        "\n",
        "print(len(vocab))\n",
        "wordsim353_model = BERTModel(vocab, big = False, window = 0)\n",
        "articlesRetriever = ArticleRetriever(\"wordsim353\", vocab)\n",
        "\n",
        "try:\n",
        "    articlesRetriever.load_all_articles(force_reload = True)\n",
        "    articlesRetriever.save()\n",
        "except Exception:\n",
        "    traceback.print_exc()\n",
        "\n",
        "wordsim353_model.convert(articlesRetriever)"
      ],
      "metadata": {
        "id": "YmFXzvYYBVzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "total_comparison = 0\n",
        "sim_list = []\n",
        "i_list = []\n",
        "\n",
        "for w1, w2, i in wordsim353:\n",
        "    try:\n",
        "        # print(f\"how much {w1} is similar to {w2}:\")\n",
        "        sim = wordsim353_model.sim_between(w1, w2)\n",
        "\n",
        "        sim_list.append(sim)\n",
        "        i_list.append(i\n",
        "                      )\n",
        "        # print(f\"\\t{sim} & {i}\")\n",
        "        total_comparison += 1\n",
        "    except Exception as e:\n",
        "        print(w1, w2)\n",
        "        continue\n",
        "\n",
        "print(spearmanr(sim_list, i_list))"
      ],
      "metadata": {
        "id": "Vwy1yo7iIgnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pearson correlation rank with different context window size\n",
        " \n",
        "| model | corpus | test set |window size | pearson rank correlation |\n",
        "|-------|--------|----------|------------|--------------------------|\n",
        "| bert large | article from wordsim vocab | wordsim353 | 100 | 0.2158 (5.8353e-05)|\n",
        "| bert base  | article from wordsim vocab | wordsim353 | 100 | 0.2284 (6.8215e-05)|\n",
        "| bert-base  | article from wordsim vocav | wordsim353 | 300 | 0.1326 (1.27e-02) |\n",
        "| bert-large  | article from wordsim vocav | wordsim353 | 300 | 0.2117 (6.2153e-05) |\n",
        "| bert-large  | article from wordsim vocav | wordsim353 | 0 | 0.2638 (5.1232-07) |\n",
        "| bert-base  | article from wordsim vocav | wordsim353 | 0 | 0.3721 (5.2306-13) |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-VuUxJc6zumT"
      }
    }
  ]
}