{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Mapping visual embeddings to textual embeddings :","metadata":{}},{"cell_type":"markdown","source":"This notebook implements the paper “DeViSE: A Deep Visual-Semantic Embedding Model”\nby Fromme et al. (2013).","metadata":{}},{"cell_type":"markdown","source":"### Importing the modules :","metadata":{}},{"cell_type":"code","source":"import os \nimport logging\nimport pandas as pd\nimport random\nimport numpy as np\nfrom tqdm import tqdm, tqdm_notebook\nimport matplotlib.pyplot as plt\nimport pickle\nimport itertools\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport seaborn as sns\nfrom typing import List, Tuple, Dict","metadata":{"execution":{"iopub.status.busy":"2022-08-20T14:25:44.299580Z","iopub.execute_input":"2022-08-20T14:25:44.300132Z","iopub.status.idle":"2022-08-20T14:25:44.308496Z","shell.execute_reply.started":"2022-08-20T14:25:44.300082Z","shell.execute_reply":"2022-08-20T14:25:44.306799Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nimport tensorflow as tf\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization, RandomFlip\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras import preprocessing\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2022-08-20T14:25:54.895517Z","iopub.execute_input":"2022-08-20T14:25:54.895977Z","iopub.status.idle":"2022-08-20T14:26:03.613639Z","shell.execute_reply.started":"2022-08-20T14:25:54.895926Z","shell.execute_reply":"2022-08-20T14:26:03.612225Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Variables:\n    path = '../input/996-imagenet'\n    embeddings_path = '../input/996-embeddings/996-embeddings.csv'\n    super_class_csv = '../input/super-classes/super_classes.csv'\n    general_mapping_model_path = '../input/mod996/resnet50-996-classes.model'\n    small_animal_mapper_path = '../input/small-animal/resnet-small-animal.model'","metadata":{"execution":{"iopub.status.busy":"2022-08-20T14:26:09.684543Z","iopub.execute_input":"2022-08-20T14:26:09.685176Z","iopub.status.idle":"2022-08-20T14:26:09.691739Z","shell.execute_reply.started":"2022-08-20T14:26:09.685144Z","shell.execute_reply":"2022-08-20T14:26:09.690440Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Building a data generator :\n---\nThis data generator aims to not overpass the usage of the RAM. You have to give the label embeddings so It can encode the labels, the filenames which represent the images's path, the classes_size (in our case is 300 which is the size of the word embeddings) and then the batch and the image size.","metadata":{}},{"cell_type":"code","source":"class ImageGenerator(tf.keras.utils.Sequence):\n    \"\"\"\n    Load and read a dataset (load images and encode labels) \n    \n    Attributes\n    ----------\n    embedding_csv :\n        A csv file containing the word embedding of the labels.\n    filenames :\n        The path of images\n    labels :\n        The labels of images\n    classes_size : \n        The size of the word embeddings\n    batch_size \n    image_size\n    shuffle :\n        Shuffle or not the batch of data (default=True)\n    \"\"\"\n    def __init__(self, embeddings_csv : pd.DataFrame, filenames : List[str], labels : List[str], classes_size : int, batch_size : int, image_size=(224, 224), shuffle=True):\n        self.embeddings_df = pd.read_csv(embeddings_csv)\n        self.image_size, self.batch_size = image_size, batch_size\n        self.items, self.items_size = filenames, len(filenames)\n        self.labels = labels\n        self.classes_size = classes_size\n        self.indexes = np.arange(self.items_size)\n        self.shuffle = shuffle\n        self.on_epoch_end()\n    \n    def encode_label(self, label : str) -> np.ndarray:\n        \"\"\"\n        Returns the word embedding of the label given in the parameter\n        \"\"\"\n        return self.embeddings_df[self.embeddings_df['embeddings'].str.lower() == label][:].to_numpy().T[1:].T\n    \n    def load_urls(self, indexes : np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Load and read the images, and encode the labels\n        \"\"\"\n        images = np.zeros((self.batch_size, self.image_size[0], self.image_size[1], 3), dtype=np.float32)\n        labels = np.zeros((self.batch_size, self.classes_size), dtype=np.float32)\n        \n        urls = [self.items[k] for k in indexes]\n        lbls = [self.labels[k] for k in indexes]\n        \n        for idx, img_path in enumerate(urls):\n            img = preprocessing.image.load_img(img_path, target_size=self.image_size)\n            img_data = preprocessing.image.img_to_array(img)\n            img_data = preprocess_input(img_data)\n            lbl_data = lbls[idx]\n            images[idx, :] = img_data\n            labels[idx] = self.encode_label(lbl_data)\n        \n        return images, labels\n    \n    def on_epoch_end(self):\n        self.indexes = np.arange(self.items_size)\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n    \n    def __len__(self):\n        return int(np.floor(self.items_size / self.batch_size))\n    \n    def __getitem__(self, index):\n        indexes = self.indexes[index * self.batch_size:(index+1) * self.batch_size]\n        X, y = self.load_urls(indexes)\n        return X, y","metadata":{"execution":{"iopub.status.busy":"2022-08-20T14:26:11.050017Z","iopub.execute_input":"2022-08-20T14:26:11.051228Z","iopub.status.idle":"2022-08-20T14:26:11.068130Z","shell.execute_reply.started":"2022-08-20T14:26:11.051186Z","shell.execute_reply":"2022-08-20T14:26:11.066517Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Visualization class :\n---\nThis class presents some functions that helps to plot images with their labels in a grid.","metadata":{}},{"cell_type":"code","source":"class Ploter:\n    @classmethod\n    def ceildiv(cls, a, b):\n        return -(-a // b)\n\n    @classmethod\n    def plots_from_files(cls, img_paths, figsize=(10,5), rows=1, titles=None, main_title=None):\n        \"\"\"Plots the images in a grid\"\"\"\n        f = plt.figure(figsize=figsize)\n        if main_title is not None: plt.suptitle(main_title, fontsize=10)\n        for i in range(len(img_paths)):\n            sp = f.add_subplot(rows, Ploter.ceildiv(len(img_paths), rows), i+1)\n            sp.axis('Off')\n            if titles is not None: sp.set_title(titles[i], fontsize=16)\n            img = plt.imread(img_paths[i])\n            plt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T14:26:11.453058Z","iopub.execute_input":"2022-08-20T14:26:11.453928Z","iopub.status.idle":"2022-08-20T14:26:11.464069Z","shell.execute_reply.started":"2022-08-20T14:26:11.453880Z","shell.execute_reply":"2022-08-20T14:26:11.462488Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Dataset builder :\n---\nThis class uses the data generator, It takes the training, validation and testing sets. This class is what we feed to the neural network.","metadata":{}},{"cell_type":"code","source":"class DataBunch():\n    \"\"\"\n    An image data bunch \n    \n    Attributes\n    ----------\n    classes_size : \n        The size of the word embeddings\n    train_data, validation_data, test_data :\n        The dataset generated by the class ImageGenerator\n    \"\"\"\n    def __init__(self, classes_size : int, train_data : ImageGenerator, validation_data=None, test_data=None):\n        self.cls_size = classes_size\n        self.train_data = train_data\n        self.validation_data = validation_data\n        self.test_data = test_data\n    \n    def show_bunch(self, get_title, rows=3, figsize=(7, 6), **kwargs):\n        \"\"\"Show a bunch of images from the dataset\"\"\"\n        imspaths = np.random.choice(self.train_data.items, 9)\n        titles = [get_title(p) for p in imspaths]\n        \n        Ploter.plots_from_files(imspaths, figsize, rows, titles)\n       \n    @property\n    def classes_size(self):\n        return self.cls_size","metadata":{"execution":{"iopub.status.busy":"2022-08-20T14:26:11.860838Z","iopub.execute_input":"2022-08-20T14:26:11.861204Z","iopub.status.idle":"2022-08-20T14:26:11.869788Z","shell.execute_reply.started":"2022-08-20T14:26:11.861174Z","shell.execute_reply":"2022-08-20T14:26:11.868378Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Building the mapping model architecture :\n---","metadata":{}},{"cell_type":"code","source":"class Learner():\n    \"\"\"Base learner object\"\"\"\n    def __init__(self):\n        pass\n            \n    @classmethod\n    def freeze(cls, model, limit=None):\n        \"\"\"freeze all layers of the model (from left to right)\"\"\"\n        # handle negative indices\n        if limit != None and limit < -1:\n            limit += len(model.layers) \n        # loop for all valid indices and mark the corresponding layer\n        for index, layer in enumerate(model.layers):\n            if limit != None and index > limit:\n                break\n            layer.trainable = False\n\n    @classmethod\n    def unfreeze(cls, model, limit=None):\n        \"\"\"unfreeze all layers of the model up to the given layer index (from right to left)\"\"\"\n        # handle negative indices\n        if limit != None and limit < -1:\n            limit += len(model.layers)\n        for index, layer in enumerate(model.layers):\n            if limit != None and index < limit:\n                continue\n            layer.trainable = True","metadata":{"execution":{"iopub.status.busy":"2022-08-20T14:26:12.248998Z","iopub.execute_input":"2022-08-20T14:26:12.249659Z","iopub.status.idle":"2022-08-20T14:26:12.261655Z","shell.execute_reply.started":"2022-08-20T14:26:12.249628Z","shell.execute_reply":"2022-08-20T14:26:12.259497Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class ZeroShotLearner(Learner):\n    \"\"\"\n    Zero shot learner\n    \n    Attributes\n    ----------\n    data :\n        Data generated by The ImageGenerator class, It can train, validation or test data\n    loss :\n        The loss function to train the neural networks\n    metrics :\n        List of metrics to observe the performance of the model while training\n    \"\"\"\n    \"\"\"\"\"\"\n    def __init__(self, data, loss=tf.keras.losses.CosineSimilarity(axis=1), metrics=['accuracy']):\n        self.data = data\n        self.model = self.create_model()\n        adam = Adam(learning_rate=0.001, epsilon=0.01, decay=0.0001)\n        self.model.compile(adam, loss, metrics)\n        \n    \n    def create_model(self):\n        base_model = ResNet50(weights='imagenet')\n        Learner.freeze(base_model, -3)\n        \n        x = base_model.layers[-3].output          # shape = (bs=None, 7, 7, 2048)\n        x = Dropout(rate=0.3)(x)                  # shape = (bs=None, 7, 7, 2048)\n        x = GlobalAveragePooling2D()(x)           # shape = (bs=None, 2048)\n        x = Dense(1024, activation='relu')(x)     # shape = (bs=None, 1024)\n        x = BatchNormalization()(x)\n        y = Dense(self.data.classes_size, activation='linear')(x)\n         \n        return Model(inputs=base_model.input, outputs=y)\n        \n    def fit(self, epochs=10):\n        history = self.model.fit(self.data.train_data, validation_data=self.data.validation_data, epochs=epochs)\n        return history\n    \n    def predict_on_one_sample(self, image_path : str, solver : Solver) -> List[str]:\n        \"\"\"\n        Predict the label of the image given its path and using the solver\n        to get the nearest labels\n        \"\"\"\n        img = preprocessing.image.load_img(image_path, target_size=(224, 224))\n        img_data = preprocessing.image.img_to_array(img)\n        img_data = preprocess_input(img_data)\n        vec = self.model.predict(img_data[None])\n        totest = torch.FloatTensor(vec.reshape(-1))\n        return solver.get_nearest_embedding_of(totest)[0][0]\n    \n    def predict_on_samples(self, image_paths : List[str], solver : Solver) -> List[List[str]]:\n        \"\"\"\n        Predict the label of the images given their path and using the solver\n        to get the nearest labels\n        \"\"\"\n        y_pred = []\n        for i in tqdm(range(len(image_paths))):\n            img = preprocessing.image.load_img(image_paths[i], target_size=(224, 224))\n            img_data = preprocessing.image.img_to_array(img)\n            img_data = preprocess_input(img_data)\n            vec = self.model.predict(img_data[None])\n            totest = torch.FloatTensor(vec.reshape(-1))\n            y_pred.append(solver.get_nearest_embedding_of(totest)[0][0])\n        return y_pred\n    \n    def save_model(self, model_name : str):\n        self.model.save(model_name+\".model\", save_format=\"h5\")","metadata":{"execution":{"iopub.status.busy":"2022-08-20T14:26:27.238527Z","iopub.execute_input":"2022-08-20T14:26:27.239159Z","iopub.status.idle":"2022-08-20T14:26:27.268334Z","shell.execute_reply.started":"2022-08-20T14:26:27.239113Z","shell.execute_reply":"2022-08-20T14:26:27.266870Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## A solver that helps to give predictions","metadata":{}},{"cell_type":"code","source":"class EmbeddingsLoader:\n\n    def __init__(self, filename : str):\n\n        self.file = filename\n        self.embeddings = {}\n\n        self._load_file()\n\n    def _load_file(self):\n        try:\n            with open(self.file, \"r\") as f:\n                lines = f.readlines()\n                \n            for line in lines:\n                data = line.split(\",\")\n                self.embeddings[data[0]] = torch.FloatTensor(list(map(float, data[1:])))\n\n        except IOError as e:\n            raise IOError(f\"No file {self.file}\")\n\nclass SimilarityCompute(EmbeddingsLoader):\n\n    def __init__(self, embeddings):\n        super(SimilarityCompute, self).__init__(embeddings)\n\n\n    def compute_sim(self):\n        \"\"\" compute cosine similarity between all vectors \"\"\"\n        if len(self.embeddings) == 0:\n            raise Exception(\"Tags not converted yet !\")\n\n        logging.info(\"Computing cosine similarity, this could take some time...\")\n\n        if self.cosine_sim_matrix is None:\n            n_tokens = len(self.embeddings)\n            self.cosine_sim_matrix = [[1 for j in range(n_tokens)] for i in range(n_tokens)]\n\n        for j, vector in tqdm(enumerate(self.embeddings), total = len(self.embeddings)):\n\n            for i, other_vector in enumerate(self.embeddings):\n\n                if i == j:\n                    continue\n\n                cos = torch.nn.CosineSimilarity(dim=0)\n                similarity = cos(vector[1], other_vector[1])\n\n                self.cosine_sim_matrix[i][j] = similarity\n                self.cosine_sim_matrix[j][i] = similarity\n\n    def export_sim_matrix(self, filename):\n        if self.cosine_sim_matrix == None:\n            self.compute_sim()\n        \n        try:\n            f = open(filename, \"w\")\n        except OSError:\n            raise OSError(\"Could not open file\")\n\n        with f:\n            print(\"/\", \",\".join([tag[0] for tag in self.embeddings]), sep = \",\", file = f)\n\n            for j, tag_y in enumerate(self.embeddings):\n                print(tag_y[0], \",\".join( [str(round(float(self.cosine_sim_matrix[j][i]), 3)) for i in range(len(self.embeddings))]), sep = \",\", file = f)\n\n    def sim_between(self, token1, token2):\n        index1, v1 = [(i, v[1]) for i, v in enumerate(self.embeddings) if v[0] == token1][0]\n        index2, v2 = [(i, v[1]) for i, v in enumerate(self.embeddings) if v[0] == token2][0]\n\n        if self.cosine_sim_matrix is None:\n            n_tokens = len(self.embeddings)\n            self.cosine_sim_matrix = [[1 for j in range(n_tokens)] for i in range(n_tokens)]\n\n        if self.cosine_sim_matrix[index1][index2] == 0 or self.cosine_sim_matrix[index2][index1]:\n            cos = torch.nn.CosineSimilarity(dim=0)\n            similarity = cos(v1, v2)\n\n            self.cosine_sim_matrix[index1][index2] = similarity\n            self.cosine_sim_matrix[index2][index1] = similarity\n\n        return self.cosine_sim_matrix[index1][index2]\n\nclass Solver(EmbeddingsLoader):\n\n    def __init__(self, embeddings, nb_predictions):\n        super(Solver, self).__init__(embeddings)\n        self.nb = nb_predictions\n\n    def get_nearest_embedding_of(self, embedding):\n\n        if self.nb > len(self.embeddings):\n            raise Exception(\"nb too high, not enough token\")\n\n        nearest = []\n        for tag, e in self.embeddings.items():\n\n            cos = torch.nn.CosineSimilarity(dim=0)\n            similarity = cos(embedding, e)\n\n            nearest.append((tag, similarity))\n        \n        nearest.sort(key = lambda tup : tup[1])\n        return nearest[-1:-self.nb-1:-1]","metadata":{"execution":{"iopub.status.busy":"2022-08-20T14:26:21.735964Z","iopub.execute_input":"2022-08-20T14:26:21.736726Z","iopub.status.idle":"2022-08-20T14:26:21.760487Z","shell.execute_reply.started":"2022-08-20T14:26:21.736693Z","shell.execute_reply":"2022-08-20T14:26:21.758408Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Building a super class handler :\n---\nThis class presents different functions that are useful when working with super classes.","metadata":{}},{"cell_type":"code","source":"class SuperClassHandler:\n    def __init__(self, super_class_csv : pd.DataFrame):\n        self.super_class_df = pd.read_csv(super_class_csv)\n        self.super_classes = self.super_class_df.iloc[:, 0].tolist()\n    \n    def get_classes(self, super_class : str) -> list:\n        \"\"\"Take a super class as parameter and returns its correspondent classes\"\"\"\n        try:\n            return [i.replace(\"_\", \" \").lower() for i in self.super_class_df[self.super_class_df.iloc[:, 0] == super_class].to_numpy().tolist()[0][1:] if not(pd.isnull(i)) == True]\n        except:\n            print('Failed to find the super class')\n        \n    def get_super_class(self, class_name : str) -> str:\n        \"\"\"return the super class of the class given in the parameter\"\"\"\n        for super_class in self.super_classes:\n            if class_name.replace(\"_\", \" \").lower() in self.get_classes(super_class):\n                return super_class\n        return None\n    \n    def get_super_classes(self) -> List[str]:\n        \"\"\"returns a list containing all super classes\"\"\"\n        return self.super_classes","metadata":{"execution":{"iopub.status.busy":"2022-08-20T14:26:32.662276Z","iopub.execute_input":"2022-08-20T14:26:32.662824Z","iopub.status.idle":"2022-08-20T14:26:32.681531Z","shell.execute_reply.started":"2022-08-20T14:26:32.662779Z","shell.execute_reply":"2022-08-20T14:26:32.680045Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Building a super class balancer :\n---\nThis class allows to balance a given dataset and equally distributing data on each class.","metadata":{}},{"cell_type":"code","source":"class SuperClassBalancer:\n    def __init__(self, path : str, super_classes : List[str]):\n        self.util = Util(path)\n        self.super_classes = super_classes\n    \n    def balance(self, samples_number : int, labels):\n        \"\"\"Return indexes equaly distributed between each label\"\"\"\n        self.balanced_indexes = dict()\n        for super_class in tqdm(self.super_classes):\n            self.balanced_indexes[super_class] = random.Random(0).sample(self.util.get_indexes(super_class, labels), samples_number)\n        \n        return list(itertools.chain.from_iterable(list(self.balanced_indexes.values())))\n            \n    def add_value_label(self, x_list, y_list):\n        \"\"\"allows to write text on a given plot\"\"\"\n        for i in range(len(x_list)):\n            plt.text(i, y_list[i], y_list[i], ha=\"center\", fontweight='bold', fontsize=\"medium\")\n    \n    def plot_distribution(self):\n        \"\"\"Plot a histogram presenting the number of samples on each class\"\"\"\n        super_class_distribution = dict()\n        for super_class in tqdm(self.super_classes):\n            super_class_distribution[super_class] = len(self.balanced_indexes[super_class])\n        \n        plt.figure(figsize=(20,8))\n        width = 1.0    \n        plt.bar(super_class_distribution.keys(), super_class_distribution.values(), align='center', width=0.5, color='g')\n        self.add_value_label(list(super_class_distribution.keys()), list(super_class_distribution.values()))","metadata":{"execution":{"iopub.status.busy":"2022-08-20T14:26:33.956538Z","iopub.execute_input":"2022-08-20T14:26:33.957150Z","iopub.status.idle":"2022-08-20T14:26:33.978351Z","shell.execute_reply.started":"2022-08-20T14:26:33.957091Z","shell.execute_reply":"2022-08-20T14:26:33.976848Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Building a super class classifier:\n---\nThis class presents a classifier that classify super classes. It uses the second approach explained in the report","metadata":{}},{"cell_type":"code","source":"class SuperClassClassifier:\n    def __init__(self, super_class_handler):\n        self.model = ResNet50(weights='imagenet')\n        self.super_class_handler = super_class_handler\n    \n    def predict(self, image_path):\n        img = preprocessing.image.load_img(image_path, target_size=(224, 224))\n        img_data = preprocessing.image.img_to_array(img)\n        img_data = preprocess_input(img_data)\n        prediction = self.model.predict(img_data[None])\n        label = tf.keras.applications.imagenet_utils.decode_predictions(prediction) \n        return self.super_class_handler.get_super_class(label[0][0][1])\n    \n    def predict_on_samples(self, image_paths):\n        predictions = []\n        for i in tqdm(range(len(image_paths))):\n            img = preprocessing.image.load_img(image_paths[i], target_size=(224, 224))\n            img_data = preprocessing.image.img_to_array(img)\n            img_data = preprocess_input(img_data)\n            prediction = self.model.predict(img_data[None])\n            label = tf.keras.applications.imagenet_utils.decode_predictions(prediction) \n            predictions.append(self.super_class_handler.get_super_class(label[0][0][1]))\n        return predictions","metadata":{"execution":{"iopub.status.busy":"2022-08-20T14:26:35.718120Z","iopub.execute_input":"2022-08-20T14:26:35.718555Z","iopub.status.idle":"2022-08-20T14:26:35.730862Z","shell.execute_reply.started":"2022-08-20T14:26:35.718525Z","shell.execute_reply":"2022-08-20T14:26:35.728699Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Building a util class :\n---\nThis class presents different functions that are useful in data manipulation, testing parts...","metadata":{}},{"cell_type":"code","source":"class Util:\n    def __init__(self, path):\n        self.path = path\n        self.directory_classes = [os.listdir(self.get_path(os.listdir(self.path)[i])) for i in range(len(os.listdir(self.path)))]\n          \n    def get_path(self, _type : str):\n        return '../input/996-imagenet/'+_type+'/data1/'+_type\n    \n    def get_data(self) -> Tuple[List[str], List[str]]:\n        \"\"\"\n        Read images and labels from a folder\n        \"\"\"\n        images_path = []\n        labels = []\n        absolute_path = os.listdir(self.path)\n        for i in range(len(self.directory_classes)):\n            for current_class in tqdm(self.directory_classes[i]):\n                pth = os.path.join(self.get_path(absolute_path[i]), current_class)\n                for dirname, _, filenames in os.walk(pth):\n                    for file in filenames:\n                        images_path.append(os.path.join(dirname, file))\n                        labels.append(current_class)\n        \n        return images_path, current_class\n    \n    def get_all_classes(self) -> List[str]:\n        \"\"\"\n        Returns the different classes of the images\n        \"\"\"\n        classes = list(itertools.chain.from_iterable(list(self.directory_classes)))\n        return [x.lower() for x in classes]\n    \n    def get_indexes(self, class_name : str, labels : List[str]) -> List[int]:\n        \"\"\"return indexes of class_name in labels\"\"\"\n        try:\n            return [index for index in range(len(labels)) if labels[index] == class_name]\n        except ValueError:\n            print(\"That item does not exist\")\n    \n    def get_label_from_fname(self, fname:str):\n        \"\"\"\n        Returns the name of an image's class given its path\n        \"\"\"\n        fname = fname.split('/')\n\n        index = fname.index('data1') + 2\n        word = fname[index]\n        return word\n    \n    def get_mean_visual_embeddings(self, model) -> dict:\n        \"\"\"computes the mean of the visual embeddigns of each class in the dataset\"\"\"\n        features = dict()\n        absolute_path = os.listdir(self.path)\n        \n        for i in range(len(self.directory_classes)):\n            for clse in tqdm(self.directory_classes[i]):\n                mean_list = []\n                pth = os.path.join(self.get_path(absolute_path[i]), clse)\n                for dirname, _, filenames in os.walk(pth):\n                    for file in filenames:\n                        image = preprocessing.image.load_img(os.path.join(dirname, file), target_size=(224, 224))\n                        image = preprocessing.image.img_to_array(image)\n                        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n                        image = preprocess_input(image)\n                        mean_list.append(model.predict(image, verbose=0))\n\n                features[clse] = np.mean(mean_list, axis=0) \n        \n        return features\n                                               \n    def get_mean_textual_embeddings(self, model) -> dict:\n        \"\"\"computes the mean of the textual embeddigns generated\n           by the mapping model given in the parameter of each class in the dataset\"\"\"\n        features = dict()\n        absolute_path = os.listdir(self.path)\n\n        for i in range(len(self.directory_classes)):\n            for clse in tqdm(self.directory_classes[i]):\n                tensor_list = []\n                pth = os.path.join(self.get_path(absolute_path[i]), clse)\n                for dirname, _, filenames in os.walk(pth):\n                    for file in filenames:\n                        img = preprocessing.image.load_img(os.path.join(dirname, file), target_size=(224, 224))\n                        img_data = preprocessing.image.img_to_array(img)\n                        img_data = preprocess_input(img_data)\n                        vec = model.predict(img_data[None])\n                        totest = torch.FloatTensor(vec.reshape(-1))\n                        tensor_list.append(totest)\n                features[clse] = torch.mean(torch.stack(tensor_list), dim=0)\n\n        return features\n    \n    def get_visual_embeddings(self, img_path : str) -> np.ndarray:\n        \"\"\"\n        Returns the visual embeddings of an image\n        given its path using the ResNet50 model\n        \"\"\"\n        model = ResNet50(weights=\"imagenet\", include_top=True)\n        model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n        \n        image = preprocessing.image.load_img(img_path, target_size=(224, 224))\n        image = preprocessing.image.img_to_array(image)\n        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n        image = preprocess_input(image)\n\n        return model.predict(image, verbose=0)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T14:26:37.412879Z","iopub.execute_input":"2022-08-20T14:26:37.413627Z","iopub.status.idle":"2022-08-20T14:26:37.445274Z","shell.execute_reply.started":"2022-08-20T14:26:37.413596Z","shell.execute_reply":"2022-08-20T14:26:37.443346Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"pickle.dump(features, open(\"visual_features_25.pkl\", \"wb\"))","metadata":{"execution":{"iopub.status.busy":"2022-08-16T21:24:56.401259Z","iopub.execute_input":"2022-08-16T21:24:56.401630Z","iopub.status.idle":"2022-08-16T21:24:56.977127Z","shell.execute_reply.started":"2022-08-16T21:24:56.401599Z","shell.execute_reply":"2022-08-16T21:24:56.976118Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"with open('../input/utils996/img_paths.pkl', 'rb') as f:\n    img_paths = pickle.load(f)\n\nwith open('../input/true-labels-super/true_labels_super.pkl', 'rb') as f:    \n    true_labels_super = pickle.load(f)\n    \nwith open('../input/utils996/true_labels.pkl', 'rb') as f:    \n    true_labels = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T14:29:31.434544Z","iopub.execute_input":"2022-08-20T14:29:31.434907Z","iopub.status.idle":"2022-08-20T14:29:31.988475Z","shell.execute_reply.started":"2022-08-20T14:29:31.434878Z","shell.execute_reply":"2022-08-20T14:29:31.987155Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Initializing the super class handler :","metadata":{}},{"cell_type":"code","source":"utils = Util(Variables.path)\nsc_handler = SuperClassHandler('../input/super-classes/super_classes.csv')\nsc_balancer = SuperClassBalancer(Variables.path, sc_handler.get_super_classes())\nsc_classifier = SuperClassClassifier(sc_handler)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T14:28:44.207438Z","iopub.execute_input":"2022-08-20T14:28:44.207947Z","iopub.status.idle":"2022-08-20T14:28:57.271890Z","shell.execute_reply.started":"2022-08-20T14:28:44.207873Z","shell.execute_reply":"2022-08-20T14:28:57.270378Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Training the mapping model on different super classes :\n---\nWe can choose to train the full ImageNet dataset or just data from a specific super class as It is shown below","metadata":{}},{"cell_type":"markdown","source":"##### Training the mapping model on the *outdoor* super class :","metadata":{}},{"cell_type":"code","source":"indexes = utils.get_indexes('outdoor', true_labels_super)\nindexes = random.Random(0).sample(indexes, 20000)\n\noutdoor_img_paths = [img_paths[i] for i in indexes]\noutdoor_true_labels = [true_labels[i] for i in indexes]\n\nbatch_size = 64\nclass_size = 300\n\ntrain_data = ImageGenerator(Variables.embeddings_path, outdoor_img_paths, outdoor_true_labels, class_size, batch_size)\nprint('Training set has %d batches of size %d' % (len(train_gen), batch_size))","metadata":{"execution":{"iopub.status.busy":"2022-08-14T14:54:02.749242Z","iopub.execute_input":"2022-08-14T14:54:02.749967Z","iopub.status.idle":"2022-08-14T14:54:02.878538Z","shell.execute_reply.started":"2022-08-14T14:54:02.749929Z","shell.execute_reply":"2022-08-14T14:54:02.877407Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"data = DataBunch(class_size, train_data)\ndata.show_bunch(utils.get_label_from_fname)","metadata":{"execution":{"iopub.status.busy":"2022-08-14T14:54:07.224705Z","iopub.execute_input":"2022-08-14T14:54:07.225059Z","iopub.status.idle":"2022-08-14T14:54:07.230234Z","shell.execute_reply.started":"2022-08-14T14:54:07.225027Z","shell.execute_reply":"2022-08-14T14:54:07.229037Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"### Training and saving the model:","metadata":{}},{"cell_type":"code","source":"learner = ZeroShotLearner(data)\nhistory = learner.fit()\nlearner.save_model('outdoor_model')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing the general mapping model :\n    * Testing set : 20 000 images of shape (224, 224, 3)","metadata":{}},{"cell_type":"code","source":"mapping_model = load_model('../input/mod996/resnet50-996-classes.model')\nmap_solver = Solver('../input/996-embeddings/996-embeddings.csv', 10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = random.Random(0).sample(img_paths, 20000)\ny = random.Random(0).sample(true_labels, 20000)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T11:01:09.839618Z","iopub.execute_input":"2022-08-06T11:01:09.840288Z","iopub.status.idle":"2022-08-06T11:01:09.952006Z","shell.execute_reply.started":"2022-08-06T11:01:09.840250Z","shell.execute_reply":"2022-08-06T11:01:09.951072Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"predictions = []\nfor i in tqdm(range(len(X))):\n    img = preprocessing.image.load_img(X[i], target_size=(224, 224))\n    img_data = preprocessing.image.img_to_array(img)\n    img_data = preprocess_input(img_data)\n\n    vec = mapping_model.predict(img_data[None])\n    totest = torch.FloatTensor(vec.reshape(-1))\n\n    predictions.append([i[0].lower() for i in map_solver.get_nearest_embedding_of(totest)])","metadata":{"execution":{"iopub.status.busy":"2022-08-06T11:01:43.386951Z","iopub.execute_input":"2022-08-06T11:01:43.387305Z","iopub.status.idle":"2022-08-06T11:41:11.851831Z","shell.execute_reply.started":"2022-08-06T11:01:43.387275Z","shell.execute_reply":"2022-08-06T11:41:11.850124Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"def top_k_accuracy(y_true, y_pred, k) -> float:\n    n = len(y_true)\n    true_pred = 0\n    for i in range(n):\n        if y_true[i] in y_pred[i][:k]:\n            true_pred += 1\n    return true_pred/n","metadata":{"execution":{"iopub.status.busy":"2022-08-06T11:41:18.476220Z","iopub.execute_input":"2022-08-06T11:41:18.476820Z","iopub.status.idle":"2022-08-06T11:41:18.482871Z","shell.execute_reply.started":"2022-08-06T11:41:18.476784Z","shell.execute_reply":"2022-08-06T11:41:18.481876Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"accuracy = []\nfor i in range(1, 11):\n    accuracy.append(top_k_accuracy(y, predictions, i) * 100)\n    print(f\"top-{i} : {top_k_accuracy(y, predictions, i)}\")\n\nx = np.arange(10)\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"top k\")\nplt.plot(x, accuracy, 'o')","metadata":{"execution":{"iopub.status.busy":"2022-08-06T11:41:19.637789Z","iopub.execute_input":"2022-08-06T11:41:19.638499Z","iopub.status.idle":"2022-08-06T11:41:20.157029Z","shell.execute_reply.started":"2022-08-06T11:41:19.638458Z","shell.execute_reply":"2022-08-06T11:41:20.156187Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"## Testing the super class classifiers :\n---","metadata":{}},{"cell_type":"markdown","source":"We start by picking 400 images from each super class. We can do this thanks to the super class balancer.","metadata":{}},{"cell_type":"code","source":"indexes = sc_balancer.balance(400, true_labels_super)\nsc_balancer.plot_distribution()","metadata":{"execution":{"iopub.status.busy":"2022-08-08T17:57:02.492414Z","iopub.execute_input":"2022-08-08T17:57:02.492861Z","iopub.status.idle":"2022-08-08T17:57:02.880185Z","shell.execute_reply.started":"2022-08-08T17:57:02.492829Z","shell.execute_reply":"2022-08-08T17:57:02.878739Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"X_test = [img_paths[i] for i in indexes]\ny_test = [true_labels_super[i] for i in indexes]","metadata":{"execution":{"iopub.status.busy":"2022-08-08T17:57:10.908562Z","iopub.execute_input":"2022-08-08T17:57:10.909977Z","iopub.status.idle":"2022-08-08T17:57:10.919926Z","shell.execute_reply.started":"2022-08-08T17:57:10.909930Z","shell.execute_reply":"2022-08-08T17:57:10.918270Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"#### Shuffling the dataset :","metadata":{}},{"cell_type":"code","source":"random.Random(0).shuffle(X_test)\nrandom.Random(0).shuffle(y_test)","metadata":{"execution":{"iopub.status.busy":"2022-08-08T17:57:12.261872Z","iopub.execute_input":"2022-08-08T17:57:12.262499Z","iopub.status.idle":"2022-08-08T17:57:12.283579Z","shell.execute_reply.started":"2022-08-08T17:57:12.262456Z","shell.execute_reply":"2022-08-08T17:57:12.282433Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### Testing the first classifier : (the second approach presented in the report)","metadata":{}},{"cell_type":"code","source":"y_pred = sc_classifier.predict_on_samples(X_test)\nencoder = LabelEncoder()\ny_test_enc = encoder.fit_transform(y_test)\ny_pred_enc = encoder.transform(y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-08-08T18:11:09.639900Z","iopub.execute_input":"2022-08-08T18:11:09.640313Z","iopub.status.idle":"2022-08-08T18:16:16.529613Z","shell.execute_reply.started":"2022-08-08T18:11:09.640279Z","shell.execute_reply":"2022-08-08T18:16:16.528161Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy score :","metadata":{}},{"cell_type":"code","source":"print(accuracy_score(y_test_enc, y_pred_enc))","metadata":{"execution":{"iopub.status.busy":"2022-08-08T18:16:24.050094Z","iopub.execute_input":"2022-08-08T18:16:24.051561Z","iopub.status.idle":"2022-08-08T18:16:24.060415Z","shell.execute_reply.started":"2022-08-08T18:16:24.051517Z","shell.execute_reply":"2022-08-08T18:16:24.058943Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"### Ploting the confusion matrix :","metadata":{}},{"cell_type":"code","source":"M = confusion_matrix(y_test_enc, y_pred_enc)\ncmn = M.astype('float') / M.sum(axis=1)[:, np.newaxis]\n\nplt.figure(figsize = (16, 7))\n\nconfusion_matrix_plot = sns.heatmap(cmn, cmap='RdYlGn', fmt='.1%', annot=True, xticklabels=encoder.classes_, yticklabels=encoder.classes_)","metadata":{"execution":{"iopub.status.busy":"2022-08-08T18:16:30.886937Z","iopub.execute_input":"2022-08-08T18:16:30.887459Z","iopub.status.idle":"2022-08-08T18:16:32.345044Z","shell.execute_reply.started":"2022-08-08T18:16:30.887415Z","shell.execute_reply":"2022-08-08T18:16:32.343744Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"### Testing the second classifier : (the first approach presented in the report)","metadata":{}},{"cell_type":"code","source":"classifier = load_model('../input/super-class-classifier/resnet-11-super-classes.model')\nclassif_solver = Solver('../input/embeddings-super-class/super_emb.csv', 1)","metadata":{"execution":{"iopub.status.busy":"2022-08-08T17:58:20.880608Z","iopub.execute_input":"2022-08-08T17:58:20.881431Z","iopub.status.idle":"2022-08-08T17:58:25.820345Z","shell.execute_reply.started":"2022-08-08T17:58:20.881386Z","shell.execute_reply":"2022-08-08T17:58:25.818998Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"predictions = []\nfor i in tqdm(range(len(X_test))):\n    img = preprocessing.image.load_img(X_test[i], target_size=(224, 224))\n    img_data = preprocessing.image.img_to_array(img)\n    img_data = preprocess_input(img_data)\n\n    vec = classifier.predict(img_data[None])\n    totest = torch.FloatTensor(vec.reshape(-1))\n\n    pred = classif_solver.get_nearest_embedding_of(totest)[0][0]\n    predictions.append(pred)","metadata":{"execution":{"iopub.status.busy":"2022-08-08T17:58:31.131054Z","iopub.execute_input":"2022-08-08T17:58:31.131738Z","iopub.status.idle":"2022-08-08T18:04:03.518219Z","shell.execute_reply.started":"2022-08-08T17:58:31.131653Z","shell.execute_reply":"2022-08-08T18:04:03.516804Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"pred_enc = encoder.transform(predictions)","metadata":{"execution":{"iopub.status.busy":"2022-08-08T18:04:18.634946Z","iopub.execute_input":"2022-08-08T18:04:18.635330Z","iopub.status.idle":"2022-08-08T18:04:18.649077Z","shell.execute_reply.started":"2022-08-08T18:04:18.635300Z","shell.execute_reply":"2022-08-08T18:04:18.647114Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy score :","metadata":{}},{"cell_type":"code","source":"print(accuracy_score(y_test_enc, pred_enc))","metadata":{"execution":{"iopub.status.busy":"2022-08-08T18:04:20.331752Z","iopub.execute_input":"2022-08-08T18:04:20.332232Z","iopub.status.idle":"2022-08-08T18:04:20.341261Z","shell.execute_reply.started":"2022-08-08T18:04:20.332201Z","shell.execute_reply":"2022-08-08T18:04:20.339732Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"### Confusion matrix :","metadata":{}},{"cell_type":"code","source":"M = confusion_matrix(y_test_enc, pred_enc)\ncmn = M.astype('float') / M.sum(axis=1)[:, np.newaxis]\n\nplt.figure(figsize = (16,7))\n\nconfusion_matrix_plot = sns.heatmap(cmn, cmap='RdYlGn', fmt='.1%', annot=True, xticklabels=encoder.classes_, yticklabels=encoder.classes_)","metadata":{"execution":{"iopub.status.busy":"2022-08-08T18:09:49.472424Z","iopub.execute_input":"2022-08-08T18:09:49.472865Z","iopub.status.idle":"2022-08-08T18:09:50.451996Z","shell.execute_reply.started":"2022-08-08T18:09:49.472830Z","shell.execute_reply":"2022-08-08T18:09:50.450693Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"## Testing the super class pipeline :","metadata":{}},{"cell_type":"code","source":"def super_class_test(super_class, classifier, sc_embeddings, mapping_model_path):\n    mapping_model = load_model(mapping_model_path)\n    indexes = utils.get_indexes(super_class, true_labels_super)\n\n    X = [img_paths[i] for i in indexes]\n    y = [true_labels[i] for i in indexes]\n\n    X_test = random.Random(0).sample(X, 1000)\n    y_test = random.Random(0).sample(y, 1000)\n    \n    predictions = []\n\n    for i in tqdm(range(len(X_test))):\n        classification_pred = classifier.predict(X_test[i])\n        if classification_pred == super_class:\n            img = preprocessing.image.load_img(X_test[i], target_size=(224, 224))\n            img_data = preprocessing.image.img_to_array(img)\n            img_data = preprocess_input(img_data)\n\n            vec = mapping_model.predict(img_data[None])\n            totest = torch.FloatTensor(vec.reshape(-1))\n\n            map_pred = [i[0].lower() for i in map_solver.get_nearest_embedding_of(totest)]\n            predictions.append(y_test[i] in map_pred)\n        \n    print(\"Classification accuracy : \", len(predictions)/1000)\n    print(\"Mapping model top-10 accuracy :\", sum(predictions)/1000)\n    \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-08-13T15:46:28.035427Z","iopub.execute_input":"2022-08-13T15:46:28.035897Z","iopub.status.idle":"2022-08-13T15:46:28.059721Z","shell.execute_reply.started":"2022-08-13T15:46:28.035859Z","shell.execute_reply":"2022-08-13T15:46:28.057202Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"predictions = super_class_test('small animal', sc_classifier, '../input/different-super-class-embeddings/small_animal_emb.csv', '../input/small-animal/resnet-small-animal.model')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = random.Random(0).sample(img_paths, 20000)\ny = random.Random(0).sample(true_labels, 20000)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T14:29:39.342599Z","iopub.execute_input":"2022-08-20T14:29:39.343017Z","iopub.status.idle":"2022-08-20T14:29:39.459708Z","shell.execute_reply.started":"2022-08-20T14:29:39.342953Z","shell.execute_reply":"2022-08-20T14:29:39.458448Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"super_class_embeddings = {\"objects\":\"../input/different-super-class-embeddings/objects_emb.csv\",\n                         \"small animal\":\"../input/different-super-class-embeddings/small_animal_emb.csv\",\n                         \"transport\":\"../input/different-super-class-embeddings/transport_emb.csv\",\n                         \"outdoor\":\"../input/different-super-class-embeddings/outdoor_emb.csv\",\n                         \"clothes\":\"../input/different-super-class-embeddings/clothes_emb.csv\",\n                         \"aquatic animal\":\"../input/different-super-class-embeddings/acquatic_animal_emb.csv\",\n                         \"insect\":\"../input/different-super-class-embeddings/insect_emb.csv\",\n                         \"food\":\"../input/different-super-class-embeddings/food_emb.csv\",\n                         \"plant\":\"../input/different-super-class-embeddings/plant_emb.csv\",\n                          \"animal\":\"../input/different-super-class-embeddings/animal_emb.csv\",\n                         \"person\":\"../input/different-super-class-embeddings/person_emb.csv\"}\n\nspecialized_models = {\"objects\":load_model(\"../input/objects/resnet-objects.model\"),\n                         \"small animal\":load_model(\"../input/small-animal/resnet-small-animal.model\"),\n                         \"transport\":load_model(\"../input/super-class-models/transport_model.model\"),\n                         \"outdoor\":load_model(\"../input/super-class-models/outdoor_model.model\"),\n                         \"clothes\":load_model(\"../input/super-class-models/clothes_model.model\"),\n                         \"aquatic animal\":load_model(\"../input/super-class-models/aquatic_animal_model.model\"),\n                         \"insect\":load_model(\"../input/super-class-models/insect_model.model\"),\n                         \"food\":load_model(\"../input/super-class-models/food_model.model\"),\n                         \"plant\":load_model(\"../input/super-class-models/plant_model.model\"),\n                          \"animal\":load_model(\"../input/super-class-models/animal_model.model\"),\n                         \"person\":load_model(\"../input/super-class-models/person_model.model\")}","metadata":{"execution":{"iopub.status.busy":"2022-08-20T14:51:01.807264Z","iopub.execute_input":"2022-08-20T14:51:01.807844Z","iopub.status.idle":"2022-08-20T14:51:39.392233Z","shell.execute_reply.started":"2022-08-20T14:51:01.807781Z","shell.execute_reply":"2022-08-20T14:51:39.390735Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Hybrid method for classification :\n---\n#### Combining the two classifiers : ","metadata":{}},{"cell_type":"code","source":"def hybrid_method_test(classifiers : list, general_model_path : str, X_test=None, y_test=None):\n    classif_solver = Solver('../input/embeddings-super-class/super_emb.csv', 1)\n    \n    general_mapping_model = load_model(general_model_path)\n    general_map_solver = Solver('../input/996-embeddings/996-embeddings.csv', 10)\n    \n    \n    \n    classifiers[1] = load_model(classifiers[1])\n    \n    predictions = []\n\n    for i in tqdm(range(len(X_test))):\n        img = preprocessing.image.load_img(X_test[i], target_size=(224, 224))\n        img_data = preprocessing.image.img_to_array(img)\n        img_data = preprocess_input(img_data)\n        \n        classification_vec = classifiers[1].predict(img_data[None])\n        \n        classifier1_pred = classifiers[0].predict(X_test[i])\n        classifier2_pred = classif_solver.get_nearest_embedding_of(torch.FloatTensor(classification_vec.reshape(-1)))[0][0]\n       \n\n        if classifier1_pred == classifier2_pred and classifier1_pred == 'person':\n            specialized_mapping_model = specialized_models[classifier1_pred]\n            vec = specialized_mapping_model.predict(img_data[None])\n            totest = torch.FloatTensor(vec.reshape(-1))\n            specialized_map_solver = Solver(super_class_embeddings[classifier1_pred], 1)\n\n            map_pred = [i[0].lower() for i in specialized_map_solver.get_nearest_embedding_of(totest)]\n            predictions.append(y_test[i] in map_pred)\n        \n        elif classifier1_pred == classifier2_pred:\n            specialized_mapping_model = specialized_models[classifier1_pred]\n            vec = specialized_mapping_model.predict(img_data[None])\n            totest = torch.FloatTensor(vec.reshape(-1))\n            specialized_map_solver = Solver(super_class_embeddings[classifier1_pred], 10)\n\n            map_pred = [i[0].lower() for i in specialized_map_solver.get_nearest_embedding_of(totest)]\n            predictions.append(y_test[i] in map_pred)\n        \n        else:\n            vec = general_mapping_model.predict(img_data[None])\n            totest = torch.FloatTensor(vec.reshape(-1))\n\n            map_pred = [i[0].lower() for i in general_map_solver.get_nearest_embedding_of(totest)]\n            predictions.append(y_test[i] in map_pred)\n    \n    print(\"Mapping model top-10 accuracy :\", sum(predictions)/len(predictions))\n    \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-08-20T14:57:15.535328Z","iopub.execute_input":"2022-08-20T14:57:15.535764Z","iopub.status.idle":"2022-08-20T14:57:15.556674Z","shell.execute_reply.started":"2022-08-20T14:57:15.535704Z","shell.execute_reply":"2022-08-20T14:57:15.555154Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"predictions = hybrid_method_test([sc_classifier, '../input/super-class-classifier/resnet-11-super-classes.model'], '../input/mod996/resnet50-996-classes.model', X, y)","metadata":{"execution":{"iopub.status.busy":"2022-08-20T14:57:16.707217Z","iopub.execute_input":"2022-08-20T14:57:16.707629Z","iopub.status.idle":"2022-08-20T16:23:52.372607Z","shell.execute_reply.started":"2022-08-20T14:57:16.707600Z","shell.execute_reply":"2022-08-20T16:23:52.371045Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"print(\"Top-10 Accuracy : \", 100*(sum(predictions)/20000))","metadata":{"execution":{"iopub.status.busy":"2022-08-20T16:24:17.263238Z","iopub.execute_input":"2022-08-20T16:24:17.263813Z","iopub.status.idle":"2022-08-20T16:24:17.284702Z","shell.execute_reply.started":"2022-08-20T16:24:17.263765Z","shell.execute_reply":"2022-08-20T16:24:17.283052Z"},"trusted":true},"execution_count":32,"outputs":[]}]}