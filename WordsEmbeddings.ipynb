{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordsEmbeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taedriel/ZSL-v2/blob/wordEmbedding/WordsEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BYxydB4dlTpi"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# %cd /content/drive/MyDrive/Kingston/ZSL-v2/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers wget tensorflow_datasets wikipedia deprecated vecto\n",
        "!mkdir temp"
      ],
      "metadata": {
        "id": "HEq14T6Nl535",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1874899-b582-4f73-86fa-89b33617eada"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (4.0.1)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.7/dist-packages (1.2.13)\n",
            "Requirement already satisfied: vecto in /usr/local/lib/python3.7/dist-packages (0.2.21)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (3.17.3)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (21.4.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.16.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.8.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.1.7)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (5.7.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.1.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.3.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated) (1.14.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from vecto) (1.4.1)\n",
            "Requirement already satisfied: system-query in /usr/local/lib/python3.7/dist-packages (from vecto) (0.2.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from vecto) (1.3.5)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.7/dist-packages (from vecto) (5.2.1.post0)\n",
            "Requirement already satisfied: brewer2mpl in /usr/local/lib/python3.7/dist-packages (from vecto) (1.4.1)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.7/dist-packages (from vecto) (3.38.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from vecto) (3.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from vecto) (1.0.2)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.7/dist-packages (from vecto) (3.7.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from vecto) (3.2.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->vecto) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->vecto) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->vecto) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->vecto) (2022.1)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2->vecto) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->vecto) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->vecto) (3.1.0)\n",
            "Requirement already satisfied: version-query==1.*,>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from system-query->vecto) (1.1.0)\n",
            "Requirement already satisfied: setuptools>=45.1 in /usr/local/lib/python3.7/dist-packages (from version-query==1.*,>=1.0.5->system-query->vecto) (57.4.0)\n",
            "Requirement already satisfied: semver~=2.9 in /usr/local/lib/python3.7/dist-packages (from version-query==1.*,>=1.0.5->system-query->vecto) (2.13.0)\n",
            "Requirement already satisfied: GitPython~=3.0 in /usr/local/lib/python3.7/dist-packages (from version-query==1.*,>=1.0.5->system-query->vecto) (3.1.27)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython~=3.0->version-query==1.*,>=1.0.5->system-query->vecto) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython~=3.0->version-query==1.*,>=1.0.5->system-query->vecto) (5.0.0)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.7/dist-packages (from tables->vecto) (2.8.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.56.1)\n",
            "mkdir: cannot create directory ‘temp’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import gc\n",
        "import wikipedia\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level = logging.INFO, filename = \"BERT.log\" )\n",
        "# logging.basicConfig(level = logging.INFO)\n",
        "\n",
        "from tqdm import tqdm\n",
        "from deprecated import deprecated\n",
        "from typing import List, Tuple\n",
        "from os.path import exists\n"
      ],
      "metadata": {
        "id": "spXWSv2iW9i5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WordToVector:\n",
        "\n",
        "    def __init__(self, list_tags : List[str] = []):\n",
        "        self.list_tags = list_tags\n",
        "        self.embeddings = []\n",
        "\n",
        "    def export(self, filename):\n",
        "        return NotImplementedError\n",
        "\n",
        "    def convert(self):\n",
        "        return NotImplementedError\n",
        "\n",
        "\n",
        "class BERTModel(WordToVector):\n",
        "\n",
        "    temp_dir = \"./temp\"\n",
        "\n",
        "    def __init__(self, list_tag : List[str] = [], big: bool = False):\n",
        "        super(BERTModel, self).__init__(list_tag)\n",
        "        self.list_articles = {}\n",
        "\n",
        "        self.model_size = \"bert-large-uncased\" if big else \"bert-base-uncased\"\n",
        "        self.cosine_sim_matrix = None\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.model_size, padding=True, truncation=True,)\n",
        "        self.model = BertModel.from_pretrained(self.model_size, output_hidden_states = True)\n",
        "\n",
        "        self.merging_strategy = Sum4LastLayers()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "    def export(self, filename):\n",
        "        if len(self.embeddings) == 0:\n",
        "            raise Exception(\"Tags not converted yet !\")\n",
        "        \n",
        "        try:\n",
        "            f = open(filename, \"w\")\n",
        "        except OSError:\n",
        "            raise OSError(\"Could not open file\")\n",
        "\n",
        "        with f:\n",
        "            for embedding in self.embeddings:\n",
        "                line = \",\".join(list(map(lambda x: str(float(x)), embedding[1])))\n",
        "                print(embedding[0], \",\", line, sep=\"\", file=f)\n",
        "\n",
        "    @deprecated(reason = \"use constructor after manual import of the file is preffered\")\n",
        "    def import_tag_list(self, filename):\n",
        "        self.embeddings.clear()\n",
        "        self.cosine_sim_matrix = None\n",
        "\n",
        "        try:\n",
        "            f = open(filename, \"r\")\n",
        "        except OSError:\n",
        "            return OSError(\"Could not open file\")\n",
        "\n",
        "        with f:\n",
        "            data = f.read().split(\"\\n\")\n",
        "            for item in data:\n",
        "                if item not in self.list_tags and str.strip(item) != \"\":\n",
        "                    self.list_tags.append(item)\n",
        "            print(self.list_tags)\n",
        "            logging.info(f\"Import finished : {len(self.list_tags)} elements imported.\")\n",
        "\n",
        "    def _retrieve_summary(self, title, depth : int, include_ambiguous : bool = True) -> Tuple[str, str]:\n",
        "            if depth == 0:\n",
        "                logging.warning(f\"Maximum recursion depth for {title}\")\n",
        "                return None\n",
        "            \n",
        "            filename = BERTModel.temp_dir + \"/\" + title.replace(\" \", \"_\")\n",
        "            page = None\n",
        "\n",
        "            if exists(filename):\n",
        "                with open(filename, \"r\") as f:\n",
        "                    return (title, f.read())\n",
        "\n",
        "            search_result = wikipedia.search(title, suggestion = False)\n",
        "            try:\n",
        "                summary = wikipedia.page(search_result[0]).summary\n",
        "                with open(filename, \"w\") as f:\n",
        "                    logging.info(f\"Saving {title}\")\n",
        "                    print(summary, file=f)\n",
        "                return (search_result[0], summary)\n",
        "\n",
        "            except wikipedia.PageError as e:\n",
        "                logging.warning(f\"{title} misspelled or article missing.\")\n",
        "                if search_result[0] is not None:\n",
        "                    logging.warning(f\"\\tBest find is {search_result[0]}\")\n",
        "                    return self._retrieve_summary(search_result[0], depth - 1, include_ambiguous)  \n",
        "                else: return None\n",
        "\n",
        "            except  wikipedia.DisambiguationError as e:\n",
        "                logging.warning(f\"{title} is ambiguous\")\n",
        "                if include_ambiguous:\n",
        "                    return self._retrieve_summary(search_result[1], depth - 1, include_ambiguous)\n",
        "                else: return None\n",
        "                \n",
        "            except IndexError as e:\n",
        "                logging.warning(f\"No result for {title}, skipping\")\n",
        "                return None\n",
        "    \n",
        "\n",
        "    def _retrieve_all_articles(self, include_ambiguous : bool = False):\n",
        "        logging.info(\"Starting retrieveing articles...\")\n",
        "        nb_success = 0\n",
        "\n",
        "        nb_token = len(self.list_tags)\n",
        "        for i, tag in tqdm(enumerate(self.list_tags), total=nb_token):\n",
        "            res = self._retrieve_summary(tag, 5, include_ambiguous)\n",
        "            self.list_articles[tag] = res\n",
        "\n",
        "            if res is not None: \n",
        "                nb_success += 1\n",
        "\n",
        "        logging.info(f\"Finished retrieving {nb_success} article(s) !\")\n",
        "\n",
        "\n",
        "    def convert(self, force_retrieve : bool = False):\n",
        "        \"\"\" convert all word in their embeddings\"\"\"\n",
        "        if len(self.list_tags) == 0:\n",
        "            raise Exception(\"not tags yet !\")\n",
        "\n",
        "        if len(self.list_tags) != len(self.list_articles) or force_retrieve:\n",
        "            self._retrieve_all_articles()\n",
        "\n",
        "        logging.info(\"Starting converting tokens...\")\n",
        "        nb_token = len(self.list_tags)\n",
        "        current_percent = 0.00\n",
        "\n",
        "        for i, tag in tqdm(enumerate(self.list_tags), total = nb_token):\n",
        "            \n",
        "            if self.list_articles[tag] is None: continue\n",
        "\n",
        "            article_size = len(self.list_articles[tag][1])\n",
        "            max_size_article = min(article_size, 300)\n",
        "\n",
        "            tag_plus_context = tag + \". \" + self.list_articles[tag][1][:max_size_article]\n",
        "            inputs = self.tokenizer(tag_plus_context, return_tensors = \"pt\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            hidden_states = outputs[2]\n",
        "\n",
        "            # [# layers, # batches, # tokens, # features] ==> [# tokens, # layers, # features]\n",
        "            token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "            token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "            token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "            # apply different strategy to summarize word embeddings\n",
        "            # tokenized_text = self.tokenizer.tokenize(tag)\n",
        "            # acc = []\n",
        "            # for i, token in reversed(list(enumerate(tokenized_text))):\n",
        "\n",
        "            #     embed = self.merging_strategy.merge(token_embeddings[i+1])\n",
        "            #     if i == 0:\n",
        "            #         if len(acc) != 0:\n",
        "            #             embed = torch.mean(torch.stack([x[1] for x in acc]), dim=0)\n",
        "            #             token = tag\n",
        "            #             acc = []\n",
        "\n",
        "            #         self.embeddings.append((token, embed))\n",
        "            #     else:\n",
        "            #         acc.append((token, embed))\n",
        "            \n",
        "            # here we are just taking the [CLS] (for classification) as an embedding for the tag\n",
        "            self.embeddings.append((tag, self.merging_strategy.merge(token_embeddings[0])))\n",
        "\n",
        "\n",
        "    def compute_sim(self):\n",
        "        \"\"\" compute cosine similarity between all vectors \"\"\"\n",
        "        if len(self.embeddings) == 0:\n",
        "            raise Exception(\"Tags not converted yet !\")\n",
        "\n",
        "        logging.info(\"Computing cosine similarity, this could take some time...\")\n",
        "\n",
        "        n_tokens = len(self.embeddings)\n",
        "        self.cosine_sim_matrix = [[1 for j in range(n_tokens)] for i in range(n_tokens)]\n",
        "\n",
        "        for j, vector in tqdm(enumerate(self.embeddings), total = len(self.embeddings)):\n",
        "\n",
        "            for i, other_vector in enumerate(self.embeddings):\n",
        "\n",
        "                if i == j:\n",
        "                    continue\n",
        "\n",
        "                \n",
        "                cos = torch.nn.CosineSimilarity(dim=0)\n",
        "                similarity = cos(vector[1], other_vector[1])\n",
        "\n",
        "                self.cosine_sim_matrix[i][j] = similarity\n",
        "                self.cosine_sim_matrix[j][i] = similarity\n",
        "\n",
        "    def export_sim_matrix(self, filename):\n",
        "        if self.cosine_sim_matrix == None:\n",
        "            self.compute_sim()\n",
        "        \n",
        "        try:\n",
        "            f = open(filename, \"w\")\n",
        "        except OSError:\n",
        "            raise OSError(\"Could not open file\")\n",
        "\n",
        "        with f:\n",
        "            print(\"/\", \",\".join([tag[0] for tag in self.embeddings]), sep = \",\", file = f)\n",
        "\n",
        "            for j, tag_y in enumerate(self.embeddings):\n",
        "                print(tag_y[0], \",\".join( [str(round(float(self.cosine_sim_matrix[j][i]), 3)) for i in range(len(self.embeddings))]), sep = \",\", file = f)\n",
        "\n",
        "    def sim_between(self, token1, token2):\n",
        "        if self.cosine_sim_matrix is None:\n",
        "            self.compute_co_sim()\n",
        "\n",
        "        index1 = [i for i, v in enumerate(self.embeddings) if v[0] == token1][0]\n",
        "        index2 = [i for i, v in enumerate(self.embeddings) if v[0] == token2][0]\n",
        "\n",
        "        return self.cosine_sim_matrix[index1][index2]\n",
        "\n",
        "    def get_embedding_of(self, token):\n",
        "        res = [v for v in self.embeddings if v[0] == token]\n",
        "        if len(res) == 0:\n",
        "            raise Exception(f\"no such token {token}\")\n",
        "        \n",
        "        return res[0]\n",
        "\n",
        "    def get_nearest_embedding_of(self, embedding, nb = 10):\n",
        "\n",
        "        if nb > len(self.embeddings):\n",
        "            raise Exception(\"nb too high, not enough token\")\n",
        "\n",
        "        nearest = []\n",
        "        for e in self.embeddings:\n",
        "\n",
        "            cos = torch.nn.CosineSimilarity(dim=0)\n",
        "            similarity = cos(embedding, e[1])\n",
        "\n",
        "            nearest.append((e[0], similarity))\n",
        "        \n",
        "        nearest.sort(key = lambda tup : tup[1])\n",
        "        return nearest[-nb:]\n",
        "\n",
        "    def get_class_list(self):\n",
        "        return [x[0] for x in self.embeddings]\n",
        "\n",
        "class Sum4LastLayers:\n",
        "\n",
        "    def merge(self, vector):\n",
        "        return torch.sum(vector[-4:], dim = 0)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UVJUSGWQorYj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingTest:\n",
        "\n",
        "    def __init__(self, filename : str):\n",
        "\n",
        "        self.file = filename\n",
        "        self.embeddings = {}\n",
        "\n",
        "        self._load_file()\n",
        "\n",
        "    def _load_file(self):\n",
        "        try:\n",
        "            with open(self.filename, \"r\") as f:\n",
        "                lines = f.readlines()\n",
        "                \n",
        "            for line in lines:\n",
        "                self.embeddings[line[0]] = line[1:]\n",
        "\n",
        "        except IOError as e:\n",
        "            raise IOError(f\"No file {self.filename}\")\n",
        "\n",
        "    def evaluate(self):\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "5AxDIhr1rkca"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
        "imagenet_labels = np.array(open(labels_path).read().splitlines())\n",
        "imagenet_labels = list(imagenet_labels)\n",
        "len(imagenet_labels)"
      ],
      "metadata": {
        "id": "NIJ8zsk-UTuh",
        "outputId": "0b37e557-ea29-4929-9c9c-630ab21a3cb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1001"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub_imagenet_labels = imagenet_labels[:200]\n",
        "print(len(sub_imagenet_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jjpQvehivmw",
        "outputId": "df8864f1-875f-436a-f134-a6490e700679"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wikipedia.set_rate_limiting(True)"
      ],
      "metadata": {
        "id": "xXLGLeBWYJzs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERTModel(sub_imagenet_labels, big = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvIn_JeFlYfK",
        "outputId": "3e5ce472-0fc0-4af6-c2bc-7dbfca7f731a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.import_tag_list(\"en-basic\")\n",
        "model.convert()\n",
        "model.export(\"imageNet-embeddeding.csv\")\n",
        "print(len(model.get_class_list()))\n",
        "\n",
        "# model.computeCoSim()\n",
        "# model.export_sim_matrix(\"sim_matrix.csv\")"
      ],
      "metadata": {
        "id": "WOOlgG8koVD6",
        "outputId": "f5cff78b-5331-4ecb-851b-62ea6c4a5f51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/200 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n",
            "100%|██████████| 200/200 [02:53<00:00,  1.15it/s]\n",
            "100%|██████████| 200/200 [03:18<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Article to search for { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "totest = \"King\" #@param {type:\"string\"}\n",
        "\n",
        "result = wikipedia.search(totest, suggestion = False)\n",
        "print(result)\n",
        "print(f\"first result with suggestion is: {result[0]}\")\n",
        "\n",
        "try:\n",
        "    print(wikipedia.page(result[0], auto_suggest=False, redirect=True))\n",
        "    print(wikipedia.page(totest, auto_suggest=False, redirect=True))\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeM8XAu6h5gD",
        "outputId": "ddfe0063-0382-4445-bfe3-6b4125445492"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['King', 'King (disambiguation)', 'Martin Luther King Jr.', 'Stephen King', 'King & King', 'King King', 'The Lion King', 'Burger King', 'George VI', 'King Star King']\n",
            "first result with suggestion is: King\n",
            "<WikipediaPage 'King'>\n",
            "<WikipediaPage 'King'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = BERTModel([\"King\", \"Queen regnant\", \"man\", \"woman\"], big = True)"
      ],
      "metadata": {
        "id": "8uNXNj_wCWhf",
        "outputId": "6c6d73e7-cf68-41d5-baa1-e534ef9cc46d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "test_model.convert(force_retrieve = True)\n",
        "\n",
        "man = test_model.get_embedding_of(\"man\")[1]\n",
        "woman = test_model.get_embedding_of(\"woman\")[1]\n",
        "king = test_model.get_embedding_of(\"King\")[1]\n",
        "\n",
        "totest = king.sub(man).add(woman)\n",
        "print(test_model.get_nearest_embedding_of(totest, nb=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "2j1Pw0um1ofC",
        "outputId": "25cc8ad0-b8bb-4ce1-a4a7-9e2a28c3d0db"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/4 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.53it/s]\n",
            "100%|██████████| 4/4 [00:03<00:00,  1.09it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-be855136d985>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mman\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embedding_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"man\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwoman\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embedding_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"woman\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embedding_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"King\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtotest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mman\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwoman\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-5ca38919b14f>\u001b[0m in \u001b[0;36mget_embedding_of\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"no such token {token}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: no such token King"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 821 / 1001 classes = 82%\n",
        "# x\n",
        "# 1024 col\n",
        "\n",
        "!ls \"/content\""
      ],
      "metadata": {
        "id": "TxVz3vl-WxlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f /content/temp/*"
      ],
      "metadata": {
        "id": "_rBO4mg7aT2d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}